<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>03-simple-reg-parameter-estimate-slide.utf8</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/duke-blue.css" rel="stylesheet" />
    <link href="libs/remark-css-0.0.1/hygge-duke.css" rel="stylesheet" />
    <script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
    <script src="libs/jquery-1.12.4/jquery.min.js"></script>
    <link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding-0.13/datatables.js"></script>
    <link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>
    <script src="libs/kePrint-0.0.1/kePrint.js"></script>
    <link rel="stylesheet" href="libs\cc-fonts.css" type="text/css" />
    <link rel="stylesheet" href="libs\figure-captions.css" type="text/css" />
    <link rel="stylesheet" href="libs\animate.css" type="text/css" />
    <link rel="stylesheet" href="libs\mycss\my-custom.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

background-image: url("pic/slide-front-page.jpg")
class: center,middle

# 计量经济学(Econometrics)

### 胡华平

### 西北农林科技大学

### 经济管理学院数量经济教研室

### huhuaping01@hotmail.com

### 2020-05-19







---
class: center, middle,inverse
# 第3章：一元回归：参数估计

.pull-left[

[3.1 普通最小二乘法（OLS）](#OLS)

[3.2 最小二乘估计的精度](#variance)

[3.3 经典线性回归模型(CLRM)](#CLRM)

[3.4 最小二乘估计的性质：BLUE](#BLUE)

]

.pull-right[

[3.5 变异分解与拟合优度](#ANOVA)

[3.6 一个数值例子](#numerical-case)

[3.7 经典正态线性回归模型（N-CLRM）](#N-CLRM)

[3.8 极大似然估计法(ML)](#ML)

]

---
layout: false
class: inverse, center, middle, duke-softblue
name: OLS

# 3.1 普通最小二乘法(OLS)

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.1 普通最小二乘法(OLS) &lt;/span&gt;&lt;/div&gt; 

---

### 引子：如何估计回归函数中的系数？

我们已经知道如何科学表达总体回归和样本回归的关系：

.pull-left[

&gt; 总体回归：
`$$\begin{cases}
  \begin{align}
  E(Y|X_i) &amp;= \beta_1 +\beta_2X_i &amp;&amp; \text{(PRF)} \\
  Y_i &amp;=  \beta_1 +\beta_2X_i + u_i &amp;&amp; \text{(PRM)}
  \end{align}
\end{cases}$$`

]

.pull-right[
&gt; 样本回归：
`$$\begin{cases}
  \begin{align}
  \hat{Y}_i &amp; =\hat{\beta}_1 + \hat{\beta}_2X_i &amp;&amp; \text{(SRF)} \\
  Y_i &amp;= \hat{\beta}_1 + \hat{\beta}_2X_i +e_i &amp;&amp; \text{(SRM)}
  \end{align}
\end{cases}$$`

]


首先需要回答的问题是，我们该如何估计得出样本回归函数中的系数？事实上，方法有多种多样：

- 图解法：比较粗糙，但提供了基本的视觉认知

- 最小二乘法(order lease squares, OLS)：最常用的方法

- 最大似然法(maximum likelihood, ML)

- 矩估计方法(Moment method, MM)

---

## 样本回归与总体回归的比较

.pull-left[
总体回归函数PRF:

`$$\begin{align}
E(Y|X_i) &amp;= \beta_1 +\beta_2X_i 
\end{align}$$`

总体回归模型PRM:

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i 
\end{align}$$`

]

.pull-right[
样本回归函数SRF:

`$$\begin{align}
\hat{Y}_i =\hat{\beta}_1 + \hat{\beta}_2X_i 
\end{align}$$`

样本回归模型SRM:

`$$\begin{align}
Y_i &amp;= \hat{\beta}_1 + \hat{\beta}_2X_i +e_i 
\end{align}$$`

]

--

思考：

- PRF无法直接观测，只能用SRF近似替代

- 估计值与观测值之间存在偏差

- SRF又是怎样决定的呢?

---

## 普通最小二乘法的原理

认识普通最小二乘法的原理：一个图示

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-demo.png" alt="最小二乘法的原理" width="469" /&gt;
&lt;p class="caption"&gt;最小二乘法的原理&lt;/p&gt;
&lt;/div&gt;

---

## 普通最小二乘法的原理

OLS的基本原理：残差平方和最小化。

`$$\begin{align}
e_i  &amp;= Y_i - \hat{Y}_i \\
     &amp;= Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) 
\end{align}$$`


`$$\begin{align}
Q  &amp;= \sum{e_i^2} \\
   &amp;= \sum{(Y_i - \hat{Y}_i)^2} \\
   &amp;= \sum{\left( Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) \right)^2} \\
   &amp;\equiv f(\hat{\beta}_1,\hat{\beta}_2)
\end{align}$$`


`$$\begin{align}
Min(Q)  &amp;= Min \left ( f(\hat{\beta}_1,\hat{\beta}_2) \right)
\end{align}$$`

---

## 普通最小二乘法的原理

认识普通最小二乘法的原理：一个数值试验。假设存在下面所示的4组观测值
`\((X_i, Y_i)\)`：

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-compare1.png" alt="数值试验：数据" width="2504" /&gt;
&lt;p class="caption"&gt;数值试验：数据&lt;/p&gt;
&lt;/div&gt;

---

## 普通最小二乘法的原理

假设随便猜想了如下两个SRF，完成下表计算，并分析哪个SRF给出的
`\((\hat{\beta}_1, \hat{\beta}_2)\)`要更好？

`$$\begin{align}
SRF1：\hat{Y}_{1i} &amp; = \hat{\beta}_1 +\hat{\beta}_2X_i = 1.572 + 1.357X_i \\
SRF2：\hat{Y}_{2i} &amp; = \hat{\beta}_1 +\hat{\beta}_2X_i = 3.0 + 1.0X_i 
\end{align}$$`


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-compare2.png" alt="数值试验：计算" width="821" /&gt;
&lt;p class="caption"&gt;数值试验：计算&lt;/p&gt;
&lt;/div&gt;

---

## 回归参数的OLS点估计

最小化求解：

`$$\begin{align}
Min(Q)  &amp;= Min \left ( f(\hat{\beta}_1,\hat{\beta}_2) \right)\\
 &amp;= Min\left(\sum{\left( Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) \right)^2} \right) \\
  &amp;= Min \sum{\left( Y_i - \hat{\beta}_1 - \hat{\beta}_2X_i \right)^2}
\end{align}$$`


方程组变形，得到**正规方程组**：

`$$\begin{align}
\left \{
  \begin{split}
   \sum{\left[ \hat{\beta}_1 - (Y_i -\hat{\beta}_2X_i) \right]}  &amp;=0 \\
   \sum{\left[ X_i^2\hat{\beta}_2 - (Y_i-\hat{\beta}_1 )X_i \right ] }&amp;=0 
   \end{split}
\right. 
\end{align}$$`

`$$\begin{align}
\left \{
  \begin{split}
   \sum{Y_i} - n\hat{\beta}_1- (\sum{X_i})\hat{\beta}_2 &amp;=0 \\
   \sum{X_iY_i}-(\sum{X_i})\hat{\beta}_1 -  (\sum{X_i^2})\hat{\beta}_2 &amp;=0 
   \end{split}
\right.
\end{align}$$`

---

## 回归参数的OLS点估计

进而得到回归系数的计算公式1（Favorite Five，FF）：

`$$\begin{align}
  \left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
  \hat{\beta}_1 &amp;=\frac{n\sum{X_i^2Y_i}-\sum{X_i}\sum{X_iY_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}
  \end{split} 
  \right.
  &amp;&amp;\text{(FF solution)}
\end{align}$$`

---

## 回归参数的OLS点估计

此外我们也可以得到如下的离差公式(favorite five，ff)

`$$\begin{align}
\left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{\sum{x_iy_i}}{\sum{x_i^2}}\\
  \hat{\beta}_1 &amp;=\bar{Y}_i-\hat{\beta}_2\bar{X}_i
  \end{split} 
\right.  
  &amp;&amp; \text{(ff solution)}
\end{align}$$`

其中离差计算
`\(x_i=X_i-\bar{X};\  y_i=Y_i - \bar{Y}\)`。

---

### 课堂测试：

以下式子为什么是等价的？你能推导出来么？

`$$\begin{align}
\left\{
  \begin{split}
    \sum{x_iy_i} &amp;= \sum{\left[ (X_i-\bar{X})(Y_i-\bar{Y})\right]} 
    &amp;&amp;= \sum{X_iY_i} - \frac{1}{n}\sum{X_i}\sum{Y_i} \\
    \sum{x_i^2} &amp;= \sum{(X_i- \bar{X})^2} 
    &amp;&amp;= \sum{X_i^2} -\frac{1}{n} \left( \sum{X_i} \right)^2
  \end{split}
\right.
\end{align}$$`

---

## 随机干扰项参数的OLS点估计

PRM公式变形：

`$$\begin{alignedat}{2}
&amp;\left.
  \begin{split}
   Y_i &amp;&amp;= \beta_1 - &amp;&amp;\beta_2X_i +u_i  \ &amp;&amp; \text{(PRM)} \Rightarrow \\
   \hat{Y} &amp;&amp;= \beta_1 - &amp;&amp;\beta_2\bar{X} +\bar{u} &amp;&amp; \\   
  \end{split}
\right \} \Rightarrow \\
 &amp; y_i = \beta_2x_i +(u_i- \bar{u})  
\end{alignedat}$$`

残差公式变形：

`$$\begin{alignedat}{2}
 &amp;\left. 
  \begin{split}
    &amp; e_i = y_i - \hat{\beta}_2x_i \\
    &amp; e_i = \beta_2x_i +(u_i- \bar{u}) -\hat{\beta}_2x_i 
  \end{split}
\right \}  \Rightarrow \\
&amp; e_i =-(\hat{\beta}_2- \beta_2)x_i + (u_i- \hat{u})
\end{alignedat}$$`

---

## 随机干扰项参数的OLS点估计

求解残差平方和：

`$$\begin{alignedat}{2}
  &amp; \sum{e_i^2} &amp;&amp; = (\hat{\beta}_2 - \beta_2)^2\sum{x_i^2} + \sum{(u-\bar{u})^2} - 2(\hat{\beta}_2 - \beta_2)\sum{x_i(u-\bar{u})}  
\end{alignedat}$$`

求残差平方和的期望：

`$$\begin{align}
E(\sum{e_i^2}) &amp;= 
 \sum{x_i^2 E \left[ (\hat{\beta}_2 - \beta_2)^2 \right ]}+ E\left[ \sum{(u-\bar{u})^2} \right ]\\
&amp;+ 2E \left[ (\hat{\beta}_2 - \beta_2)\sum{x_i(u-\bar{u})} \right ] \\
&amp; \equiv   A + B + C \\
&amp; = \sigma^2 + (n-1)\sigma^2 -2\sigma^2 \\
&amp; = (n-2)\sigma^2 
\end{align}$$`

---

## 随机干扰项参数的OLS点估计

**回归误差方差**（Deviation of Regression Error）：

- 采用OLS方法下，总体回归模型PRM中随机干扰项
`\(u_i\)`的总体方差的无偏估计量，记为
`\(E(\sigma^2) \equiv \hat{\sigma}^2\)`，简单地记为
`\(\hat{\sigma}^2\)`。

`$$\begin{align}
\hat{\sigma}^2=\frac{\sum{e_i^2}}{n-2}
\end{align}$$`

--

**回归误差标准差**（Standard Deviation of Regression Error）：有时候也记为**se**。

`$$\begin{align}
\hat{\sigma}=\sqrt{\frac{\sum{e_i^2}}{n-2}}
\end{align}$$`

???
- 采用OLS方法下，总体回归模型PRM中随机干扰项
`\(u_i\)`的总体标准差的无偏估计量，记为
`\(E(\sigma) \equiv \hat{\sigma}\)`，代数表达式一般简单地记为
`\(\hat{\sigma}\)`

---
class: duke-orange

### A过程证明

`$$\begin{align}
A &amp; = \sum{x_i^2 E \left[ (\hat{\beta}_2 - \beta_2)^2 \right ]} \\
  &amp; = \sum{ \left[ x_i^2 \cdot var(\hat{\beta}_2) \right] } \\
  &amp; = var(\hat{\beta}_2) \cdot \sum{x_i^2}  \\
  &amp; = \frac{\sigma^2}{\sum{ x_i^2}} \cdot \sum{ x_i^2}  \\
  &amp; = \sigma^2
\end{align}$$`

---
class: duke-orange
### B过程证明

`$$\begin{align}
B  = E \left[ \sum{(u-\bar{u})^2} \right ] 
  &amp; = E(\sum{u_i^2}) - 2E \left[ \sum{(u_i\bar{u})} \right] +nE(\bar{u}^2) \\
  &amp; = n \cdot Var(u_i) - 2E \left[ \sum{(u_i \cdot \frac{\sum{u_i}}{n} )}  \right]  + nE(\frac{\sum{u_i}}{n})^2 \\
  &amp; = n \sigma^2 - 2E \left[ \frac{\sum{u_i}}{n} \sum{u_i} \right] + E\left[ \frac{(\sum{u_i})^2}{n} \right]\\
  &amp; = n \sigma^2- E\left[ (\sum{u_i})^2/{n} \right] 
  = n \sigma^2  -  \frac{E(u_i^2) + E(u_2^2) + \cdots +  E(u_n^2) )}{n} \\
  &amp; =  n \sigma^2 -  \frac{nVar{u_i}}{n} 
   =  n \sigma^2 -  \sigma^2 =  (n-1) \sigma^2
\end{align}$$`

---
class: duke-orange
###  C过程证明

`$$\begin{align}
C &amp;= - 2E \left[ (\hat{\beta}_2 - \beta_2)\sum{x_i(u_i-\bar{u})} \right ] \\
  &amp;= - 2E \left[ \frac{\sum{x_iu_i}}{\sum{x_i^2}} \left( \sum{x_iu_i}-\bar{u}\sum{x_i} \right) \right ] \\
  &amp;= - 2E \left[ \frac{ \left( \sum{x_iu_i} \right)^2}{\sum{x_i^2}}  \right ]  \\
  &amp;= -2E \left[(\hat{\beta}_2 - \beta_2)^2 \right] = -2\sigma^2
\end{align}$$`

--

- 其中：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} = \sum{k_i(\beta_1 +\beta_2X_i +u_i)}   = \beta_1\sum{k_i} +\beta_2 \sum{k_iX_i}+\sum{k_iu_i}  
= \beta_2 +\sum{k_iu_i} \\
\hat{\beta}_2 - \beta_2 &amp; = \sum{k_iu_i} = \frac{ \sum{x_iu_i} }{\sum{x_i^2}}
\end{align}$$`

---

## 随机干扰项参数的OLS点估计

估计出总体参数
`\(\sigma^2\)`：

`$$\begin{align}
\hat{\sigma}^2 = \frac{\sum{e_i^2}}{(n-2)}
\end{align}$$`

`$$\begin{align}
\hat{\sigma} = \sqrt{\frac{\sum{e_i^2}}{(n-2)}}
\end{align}$$`

---

## OLS方法讨论：“估计值”与“估计量”

理解OLS方法下的“估计值”与“估计量”

回归系数的计算公式1（Favorite Five，FF）：

`$$\begin{align}
  \left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
  \hat{\beta_1} &amp;=\frac{n\sum{X_i^2Y_i}-\sum{X_i}\sum{X_iY_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}
  \end{split} 
  \right.
  &amp;&amp;\text{(FF solution)}
\end{align}$$`


- 如果给出的参数估计结果是由一个具体样本资料计算出来的，它是一个“估计值”，或者“点估计”，是参数估计量的一个具体数值；

- 如果把上式看成参数估计的一个表达式，那么，则它是
`\((X_i,Y_i)\)`的函数，而$Y_i$是随机变量，所以参数估计也是随机变量，在这个角度上，称之为“估计量”。  

---

## OLS方法下SRF和SRM的特征

OLS估计量是纯粹由可观测的(即样本)量(指X和Y)表达的，因此它们很容易计算。

它们是点估计量(point estimators)，即对于给定样本，每个估计量仅提供有关总体参数的一个(点)值。[我们以后还将考虑区间估计量(interval Estimators)]

一旦从样本数据得到OLS估计值，便容易画出样本回归线。

---

## OLS方法下SRF和SRM的特征

- 特征1：样本回归线一定会经过样本均值点
`\((\bar{X}, \bar{Y})\)`：

`$$\begin{align}
\bar{Y} = \hat{\beta}_1 +\hat{\beta}_2\bar{X}
\end{align}$$`

- 特征2：
`\(Y_i\)`的**估计值**(
`\(\hat{Y}_i\)`)的均值(
`\(\bar{\hat{Y_i}}\)`)等于Y的样本均值(
`\(\bar{Y}\)`)

`$$\begin{align}
\hat{Y_i} &amp;= \hat{\beta}_1 +\hat{\beta}_2\bar{X} \\
&amp; =(\bar{Y} - \hat{\beta}_2\bar{X}) + \hat{\beta_2}X_i \\
&amp; = \bar{Y} - \hat{\beta}_2(X_i - \bar{X}) 
\end{align}$$`

`$$\begin{align}
&amp;\Rightarrow  1/n\sum{\hat{Y_i}} =  1/n\sum{\bar{Y} - \hat{\beta}_2(X_i - \bar{X})} \\
&amp;\Rightarrow  \bar{\hat{Y_i}}  = \bar{Y}
\end{align}$$`

---

## OLS方法下SRF和SRM的特征

- 特征3：残差的均值(
`\(\bar{e_i}\)`)为零：

`$$\begin{align}
\sum{\left[ \hat{\beta}_1 - (Y_i -\hat{\beta}_2X_i) \right]}  &amp;=0 \\
\sum{\left[ Y_i- \hat{\beta}_1 - \hat{\beta}_2X_i) \right]}  &amp;=0 \\
\sum{( Y_i- \hat{Y}_i )} &amp;=0 \\
\sum{e_i}  &amp;=0 \\
\bar{e_i} &amp;=0
\end{align}$$`

---

## OLS方法下SRF和SRM的特征

- 特征4：SRM和SRF可以写成离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i + e_i \\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X}) + e_i \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i +e_i \  &amp;&amp;\text{(SRM-dev)}
\end{align}$$`

`$$\begin{align}
&amp; \left.
  \begin{split}
  \hat{Y}_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i\\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; \hat{Y}_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X})  \Rightarrow  \\
&amp; \hat{y}_i=\hat{\beta_2}x_i \  &amp;&amp;\text{(SRF-dev)} 
\end{align}$$`

---

## OLS方法下SRF和SRM的特征

- 特征5：残差(
`\(e_i\)`)和
`\(Y_i\)`的拟合值(
`\(\hat{Y_i}\)`)不相关

`$$\begin{align}
Cov(e_i, \hat{Y_i}) &amp;= E \left[ \left( e_i-E(e_i)\right )\cdot \left( \hat{Y_i}-E(\hat{Y_i})\right ) \right]
= E(e_i \cdot \hat{y_i}) \\
&amp; = \sum(e_i \cdot \hat{\beta_2}x_i) \\
&amp; = \sum{ \left[ (y_i-\hat{\beta_2}x_i) \cdot \hat{\beta_2}x_i \right]} \\
&amp; = \hat{\beta_2}\sum \left[ (y_i-\hat{\beta_2}x_i)\cdot x_i \right]\\
&amp; = \hat{\beta_2}\sum \left[ (y_ix_i-\hat{\beta_2}x_i^2)  \right]\\
&amp; = \hat{\beta_2}\sum{x_iy_i}-\hat{\beta}_2^2\sum{x_i^2}  &amp;&amp; \Leftarrow \hat{\beta_2} = \frac{\sum{x_iy_i}}{x_i^2} \\
&amp; = \hat{\beta}_2^2\sum{x_i^2}-  \hat{\beta_2}^2\sum{x_i^2}   = 0
\end{align}$$`


- 特征6：残差(
`\(e_i\)`)和自变量(
`\(X_i\)`)不相关

---

## OLS方法下的离差公式总结

- 离差定义与符号：

`$$\begin{align}
x_i &amp;= X_i - \bar{X} \\
y_i &amp;= Y_i - \bar{Y} \\
\hat{y}_i &amp;= \hat{Y}_i - \bar{\hat{Y}}_i = \hat{Y}_i - \bar{Y}
\end{align}$$`

- PRM及其离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \beta_1 + \beta_2X_i + u_i \\
  \bar{Y} &amp;&amp;= \beta_1 + \beta_2\bar{X} + \bar{u}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\beta_2x_i + (u_i- \bar{u}) \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i + (u_i- \bar{u})  \  &amp;&amp;\text{(PRM-dev)}
\end{align}$$`

---

## OLS方法下的离差公式总结

--
.pull-left[
- SRM及其离差形式：
`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i + e_i \\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X}) + e_i \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i +e_i 
\end{align}$$`
]

--
.pull-right[
- SRF及其离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  \hat{Y}_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i\\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; \hat{Y}_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X})  \Rightarrow  \\
&amp; \hat{y}_i=\hat{\beta_2}x_i \   
\end{align}$$`
]

--

- 残差的离差形式：

`$$\begin{align}
 y_i=\hat{\beta_2}x_i +e_i  &amp;&amp;\text{(SRM-dev)} \ \Rightarrow  \\
 e_i =y_i - \hat{\beta_2}x_i \  &amp;&amp;\text{(residual-dev)}
\end{align}$$`


---

### 小结与讨论

本节**内容小结**：

- 普通最小二乘方法（OLS）采用“铅垂线距离平方和最小化”的思想，来拟合一条样本回归线，进而求解出模型参数估计量。

- 拟合样本回归线，并求解出模型参数估计量的方法有很多，还有**极大似让估计法**（MLE）、**据估计方法**（MM）等等。不同估计方法代表不同的思想理念，当然各种方法的优劣势差异都需要考虑到“模型环境”（或模型假设）。

- 大家需要很熟练地记住OLS参数估计量公式，以及它们的几大重要特征！

**思考与讨论**：

- OLS采用的“铅垂线距离平方和最小化”这一方案，凭什么它被奉为计量分析的经典方法？你觉得还有其他可行替代方案么？可以是“垂线距离平方和最小化”么？如果是距离的3次方或4次方之和，又会怎样？距离的绝对值之和可以么？对于这些方案，你有什么想法？

- 回归标准误差
`\(se\)`的现实含义是什么？回归参数估计与随机干扰项的方差估计有什么内在联系么？

- OLS方法的几个特征，是不是使它“天生丽质”、“娘胎里生下来就含着金钥匙”？为什么能这么说？


---
layout: false
class: inverse, center, middle, duke-softblue
name: variance

# 3.2 OLS估计量的精度

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.2 OLS估计量的精度 &lt;/span&gt;&lt;/div&gt; 


---

### 引子：如何知道OLS方法点估计量是否可靠？

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i 
\end{align}$$`

我们已经使用OLS方法分别得到总体回归模型(PRM)的3个重要参数（实际不止3个）的点估计量：

`$$\begin{align}
\left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{\sum{x_iy_i}}{\sum{x_i^2}}\\
  \hat{\beta}_1 &amp;=\bar{Y}_i-\hat{\beta}_2\bar{X}_i \\
  \hat{\sigma}^2 &amp;=\frac{\sum{e_i^2}}{n-2}
  \end{split} 
  \right.  
\end{align}$$`

&gt;问题是：
&gt; - OLS方法的点估计量是否稳定？ OLS方法的点估计量是否可信？

因此，我们需要找到一种表达OLS方法估计稳定性或估计精度的指标！

&gt; 点估计量的**方差**（variance）和**标准差**（standard deviation）就是衡量估计稳定性或估计精度的一类重要指标！

---

## 斜率系数的方差和样本方差

.pull-left[
斜率系数（
`\(\hat{\beta}_2\)`）的**总体方差**（
`\(\sigma^2_{\hat{\beta}_2}\)`）和**总体标准差**（
`\(\sigma_{\hat{\beta}_2}\)`）：

`$$\begin{align}
Var(\hat{\beta}_2) \equiv \sigma_{\hat{\beta}_2}^2  &amp; =\frac{\sigma^2}{\sum{x_i^2}} \\
\sigma_{\hat{\beta}_2} &amp;=\sqrt{\frac{\sigma^2}{\sum{x_i^2}}} 
\end{align}$$`

- 其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。

]

.pull-right[
斜率系数（
`\(\hat{\beta}_2\)`）的**样本方差**（
`\(S^2_{\hat{\beta}_2}\)`）和**样本标准差**（
`\(S_{\hat{\beta}_2}\)`）：

`$$\begin{align}
S_{\hat{\beta}_2}^2 &amp;=\frac{\hat{\sigma}^2}{\sum{x_i^2}} \\
S_{\hat{\beta}_2} &amp;=\sqrt{\frac{\hat{\sigma}^2}{\sum{x_i^2}}}
\end{align}$$`

- 其中，
`\(E(\sigma^2) = \hat{\sigma}^2 = \frac{\sum{e_i^2}}{n-2}\)`表示对随机干扰项（
`\(u_i\)`）的总体方差的**无偏估计量**。
]

---

### 证明过程1

**步骤1**
`\(\hat{\beta}_2\)`的变形：

`$$\begin{align}
\hat{\beta}_2 &amp;=\frac{\sum{x_iy_i}}{\sum{x_i^2}}= \frac{\sum{\left[ x_i (Y_i -\bar{Y}) \right]} }{\sum{x_i^2}}  \\
&amp; = \frac{\sum{ x_iY_i}- \sum{ x_i \bar{Y} } }{\sum{x_i^2}}    \\
&amp; = \frac{\sum{x_iY_i}- \bar{Y}\sum{x_i} }{\sum{x_i^2}}  &amp;&amp; \leftarrow \left[ \sum{x_i}=\sum{(X_i -\bar{X})} = 0 \right]  \\
&amp; = \sum{ \left(\frac{x_i}{\sum{x_i^2}} \cdot Y_i \right) }   &amp;&amp; \leftarrow  \left[ k_i \equiv \frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sum{k_iY_i}
\end{align}$$`

&gt; - 其中，
`\(k_i \equiv \frac{x_i}{\sum{x_i^2}}\)`。

---

### 证明过程2

**步骤2**：计算
`\(\hat{\beta}_2\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_2}\)`）：

`$$\begin{align}
\sigma^2_{\hat{\beta}_2} &amp; \equiv Var(\hat{\beta}_2) 
 = Var(\sum{k_iY_i} ) \\
&amp; = \sum{\left( k_i^2Var(Y_i) \right)} \\
&amp; = \sum{\left( k_i^2Var(\beta_1 +\beta_2X_i +u_i) \right)} \\
&amp; = \sum{ \left( k_i^2Var(u_i) \right)}  &amp;&amp; \leftarrow \left[ k_i  \equiv \frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sum{ \left( \left(\frac{x_i}{\sum{x_i^2}} 
                 \right)^2 \cdot \sigma^2 
          \right)} \\
&amp; = \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

&gt; 其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。

---

## 截距系数的方差和样本方差

.pull-left[
截距系数（
`\(\hat{\beta}_1\)`）的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）和**总体标准差**（
`\(\sigma_{\hat{\beta}_1}\)`）：


`$$\begin{align}
Var(\hat{\beta}_1) \equiv \sigma_{\hat{\beta}_1}^2  &amp;=\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}} \\
\sigma_{\hat{\beta}_1} &amp; =\sqrt{\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}}
\end{align}$$`


&gt; - 其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。

]

.pull-right[
截距系数（
`\(\hat{\beta}_1\)`）的**样本方差**（
`\(S^2_{\hat{\beta}_1}\)`）和**样本标准差**（
`\(S_{\hat{\beta}_1}\)`）：

`$$\begin{align}
S_{\hat{\beta}_1}^2 &amp;=\frac{\sum{X^2_i}}{n} \cdot \frac{\hat{\sigma}^2}{\sum{x_i^2}} \\
S_{\hat{\beta}_1} &amp;=\sqrt{\frac{\sum{X^2_i}}{n} \cdot \frac{\hat{\sigma}^2}{\sum{x_i^2}}}
\end{align}$$`

&gt; - 其中，
`\(E(\sigma^2) = \hat{\sigma}^2 = \frac{\sum{e_i^2}}{n-2}\)`表示对随机干扰项（
`\(u_i\)`）的总体方差的**无偏估计量**。

]

---

## 证明过程1

**步骤1**
`\(\hat{\beta}_1\)`的变形：

`$$\begin{align}
\hat{\beta_1} &amp; = \bar{Y}_i-\hat{\beta}_2\bar{X}_i &amp;&amp; \leftarrow \left[ \hat{\beta}_2= \sum{k_iY_i} \right] \\
&amp; = \frac{1}{n} \sum{Y_i} - \sum{\left( k_iY_i \cdot \bar{X} \right)} \\
&amp; = \sum{\left( (\frac{1}{n} - k_i\bar{X}) \cdot Y_i  \right)}   &amp;&amp; \leftarrow \left[ w_i \equiv \frac{1}{n} - k_i\bar{X} \right]\\     
&amp; = \sum{w_iY_i}
\end{align}$$`

&gt; - 其中：令
`\(w_i \equiv \frac{1}{n} - k_i\bar{X}\)`

---

### 证明过程2

**步骤2**计算
`\(\hat{\beta}_1\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）：

`$$\begin{align}
\sigma^2_{\hat{\beta}_1} &amp; \equiv  Var(\hat{\beta_1})  = Var(\sum{w_iY_i}) \\
&amp; = \sum{\left( w_i^2Var(\beta_1 +\beta_2X_i + u_i) \right)} &amp;&amp; \leftarrow \left[w_i \equiv \frac{1}{n} - k_i\bar{X} \right]\\
&amp; = \sum{\left( 
            \left( \frac{1}{n} - k_i\bar{X} \right)^2Var(u_i) 
         \right)} \\
&amp; = \sigma^2 \cdot \sum{ \left( \frac{1}{n^2} - \frac{2 \bar{X} k_i}{n} + k_i^2 \bar{X}^2 \right) }  &amp;&amp; \leftarrow \left[ \sum{k_i} = \sum{\left( \frac{x_i}{\sum{x_i^2}} \right)= \frac{\sum{x_i}} {\sum{x_i^2}}}=0 \right] \\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2\sum{k_i^2} \right)  &amp;&amp; \leftarrow \left[ k_i \equiv \frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2\sum{ \left( \frac{x_i}{\sum{x_i^2}} \right) ^2} \right) 
\end{align}$$`

---

### 证明过程2


**步骤2**计算
`\(\hat{\beta}_1\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）（续前）：

`$$\begin{align}
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2  \frac{\sum{x_i^2}}{\left( \sum{x_i^2} \right)^2}  \right) \\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} +   \frac{ \bar{X}^2 } { \sum{x_i^2} }  \right) \\
&amp; =  \frac{\sum{x_i^2} + n\bar{X}^2} {n\sum{x_i^2}} \cdot \sigma^2 &amp;&amp; \leftarrow  \left[ \sum{x_i^2} + n\bar{X}^2 = \sum{(X_i-\bar{X})^2} + n\bar{X}^2 = \sum{X_i^2}\right]\\
&amp; = \frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

---

### 小结与思考

现在做一个**内容小结**：

- 为了衡量OLS方法的点估计量是否稳定或是否可信，我们一般采用方差和标准差指标来表达。

- 大家应熟记**斜率**和**截距**估计量的**总体方差**和**样本方差**最终公式。

请大家**思考**如下问题：


- 总体方差和样本方差都是确定的数么？

- 二者分别受那些因素的影响？二者又有什么联系？

- 证明过程中，约定的ki和wi，有什么特征？

--

.pull-left[

`$$\begin{cases}
  \begin{align}
  \sum{k_i}  &amp; =0 \\
   \sum{k_iX_i} &amp; = 1
  \end{align}
\end{cases}$$`

]

.pull-right[

`$$\begin{cases}
  \begin{align}
  \sum{w_i}  &amp; =1 \\
   \sum{w_iX_i} &amp; = 0
  \end{align}
\end{cases}$$`

]


---
layout: false
class: inverse, center, middle, duke-softblue
name: CLRM

# 3.3 经典线性回归模型假设(CLRM)

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.3 经典线性回归模型假设(CLRM) &lt;/span&gt;&lt;/div&gt; 

---

### 引子：仅仅利用OLS估计方法就足够了么？

我们已经知道OLS方法的原理和基本特征。

**问题**是：

- OLS方法凭什么能在其他众多拟合估计方法中“脱颖而出”？

- 要跨越“从样本推断总体”的巨大“鸿沟”，仅仅使用OLS方法就足够了么？


**答案**是：

**OLS估计方法**，还需要**经典线性回归模型（CLRM）假设**的加持，二者“双剑合璧”才能真正完成“从样本推断总体”的逻辑证明过程。

---

## 经典线性回归模型(CLRM)假设

**经典线性回归模型**(classical linear regression model, CLRM)：

- 又称为高斯或标准线性回归模型

- 成为计量经济学理论的基石，主要包括7个基本假设

- 本章以双变量回归模型为讨论基础。



---

## 关于模型的假设

**CLRM假设1（模型是正确设置的）**：这里大有学问，也是一切计量分析问题的根本来源。

&gt; 思考：
&gt; - 我们怎么知道自己设置的模型是“正确的”？
&gt; - 我们有可能知道“正确的”模型么？

&lt;/br&gt;

**CLRM假设2（模型是参数线性的）**：模型应该是参数线性的，具体而言模型中**参数**和**随机干扰项**必须线性，变量可以不是线性。

`$$\begin{align}
Y_i  = \beta_1 + \beta_2X_i + u_i 
\end{align}$$`

&gt; 思考：为什么需要模型是“线性的”？

---

### 课堂讨论

以下模型都是**线性的**：

`$$\begin{align}
Y_i &amp;= \beta_1 + \beta_2 X_i + \beta_3 X_i^2 +u_i &amp;&amp; \text{(quadratic polynomial)}
\end{align}$$`

`$$\begin{align}
Y_i &amp;= \beta_1 + \beta_2 X_i + \beta_3 X_i^2 + \beta_4 X_i^3 +u_i &amp;&amp; \text{(cubic polynomial)}
\end{align}$$`

--

`$$\begin{align}
Y_i &amp;= \beta_1 + \beta_2 ln(X_i) +u_i &amp;&amp; \text{(linear-log)} \\
\end{align}$$`


`$$\begin{align}
ln(Y_i) &amp;= \beta_1 + \beta_2 X_i +u_i &amp;&amp; \text{(log-linear)}
\end{align}$$`

--

`$$\begin{align}
Y_i &amp;= \beta_1 + \beta_2 \frac{1}{X_i} +u_i &amp;&amp; \text{(reciprocal)}
\end{align}$$`

`$$\begin{align}
Q_t &amp;= AK_t^{\alpha}L_t^{\beta}u_t &amp;&amp; \text{(Cobb-douglas)}
\end{align}$$`

---

## 关于自变量X的假设

**CLRM假设3（自变量X是外生的）**：X是固定的（给定的）或**独立于**误差项。也即自变量X**不是**随机变量。

`$$\begin{align}
Cov(X_i, u_i)= 0\\ 
E(u_i|X_i)= 0
\end{align}$$`

自变量
`\(X\)`是固定的（给定的）是什么含义？

- 其方差
`\(Var(X)\)`是有限的正数。
    - 如X取值不能全部相同。如果全部X取值都一样，也即
`\(Var(X)=0\)`，则会形成什么样的散点图？

    - 又例如回归系数估计值公式中分母为0，无法求解！

- X变量没有异常值(outlier)，即没有一个X值对于其他值过大或过小。

--

&gt; **课堂思考**：
- X值固定不变现实么？为什么要假设这种情形？
- 如果X是随机变量，仍旧坚持用OLS方法，得到的结果还是不变么？

---

### 样本数据示例



扁数据形态：“非标准”数据形态（但很直观）

<div id="htmlwidget-2cb5d2cf276a46f04f27" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2cb5d2cf276a46f04f27">{"x":{"filter":"none","caption":"<caption>60个家庭的收入和支出情况：假设的总体<\/caption>","data":[["1","2","3","4","5","6","7","8"],["X","Y1","Y2","Y3","Y4","Y5","Y6","Y7"],[80,55,60,65,70,75,null,null],[100,65,70,74,80,85,88,null],[120,79,84,90,94,98,null,null],[140,80,93,95,103,108,113,115],[160,102,107,110,116,118,125,null],[180,110,115,120,130,135,140,null],[200,120,136,140,144,145,null,null],[220,135,137,140,152,157,160,162],[240,137,145,155,165,175,189,null],[260,150,152,175,178,180,185,191]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Mark<\/th>\n      <th>G1<\/th>\n      <th>G2<\/th>\n      <th>G3<\/th>\n      <th>G4<\/th>\n      <th>G5<\/th>\n      <th>G6<\/th>\n      <th>G7<\/th>\n      <th>G8<\/th>\n      <th>G9<\/th>\n      <th>G10<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"tip","columnDefs":[{"className":"dt-center","targets":"_all"},{"visible":false,"targets":0},{"orderable":false,"targets":0}],"pageLength":9,"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[9,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

### 样本数据示例

长数据形态：标准数据形态（但不直观）

<div id="htmlwidget-1371c7d95c00e86f1af1" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-1371c7d95c00e86f1af1">{"x":{"filter":"none","caption":"<caption>60个家庭的收入和支出情况：假设的总体<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60"],[1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,8,8,8,8,8,8,8,9,9,9,9,9,9,10,10,10,10,10,10,10],[80,80,80,80,80,100,100,100,100,100,100,120,120,120,120,120,140,140,140,140,140,140,140,160,160,160,160,160,160,180,180,180,180,180,180,200,200,200,200,200,220,220,220,220,220,220,220,240,240,240,240,240,240,260,260,260,260,260,260,260],[55,60,65,70,75,65,70,74,80,85,88,79,84,90,94,98,80,93,95,103,108,113,115,102,107,110,116,118,125,110,115,120,130,135,140,120,136,140,144,145,135,137,140,152,157,160,162,137,145,155,165,175,189,150,152,175,178,180,185,191]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>group<\/th>\n      <th>X<\/th>\n      <th>Y<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"tip","columnDefs":[{"className":"dt-center","targets":"_all"},{"visible":false,"targets":0},{"orderable":false,"targets":0}],"pageLength":7,"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[7,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>




---

### 样本数据示例

- X为**固定值**情形下，两份随机样本：

&lt;table class="table" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; var &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G4 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G5 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G6 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G7 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G8 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G9 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G10 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr grouplength="2"&gt;&lt;td colspan="11" style="background-color: #666; color: #fff;"&gt;&lt;strong&gt;样本1&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; X1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; Y1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 65 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 88 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 107 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 165 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 185 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr grouplength="2"&gt;&lt;td colspan="11" style="background-color: #666; color: #fff;"&gt;&lt;strong&gt;样本2&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; X2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; Y2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 108 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 136 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 155 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 178 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

### 样本数据示例

- X为**随机变量**时，两份随机样本：

&lt;table class="table" style="margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; var &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G4 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G5 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G6 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G7 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G8 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G9 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; G10 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr grouplength="2"&gt;&lt;td colspan="11" style="background-color: #666; color: #fff;"&gt;&lt;strong&gt;样本3&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; X3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; Y3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 103 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 102 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 110 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 137 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 165 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 191 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr grouplength="2"&gt;&lt;td colspan="11" style="background-color: #666; color: #fff;"&gt;&lt;strong&gt;样本4&lt;/strong&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; X4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left; padding-left: 2em;" indentlevel="1"&gt; Y4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 79 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 110 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 137 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 189 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 185 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## 关于随机干扰项的假设1


**CLRM假设4（随机干扰项条件期望值为零）**：假设随机干扰项条件期望值为零。也即给定
`\(X_i\)`的情形下，假定随机干扰项
`\(u_i\)`的**条件期望**为零。

`$$\begin{align}
E(u|X_i)= 0
\end{align}$$`


&lt;img src="pic/chpt3-CLRM-mean0.jpg" width="454" style="display: block; margin: auto;" /&gt;

---

### 课堂讨论：

**条件期望**
`\(E(u|X_i)= 0\)` 比**无条件期望**
`\(E(u)\)`的假设要更严格：

&lt;img src="pic/chpt03-hypothesis-disturbance-1.png" width="633" style="display: block; margin: auto;" /&gt;


&gt; - 此时，在
`\(X\)`的特定范围内
`\(E(u|X_i) \neq 0\)`；但是对于全部的
`\(X\)`则有
`\(E(u)=0\)`。
&gt; - 实际的数据是由如下生成机制（DGP）模拟得到：
`\(Y_i=25 +5X_i(1+2X_i)+u_i\)`。

---

### 课堂讨论：

- 讨论1：这条假设是多余的么？——因为前面已经假定
`\(X_i\)`与
`\(u_i\)`不相关（也即，
`\(E(X_i, u_i)=0\)`见**CLRM假设3**），是不是就必然
`\(E(u|X_i)= 0\)`？

--

&gt; 答案：如果
`\(X_i\)`与
`\(u_i\)`**相关**（也即
`\(E(X_i, u_i) \neq 0\)`），则随机干扰项的条件期望往往不等于零（也即
`\(E(u|X_i) \neq 0\)`）；同时，如果
`\(X_i\)`与
`\(u_i\)`**不相关**（也即
`\(E(X_i, u_i) = 0\)`），则随机干扰项的条件期望也可能等于零（也即
`\(E(u|X_i) = 0\)`）。因此这一条假设是必须的。

--

- 讨论2：若自变量
`\(X_i\)`为**随机变量**，我们还应当假定随机干扰项
`\(u_i\)`的**无条件期望**为零么？也即能否假定
`\(E(u_i)=0\)`？

---

## 关于随机干扰项的假设2

**CLRM假设5（随机干扰项的方差为同方差）**：随机干扰项的方差为同方差。也即给定
`\(X_i\)`的情形下，随机干扰项
`\(u_i\)`的方差，处处都是相等的。记为：

`$$\begin{align}
Var(u_i|X_i) &amp; = E \left[ \left( u_i -E(u_i) \right)^2|X_i \right] \\
&amp; = E(u_i^2|X_i) \\
&amp; = E(u_i^2) \\
&amp; \equiv \sigma^2
\end{align}$$`

---

### 随机干扰项的方差为同方差

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-homoscedasticity.png" alt="随机干扰项的方差处处相等" width="608" /&gt;
&lt;p class="caption"&gt;随机干扰项的方差处处相等&lt;/p&gt;
&lt;/div&gt;

---

### 随机干扰项的方差为同方差

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-heteroscedasiticity.png" alt="随机干扰项的方差随X取值不同而不同" width="599" /&gt;
&lt;p class="caption"&gt;随机干扰项的方差随X取值不同而不同&lt;/p&gt;
&lt;/div&gt;

---

### 随机干扰项的方差为同方差

- 同方差性(homoscedasticity) ：
`\(Var(u_i|X_i) \equiv \sigma^2\)`

- 异方差性(heteroscedasticity) ：
`\(Var(u_i|X_i) \equiv \sigma_i^2\)`

&lt;/br&gt;

&gt; **课堂讨论**：
&gt;
- 讨论1: 如果
`\(Var(u_i|X_1) &lt; Var(u_2|X_2)\)`，是否意味着来自
`\(X=X_1\)`的总体，相比来自
`\(X=X_2\)`的总体，更靠近总体回归线PRL?

&gt; - 讨论2：如何看待**随机样本**的质量？或者，那些离均值较近的Y总体的随机样本，与远为分散的Y总体的随机样本，前者是不是质量更好?

&gt; - 讨论3：此时，
`\(Y_i\)`的条件方差
`\(Var(Y_i|X_i)\)`是多少？
`\(Y_i\)`的无条件方差
`\(Var(Y_i)\)`又是多少？

&gt; - 讨论4：如果出现异方差，会对OLS估计产生什么后果？

---

## 关于随机干扰项的假设3


**CLRM假设6（随机干扰项之间无自相关）**：各个随机干扰之间无自相关。也即给定两个不同的自变量取值（
`\(X_i,X_j;i \neq j\)`）情形下，随机干扰项
`\(u_i,u_j\)`的相关系数为0。或者说
`\(u_i,u_j\)`最好是相互独立的。记为：

在
`\(X_i\)`为给定情形下，且
`\(i,j \in (1, 2, \cdots, n); i \neq j\)`，假定：

`$$\begin{align}
Cov(u_i, u_j|X_i,X_j) &amp; = E \left[ \left( u_i -E(u_i) \right)\left( u_i -E(u_i) \right) \right]  \\
&amp; = E(u_iu_j) \\
&amp; \equiv 0
\end{align}$$`


重要概念区别：

- 无**序列相关**(no serial correlatìon)：

- 无**自相关**(no autocorrelation)：

`$$\begin{align}
Y_t &amp;= \beta_1 + \beta_2X_t + u_t \\
Y_t &amp; = \beta_1 + \beta_2X_{2t} + \beta_2X_{3t} + u_t 
\end{align}$$`

---

### 课堂讨论:

.pull-left[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-correlation.png" alt="随机干扰项之间的相关情形" width="486" /&gt;
&lt;p class="caption"&gt;随机干扰项之间的相关情形&lt;/p&gt;
&lt;/div&gt;
]

--
.pull-right[
课堂讨论：

- 讨论1：该假设的目的和用处是什么？

- 讨论2：如果出现自相关，会对OLS估计产生什么影响？

]

---

## 关于样本数的要求

**CLRM假设7（观测样本数假设）**：观测次数n，要大于待估计参数个数。否则方程无法解出，参数不能估计出来。

---

### 小结与讨论

下面我们对本小节内容做一个总结和讨论：

- 所有这些假设有多真实?

&gt; “假定无关紧要论”——弗里德曼


- 上述说有假设都是针对PRF，而不是SRF！

&gt; 例如：PRF中随机干扰项有无自相关的假设
`\(Cov(u_i, u_j)=0\)`；但是在SRF中，可能就会出现
`\(Cov(e_i, e_j) \neq 0\)`

- 前面提到的OLS方法正是试图“复制”CLRM的假设！

&gt; OLS方法中，
`\(\sum{e_iX_i}=0\)`，就类似于自变量X与随机干扰项不相关的假设（也即
`\(Cov(u_i,X_i)=0\)`）。

&gt; OLS方法中，
`\(\sum{e_i}=0,(\bar{e_i}=0)\)`，就类似于随机干扰项期望值为0的假设（也即
`\(E(u|X_i)=0\)`）


---

### 小结与讨论

**思考1**：CLRM假设本质上是在讨论什么？

--

&gt; **回答**：数据是依据什么机制产生的？(data-generating process, DGP)
- 我们手头只有
`\(n\)`个
样本数据对
`\((Y_i,X_i)\)`。
- 但是我们希望能得到对总体**参数集**
`\(\Phi\)`的合理推断。
- 因此，如果不对总体回归模型（PRM）作任何假设的话，我们就没有更多的信息，来对总体参数集进行任何有价值的推断。

&lt;/br&gt;

--

**思考2**：CLRM假设既然有很多地方明显不符合现实，那么我们可以**放宽**这些假设么？如果现实根本就是**违背**了CLRM假设，OLS方法又将何去何从？对于参数估计量的性质会造成致命性的打击么？

--

&gt; **回答**：**放宽**CLRM假设和**违背**CLRM假设的后果是不同的。
- 如果只是**放宽**CLRM假设，则不会影响OLS方法的参数估计量的BLUE性质。
- 但是如果是**违背**了CLRM假设，则OLS方法的参数估计量的BLUE性质很可能无法保持！



---
layout: false
class: inverse, center, middle, duke-softblue
name: BLUE

# 3.4 最小二乘估计量的性质

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.4 最小二乘估计量的性质 &lt;/span&gt;&lt;/div&gt; 

---

### 引子：OLS方法怎么就“天生丽质”了？

我们已经知道了，OLS方法和CLRM假设“双剑合璧”下，参数估计量是最优线性无偏估计量（BLUE）。

.pull-left[

问题是：

- OLS拟合估计方法很有“特点”，是不是意味着它就很“优秀”呢？

- 我们怎么知道，OLS方法和CLRM假设“双剑合璧”就是所向披靡呢？

- 同样在CLRM假设下，有没有一种不同于OLS的其他估计方法，也是同样那么优秀，甚至更好呢？

]

.pull-right[

&gt; 参数估计量的可能行为：

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-BLUE-demo-manu.png" alt="估计量的性质的一个图形说明" width="463" /&gt;
&lt;p class="caption"&gt;估计量的性质的一个图形说明&lt;/p&gt;
&lt;/div&gt;

]


---

### 引子：OLS方法怎么就“天生丽质”了？

某种参数**估计方法**（如OLS方法），得到的**估计量**（如
`\(\hat{\beta}_2,\hat{\beta}_1, \hat{\sigma}^2\)`）是总体**参数**（如
`\(\beta_2, \beta_1, \sigma^2\)`）的**最优线性无偏估计量**（**B**est **L**inear **U**nbiased **E**stimate，**BLUE**）需要满足如下三个条件：

- 线性的(Linear)：估计量是因变量
`\(Y_i\)`的线性函数。

- 无偏的(Unbiased)：**估计量**的均值或期望值（
`\(E(\hat{\beta}_i)\)`）等于**参数**的真值（
`\(\beta_i\)`）。

- 方差最小的（Best）：也即估计量是最有效的(Efficient)，是所有线性无偏估计量中有最小方差的那个估计量。

我们下面将证明：OLS方法在给定条件下就是那么“天生丽质”！

&gt; - 用记号表达为：
`\(\hat{\Phi} \text{ of} \overset{OLS}{\underset{CLRM}{\Longrightarrow}} \Phi  \text{ is BLUE}\)`

&gt; - 以上表达读作：在经典详细回归模型假设下（CLRM），采用普通最小二乘法（OLS），得到参数
`\(\Phi\)`的估计量
`\(\hat{\Phi}\)`，是最优线性无偏估计量（BLUE）。


---

## 高斯-马尔可夫定理(Gauss-Markov Theorem)：

**高斯-马尔可夫定理**(Gauss-Markov Theorem)：在给定经典线性回归模型(CLRM)的假定下，最小二乘(OLS)**估计量**（如
`\(\hat{\beta}_2,\hat{\beta}_1,\hat{\sigma}^2\)`），在无偏线性估计量一类中，有最小方差，就是说它们是**总体参数**（如
`\(\beta_2, \beta_1, \sigma^2\)`）的**最优线性无偏估计量**(BLUE)。

--

课堂讨论：

- 讨论1：为什么最小二乘法（OLS）被计量学家奉为神明？还有其他选择吗？

- 讨论2：OLS得到的BLUE为到底有什么值得你称赞？

- 讨论3：OLS得到BLUE还需要CLRM假设以外的更多假设吗？(正态性？？)

---

## OLS方法最优线性无偏估计性质的证明

不同估计方法下两个估计量的抽样分布

.pull-left[

&lt;img src="pic/chpt3-BLUE-demo.png" width="304" style="display: block; margin: auto;" /&gt;
]

.pull-right[

- 图(a) **OLS方法**下估计量
`\(\hat{\beta}_2\)`是总体参数
`\(\beta_2\)`的一个**线性无偏估计量**

- 图(b) **其他某种方法**下估计量
`\(\hat{\beta}_2^{\ast}\)`也是总体参数
`\(\beta_2\)`的一个**线性无偏估计量**

- 图(c)  那么哪一个估计量（
`\(\hat{\beta}_2\)`还是
`\(\hat{\beta}_2^{\ast}\)`）更能为我们所接受呢？

课程讨论：

- 讨论1：什么是抽样分布？

- 讨论2：怎样获得估计量分布？

- 讨论3：没有比OLS估计量更好的估计量吗？
]

---

## 性质1：线性性

**线性性**（Linearity）：是指
`\(\hat{\beta}_2\)`和
`\(\hat{\beta}_1\)`对
`\(Y_i\)`是线性的。

具体证明过程如下：

**步骤1**：证明斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} &amp;&amp; \leftarrow  \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right] \\
\end{align}$$`

又因为
`\(k_i =\frac{x_i}{\sum{x_i^2}}\)`是不全为0的（为什么？），所以斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

---

## 性质1：线性性

详细证明（反证法）：

- 假设
`\(H_0\)`：
`\(k_i =\frac{x_i}{\sum{x_i^2}}= 0\)`，也即全为零。

- 则有：
`\(x_i= X_i-\bar{X}=0\)`，

- 则有：
`\(X_i\)`处处等于
`\(\bar{X}\)`，

- 也就意味着：
`\(x_i\)`是一个不变的量（只有一个取值）

- 因此，这是明显违背CLRM假设中关于自变量
`\(X_i\)`的设定（见前面）。

- 因此，
`\(H_0\)`是显然不成立的，认为
`\(k_i\)`不能全为零。[证明完毕]


---

## 性质1：线性性

**步骤2**：证明截距系数估计量
`\(\hat{\beta}_1\)`对
`\(Y_i\)`是线性的。

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{w_iY_i} &amp;&amp; \leftarrow \left[ w_i = \frac{1}{n} - k_i\bar{X} \right]
\end{align}$$`

又因为
`\(w_i = \frac{1}{n} - k_i\bar{X}\)`是不全为0的（为什么？），所以斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

---

## 性质1：线性性

详细证明（反证法）：

- 假设
`\(H_0\)`：
`\(w_i = \frac{1}{n} - k_i\bar{X}= 0\)`，也即全为零。

- 则有：
`\(\sum{w_i} = \sum{(\frac{1}{n} - k_i\bar{X})}=0\)`，

- 则有：
`\(1-\bar{X}\sum{k_i}=0\)`，

- 又因为：
`\(\sum{k_i} = \sum{\frac{x_i}{\sum{x_i^2}}=\frac{\sum{x_i}}{\sum{x_i^2}} }=0\)`，

- 因此有：
`\(1-0=0\)`，也即
`\(1=0\)`

- 因此，这显然是错误的。

- 因此
`\(H_0\)`是显然不成立的，认为
`\(w_i\)`不能全为零。[证明完毕]

---

## 性质2：无偏性

**无偏性**(Unbias)：**估计量**期望值（
`\(E(\hat{\beta}_i)\)`）等于**参数**的真值（
`\(\beta_i\)`）。

**步骤1**：证明斜率系数估计量
`\(\hat{\beta}_2\)`是无偏的，也即
`\(E(\hat{\beta}_2)= \beta_2\)`。

容易有：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} &amp;&amp; \leftarrow  \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right] \\
&amp; = \sum{k_i(\beta_1 +\beta_2X_i +u_i)} \\
&amp; = \beta_1\sum{k_i}+\beta_2\sum{k_iX_i} + \sum{k_iu_i}\\
&amp; = 0 + \beta_2 +\sum{k_iu_i}
\end{align}$$`


---

## 性质2：无偏性

&gt; （续前）因为有：

`$$\begin{align}
\sum{k_i}  &amp;= \sum{\frac{x_i}{\sum{x_i^2}}=\frac{\sum{x_i}}{\sum{x_i^2}} }=0 \\
 \sum{k_iX_i} 
&amp;= \sum{\left[ \frac{x_i}{\sum{x_i^2} }\cdot X_i \right]} 
= \frac{ \sum{x_iX_i}} {\sum{x_i^2}} 
= \frac{ \sum{x_i(x_i+ \bar{X})}} {\sum{x_i^2}} \\
&amp;= \frac{ \sum{x_i^2}+\sum{x_i \bar{X}}} {\sum{x_i^2}} 
= \frac{ \sum{x_i^2}+\bar{X}\sum{x_i} } {\sum{x_i^2}}
= 1
\end{align}$$`

&gt; 所以有：

`$$\begin{align}
\hat{\beta}_2 &amp; =  \beta_2 +\sum{k_iu_i} \\
E(\hat{\beta}_2) &amp; =  E(\beta_2 +\sum{k_iu_i}) 
 =  \beta_2 +E(\sum{k_iu_i}) \\
 &amp; =  \beta_2 +\sum{\left[ k_iE(u_i) \right]} 
 =  \beta_2
\end{align}$$`


???
**[证明完毕]**！

---

## 性质2：无偏性

**步骤2**：证明截距系数估计量
`\(\hat{\beta}_1\)`是无偏的，也即
`\(E(\hat{\beta}_1)= \beta_1\)`。

&gt; 容易有：

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{w_iY_i} &amp;&amp; \leftarrow \left[ w_i = \frac{1}{n} - k_i\bar{X} \right] \\
&amp; = \sum{w_i(\beta_1 +\beta_2X_i +u_i)} \\
&amp; = \beta_1\sum{w_i} + \beta_2\sum{w_iX_i} + \sum{w_iu_i}\\
&amp; = \beta_1 + 0 +\sum{k_iu_i}
\end{align}$$`

---

## 性质2：无偏性

&gt; （续前）因为有：

`$$\begin{align}
&amp;\sum{w_i}  = \sum{ \left[ \frac{1}{n} - k_i\bar{X} \right]}
= 1- \bar{X}\sum{k_i} 
= 1 \\
&amp; \sum{w_iX_i} 
= \sum{\left[ \left( \frac{1}{n} - k_i\bar{X} \right) \cdot X_i \right] }
= \sum{\left( \frac{X_i}{n} - \bar{X} k_i X_i \right) }
= \bar{X} -\bar{X}\sum{( k_i X_i) }
= 0
\end{align}$$`

&gt; 所以有：

`$$\begin{align}
\hat{\beta}_1 &amp; =  \beta_2 +\sum{w_iu_i} \\
E(\hat{\beta}_1) &amp; =  E(\beta_1 +\sum{k_iu_i}) \\
&amp; =  \beta_1 +E(\sum{k_iu_i}) \\
&amp; =  \beta_1 +\sum{\left[ k_iE(u_i) \right]} \\
&amp; =  \beta_1
\end{align}$$`

**[证明完毕]**！

---

## 性质3：方差最小性

**方差最小性**（Best）：也即估计量是最有效的(Efficient)，是所有线性无偏估计量中，方差为最小的那个估计量。

&gt; 证明：

- 已知估计量
`\(\hat{\beta}_2\)`和
`\(\hat{\beta}_1\)`的**总体方差**分别是：

`$$\begin{align}
Var(\hat{\beta}_2) \equiv \sigma_{\hat{\beta}_2}^2  &amp;=\frac{\sigma^2}{\sum{x_i^2}} \\
Var(\hat{\beta}_1) \equiv \sigma_{\hat{\beta}_1}^2  &amp;=\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}  
\end{align}$$`

---

## 性质3：方差最小性

假设存在用其他方法估计的线性无偏估计量
`\(\hat{\beta}_2^{\ast}\)`和
`\(\hat{\beta}_1^{\ast}\)`：

`$$\begin{align}
\hat{\beta}_2^{\ast} = \sum{\left[ (k_i + d_i)Y_i \right]} = \sum{c_iY_i}\\
\hat{\beta}_1^{\ast} = \sum{\left[ (w_i + g_i)Y_i \right]} = \sum{h_iY_i}
\end{align}$$`

其中，
`\(d_i\)`和
`\(g_i\)`为不全为零的常数（证明略），则可以证明（此处略）：

`$$\begin{align}
Var(\hat{\beta}_2^{\ast} ) \geq Var(\hat{\beta}_2)  \\
Var(\hat{\beta}_1^{\ast}) \geq  Var(\hat{\beta}_1) 
\end{align}$$`

因此，方差最小性得以证明！

---

### 小结与讨论

**小结**：

- 评价不同估计方法的**参数估计量性质**，一般是从线性的(Linear)、无偏性(Unbiased)和有效性（方差最小性，Best）三个维度来共同测量。

- OLS估计方法，在CLRM假设下，其参数估计量很好地满足如上三个性质，因此我们称OLS方法估计的参数估计量是最优线性无偏估计量（BLUE）。

**思考**：

- 关于OLS估计量**有效性**（方差最小性，best）的证明过程你满意么？你能不能自己查阅资料证明一下？找到自己满意的证明过程！
&gt; 参考答案：建议参阅Greene的《计量经济分析》，他采用的矩阵方法做了完美的证明！

- 还有没有**其他维度**来评价不同估计方法的**参数估计量性质**？
&gt; 参考答案：还可以从“一致性”（consistency）维度来评价，主要考察参数估计量的**渐进性质**，也即在样本不断接近总体时估计量的表现。

- 使得参数估计量具备BLUE性质，仅有OLS方法么（独孤求败）？你能说出一个么？



---
layout: false
class: inverse, center, middle, duke-softblue
name: ANOVA

# 3.5 变异分解与拟合优度

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.5 变异分解拟合优度 &lt;/span&gt;&lt;/div&gt; 

---

### 引子：怎么来判定OLS方法对特定样本数据拟合的好坏？

请大家思考如下几个**问题**：

- 样本数据不完全落在拟合的直线（或曲线）上，是经常发生的么？

- 怎么来表达或测量这种对样本数据拟合的不完全性？

- 在OLS方法和CLRM假设“双剑合璧”下，对特定样本数据的拟合不是已经证明最好的么（BLUE）？为什么还要说“拟合”有“好坏之分”？



---

## Y变异的分解

&lt;img src="pic/chpt2-1-PRL-SRL.png" width="601" style="display: block; margin: auto;" /&gt;


`$$\begin{alignedat}{2}
&amp;&amp;(Y_i - \bar{Y}) &amp;&amp;= (\hat{Y}_i - \bar{Y}) &amp;&amp;+ (Y_i - \hat{Y}_i ) \\
&amp;&amp;y_i &amp;&amp;= \hat{y}_i &amp;&amp;+ e_i 
\end{alignedat}$$`

---

## 平方和分解

`$$\begin{alignedat}{2}
&amp;&amp;(Y_i - \bar{Y}) &amp;&amp;= (\hat{Y}_i - \bar{Y}) &amp;&amp;+ (Y_i - \hat{Y}_i ) \\
&amp;&amp;y_i &amp;&amp;= \hat{y}_i &amp;&amp;+ e_i \\
&amp;&amp;\sum{y_i^2} &amp;&amp;= \sum{\hat{y}_i^2} &amp;&amp;+ \sum{e_i^2} \\
&amp;&amp;TSS &amp;&amp;=ESS &amp;&amp;+RSS
\end{alignedat}$$`

- 其中：
`\(TSS\)`表示**总离差平方和**;
`\(ESS\)`表示**回归平方和**;
`\(RSS\)`表示**残差差平方和**

`$$\begin{align}
\sum{y_i^2} &amp;= \sum{(\hat{y}_i e_i)^2} \\
&amp;= \sum{(\hat{y}_i^2 +2\hat{y}_ie_i +e_i^2)}\\
&amp;= \sum{\hat{y}_i^2 } +2\sum{\hat{y}_ie_i} + \sum{e_i^2}\\
&amp;= \sum{\hat{y}_i^2 } +2\sum{\left( \hat{(\beta_2}x_i)e_i \right)} + \sum{e_i^2}\\
&amp;= \sum{\hat{y}_i^2 } +2\hat{\beta_2}\sum{\left( x_ie_i \right)} + \sum{e_i^2} &amp;&amp; \leftarrow \left[ \sum{x_ie_i} =0 \right]\\ 
&amp;= \sum{\hat{y}_i^2} + \sum{e_i^2}
\end{align}$$`

---

## 拟合优度的度量


**拟合优度**（Goodness of fit）：判断样本回归线对一组数据拟合优劣水平的度量。

**判定系数**（coefficient of determination）：一种利用平方和分解，考察样本回归线对数据拟合效果的总度量。一元回归中，一般记为
`\(r^2\)`；多元回归中，一般记为
`\(R^2\)`。

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-fitness-venn.png" alt="维恩图看拟合优度" width="549" /&gt;
&lt;p class="caption"&gt;维恩图看拟合优度&lt;/p&gt;
&lt;/div&gt;

---

## 拟合优度的度量

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt2-1-PRL-SRL.png" alt="平方和分解看拟合优度" width="651" /&gt;
&lt;p class="caption"&gt;平方和分解看拟合优度&lt;/p&gt;
&lt;/div&gt;

---

## 拟合优度的度量

判定系数
`\(r^2\)`计算公式1：

`$$\begin{align}
r^2 &amp;=\frac{ESS}{TSS} = \frac{\sum{(\hat{Y}_i - \bar{Y})^2}}{\sum{(Y_i - \bar{Y})^2}} 
\end{align}$$`

判定系数
`\(r^2\)`计算公式2：

`$$\begin{align}
r^2 &amp;=1- \frac{RSS}{TSS} = 1- \frac{\sum{e_i^2}}{\sum{(Y_i - \bar{Y})^2}} \\
\end{align}$$`

---

## 拟合优度的度量

判定系数
`\(r^2\)`计算公式3：

`$$\begin{align}
r^2 &amp;=\frac{ESS}{TSS} 
= \frac{\sum{\hat{y}_i^2}}{\sum{y_i^2}} 
= \frac{\sum{(\hat{\beta}_2x_i)^2}}{\sum{y_i^2}} 
= \hat{\beta}_2^2\frac{\sum{x_i^2}}{\sum{y_i^2}} 
= \hat{\beta}_2^2 \frac{S_{X_i}^2}{S_{Y_i}^2}
\end{align}$$`

判定系数
`\(r^2\)`计算公式4：

`$$\begin{align}
r^2 &amp;= \hat{\beta}_2^2 \cdot \frac{\sum{x_i^2}}{\sum{y_i^2}} 
= \left( \frac{\sum{x_iy_i}}{\sum{x_i^2}} \right)^2 \cdot \left( \frac{\sum{x_i^2}}{\sum{y_i^2}} \right)
= \frac{(\sum{x_iy_i})^2}{\sum{x_i^2 }\sum{y_i^2}}
\end{align}$$`

课堂讨论：

- 讨论1： 
`\(r^2\)`是一个非负量。为什么？

- 讨论2：
`\(0 \leq r^2 \leq 1\)`，两个端值分别意味什么？

---

## 判定系数和简单相关系数的区别与联系

**总体相关系数**：是变量
`\(X_i\)`与变量
`\(Y_i\)`总体相关关系的参数，一般记为
`\(\rho\)`。

`$$\begin{align}
\rho &amp;=\frac{Cov(X,Y)}{\sqrt{Var(X_i)Var(Y_i)}}
=\frac{E(X_i-EX)(Y_i-EY)}{\sqrt{E(X_i-EX)^2E(Y_i-EY)^2}}
\end{align}$$`

**样本相关系数**：是从总体中抽取随机样本，获得变量
`\(X_i\)`与变量
`\(Y_i\)`样本相关关系的统计量度量，一般记为
`\(r\)`。

`$$\begin{align}
r &amp;=\frac{S_{XY}^2}{S_X\ast S_Y}
=\frac{\sum{(X_i-\bar{X})(Y_i-\bar{Y})}}{\sqrt{\sum{(X_i-\bar{X}})^2\sum{(Y_i-\bar{Y})^2}}}
= \frac{\sum{x_iy_i}}{\sqrt{\sum{x_i^2 }\sum{y_i^2}}}
\end{align}$$`

---

## 判定系数和相关系数的区别与联系

判定系数和简单相关系数的联系:

- 在一元回归中，判定系数
`\(r^2\)`等于样本相关系数
`\(r\)`的平方。

判定系数和简单相关系数的区别：

- 判定系数
`\(r^2\)`表明因变量变异由解释变量所解释的比例，而相关系数
`\(r\)`只能表明变量间的线性关联强度。

- 在多元回归中，这种区别会更加凸显！因为那时的相关系数r出现了偏相关的情形(交互关联)！

---

### 小结与思考

本节**内容小结**：

- 即使采用OLS方法，它对样本数据的拟合也是不完全的。意味着实际数据点在样本回归线附近，而不是在样本回归线上。我们可以把样本点行为的“变异”，划分为“回归”能解释的部分和“随机”的部分。并进一步获得变异平方和的分解。

- 判定系数
`\(R^2\)`是对OLS拟合程度的测量，它使用了变异平方和分解的思想。在一元线性回归（含截距）中，判定系数与相关系数存在如下关系
`\(R^2 = r^2_{(X_i,Y_i)}\)`。注意，在多元回归中则不存在这种关系。

**问题与思考**：

- OLS方法的参数估计量，在CLRM假设满足情况下，就是最优线性无偏估计量（BLUE），为什么还要用**判定系数**来判断“拟合好还是不好？”。对此，你的回答是什么？

- 还有没有其他指标，来反映估计方法对样本数据的拟合好坏程度？请说出一两个。

&gt; 参考答案：还可以有**均方误差和**（MSE）
`\(MSE=RSS/n= 1/n\sum{(Y_i - \hat{Y}_i})^2\)`，以及**均方误差根**（RMSE）等。

---
layout: false
class: inverse, center, middle, duke-softblue
name: numerical-case

# 3.6 一个数值例子

???
受教育程度（
`\(X_i\)`，年）与时均工资(
`\(Y_i\)`，美元/小时)。


---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.6 一个数值例子 &lt;/span&gt;&lt;/div&gt; 

---

### 引子：如何把计量分析思想的进行软件验证？

**说一千道一万**，重要的是我们自己能不能动手，利用统计软件对前述的计量分析理论进行计算和验证！

下面，我们利用样本数据对**教育和工资案例**进行一次完整的计算和验证吧！

&gt; **教育和工资案例**的总体回归模型（PRM）如下：

`$$\begin{align}
Wage_i &amp; = \beta_1 + \beta_2 Edu_i +u_i \\
Y_i &amp; = \beta_1 + \beta_2 X_i +u_i \\
\end{align}$$`



---

### 计算表FF和ff





&lt;table class="table" style="font-size: 20px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:center;"&gt; obs &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(X_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(Y_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(X_iY_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(X_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(Y_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(y_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(x_iy_i\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(x_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:center;"&gt; `\(y_i^2\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.46 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 26.74 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 19.86 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -6.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -4.22 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 25.31 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 17.79 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5.77 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 40.39 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 49.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 33.29 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -5.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -2.90 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14.52 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 25.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8.44 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5.98 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 47.83 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 64.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 35.74 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -4.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -2.70 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10.78 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.27 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.33 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 65.99 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 81.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 53.75 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -3.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -1.34 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.03 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1.80 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.32 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 73.18 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 100.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 53.56 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -2.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -1.36 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.71 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1.84 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 11.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6.58 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 72.43 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 121.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 43.35 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -1.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -2.09 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.37 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 12.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.82 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 93.82 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 144.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61.12 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -0.86 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -0.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.73 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 7.84 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 101.86 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 169.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 61.39 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -0.84 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; -0.84 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.70 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 14.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 11.02 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 154.31 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 196.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 121.49 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.35 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.70 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5.51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 15.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10.67 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 160.11 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 225.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 113.93 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 3.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 10.84 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 173.38 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 256.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 117.42 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2.16 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 8.65 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.67 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 17.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13.62 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 231.46 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 289.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 185.37 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 5.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.94 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 24.70 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 25.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 24.41 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 18.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 13.53 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 243.56 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 324.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 183.09 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 4.86 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 29.14 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 23.58 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:center;"&gt; sum &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 156.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 112.77 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1485.04 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 2054.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 1083.38 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 131.79 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 182.00 &lt;/td&gt;
   &lt;td style="text-align:center;"&gt; 105.12 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## 计算回归系数

公式1: （Favorite Five，FF形式）

`$$\begin{align}
\hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
&amp;=\frac{13\ast1485.04-156\ast112.771}{13\ast2054-156^2}=0.7241
\end{align}$$`


`$$\begin{align}
\hat{\beta_1} &amp;= \bar{Y} - \hat{\beta}_2 \bar{X}
=8.6747-0.7241\ast12=-0.0145
\end{align}$$`

---

## 计算回归系数

公式2：（离差形式，favorite five，ff形式）


`$$\begin{align}
\hat{\beta}_2 =\frac{\sum{x_iy_i}}{\sum{x_i^2}}
=\frac{131.786}{182}=0.7241
\end{align}$$`


`$$\begin{align}
\hat{\beta_1} = \bar{Y} - \hat{\beta}_2 \bar{X}
=8.6747-0.7241\ast12=-0.0145
\end{align}$$`

---

## 样本回归方程SRF

`$$\begin{align}
\hat{Y}_i= \hat{\beta}_1 + \hat{\beta}_2 X_i
=-0.0145+0.7241X_i
\end{align}$$`

---

## 样本回归线SRL

&lt;img src="03-simple-reg-parameter-estimate-slide_files/figure-html/unnamed-chunk-28-1.png" style="display: block; margin: auto;" /&gt;

---

## 样本回归线SRL

&lt;img src="03-simple-reg-parameter-estimate-slide_files/figure-html/unnamed-chunk-29-1.png" style="display: block; margin: auto;" /&gt;


---

### 计算得到拟合值和残差

.pull-left[

&lt;table class="table" style="font-size: 20px; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; obs &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(X_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(Y_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(\hat{Y}_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(e_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(e_i^2\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.4567 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.3301 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1266 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.7700 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.0542 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.7158 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5123 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.9787 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.7783 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.2004 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0402 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.3317 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.5024 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8293 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.6877 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.3182 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.2265 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0917 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0084 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.5844 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.9506 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.3662 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.8665 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.8182 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.6747 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.8565 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.7336 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.8351 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.3988 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.5637 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.4452 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 14.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.0223 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.1229 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8994 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8089 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 15.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.6738 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.8470 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.1732 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 16.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.8361 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.5711 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.7350 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5402 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 17.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.6150 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12.2952 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.3198 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.7419 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 18.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.5310 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.0193 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5117 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.2618 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sum &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 156.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 112.7712 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 112.7712 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.6928 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

]

.pull-right[

根据以上样本回归方程，可以计算得到
`\(Y_i\)`的回归拟合值
`\(\hat{Y}_i\)`，以及回归残差
`\(e_i\)`。

`$$\begin{align}
\hat{Y}_i &amp;=\hat{\beta}_1 +\hat{\beta}_2X_i\\
e_i &amp;= Y_i - \hat{Y}_i
\end{align}$$`
]




---

## 回归误差方差和标准差

回归误差方差
`\(\hat{\sigma}^2\)`

`$$\begin{align}
\hat{\sigma}^2= \frac{\sum{e_i^2}} {(n-2)}
=\frac{9.693}{11}=0.8812
\end{align}$$`

回归误差标准差
`\(\hat{\sigma}\)`：

`$$\begin{align}
\hat{\sigma}=\sqrt{\frac{\sum{e_i^2}}{(n-2)}}
=\sqrt{0.8812}=0.9387
\end{align}$$`

---

## 计算回归系数的样本方差

`$$\begin{align}
S^2_{\hat{\beta}_2} &amp;= \frac{\hat{\sigma}^2} {\sum{x_i^2}}
=\frac{0.8812}{182}=0.0048\\
S_{\hat{\beta}_2} &amp;= \sqrt{\frac{\hat{\sigma}^2} {\sum{x_i^2}}}
=\sqrt{0.0048}=0.0696
\end{align}$$`

`$$\begin{align}
S^2_{\hat{\beta}_1} &amp;= \frac{\sum{X_i^2}} {n} \frac{\hat{\sigma}^2} {\sum{x_i^2}}
=\frac{2054}{13}\frac{0.8812}{182}=0.765\\
S_{\hat{\beta}_1} &amp;= \sqrt{\frac{\sum{X_i^2}}{n}\frac{\hat{\sigma}^2} {\sum{x_i^2}}}
=\sqrt{0.765}=0.8746
\end{align}$$`

---

## 计算平方和分解

`$$\begin{align}
TSS &amp;= \sum{(Y_i- \bar{Y})^2}=105.1183\\
RSS &amp;= \sum{(Y_i- \hat{Y}_i)^2}=9.693\\
ESS &amp;= \sum{(\hat{Y}_i- \bar{Y})^2}=95.4253
\end{align}$$`

---

## 相关系数和判定系数

样本相关系数
`\(r\)`：

`$$\begin{align}
r =\frac{S_{XY}^2}{S_X\ast S_Y}=\frac{10.9821}{3.8944\ast2.9597} =0.9528\\
\end{align}$$`

回归方程的判定系数
`\(r^2\)`：

`$$\begin{align}
r^2 &amp;= 1- \frac{RSS}{TSS}=1-\frac{9.693}{105.1183} =0.9078\\
\end{align}$$`

二者关系

---

### 小结与思考

下面对本节内容做一个**小结**：

- 通过样本数值例子，我们可以全部自己“手工计算”OLS方法参数估计、估计的精度（估计量样本方差）、变异的平方和分解，以及判定系数等过程环节。

- 实际上任何一款统计软件都会很快**帮我们**完成这些计算过程，实证分析中我们不需要亲自去计算它们。

- 送上一句忠告，统计软件就像一个“技术黑箱”，如果你不理解“黑箱”里面的运作原理，那么你就永远只是被它“牵着鼻子走”！所以，大家起码要自己手工计算**一遍**！

- 而且，随着开源软件（如`R`或`Python`等）的普遍流行，**“你”**的介入和作用可能越来越重要！因为你可以更加灵活、更加自由地进行改造、重塑和创新！

**问题与思考**：

- 请大家自己使用任何熟悉的统计软件，完成本节的所有环节的计算！

- 如果你和其他同学的随机样本数据，都来自同一个总体，你们的计算结果会是一样的么？全班同学全部计算结果，会给“从样本推断总体”带来什么启示？


---
layout: false
class: inverse, center, middle, duke-softblue
name: N-CLRM

# 3.7 经典正态线性回归模型（N-CLRM）

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.7 经典正态线性回归模型（N-CLRM） &lt;/span&gt;&lt;/div&gt; 


---

### 引子：我们对总体参数的认识已经足够了么？

我们已经证明了在经典详细回归模型假设下（CLRM），采用普通最小二乘法（OLS），得到参数
`\(\Phi\)`的估计量
`\(\hat{\Phi}\)`，是最优线性无偏估计量（BLUE）。

而且也知道了，在随机抽样下，OLS的参数估计量是“随机变量”，其期望和方差分别是（以斜率参数的估计量为例）：

`$$\begin{cases}
  \begin{align}
  E(\hat{\beta}_2) &amp; =\beta_2 \\
  var(\hat{\beta}_2) &amp;= \frac{\sigma^2}{\sum{x_i^2}}
  \end{align}
\end{cases}$$`

我们马上想要提出的问题就是：既然这个参数估计量是**随机变量**，那么它服从什么分布呢？


---

## 随机干扰项的正态性假设

CLRM假设下：OLS方法得到的已经是BLUE了

CLRM假设下：随机干扰项
`\(u_i\)`的期望值为零（
`\(Eu_i=0\)`），
`\(u_i, u_j\)`不相关（
`\(E(u_i,u_j)=0\)`），且有一个不变方差（
`\(Var(u_i) \equiv \sigma^2\)`）。

总体回归模型PRM:

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i 
\end{align}$$`

样本回归模型SRM:

`$$\begin{align}
Y_i &amp;= \hat{\beta}_1 + \hat{\beta}_2X_i +e_i 
\end{align}$$`

--

讨论：

- 一个样本怎样才推断总体PRF？

- 多份样本而言，OLS估计量（
`\(\hat{\beta}_i,\hat{\sigma}^2\)`）因为样本变化而变化(随机变量)

---

## 随机干扰项的正态性假设

随机干扰项
`\(u_i\)`对于估计SRF
`\(\rightarrow\)`推断PRF的重要性：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{ \left( \frac{x_i}{\sum{x_i^2}} \cdot Y_i \right)} = \sum{k_iY_i}  = \sum{ \left[ k_i(\beta_1 +\beta_2X_i +u_i) \right] } 
\end{align}$$`

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{\left( \left( \frac{1}{n} - k_i\bar{X} \right) \cdot Y_i \right)} = \sum{w_iY_i} = \sum{ \left[ w_i(\beta_1 +\beta_2X_i +u_i) \right] } 
\end{align}$$`

`$$\begin{align}
\hat{\sigma}^2=\frac{\sum{e_i^2}}{n-2}
\end{align}$$`


---

## 经典正态线性回归模型假设（N-CLRM）

**经典正态线性回归模型**(classical normal linear regression model , N-CLRM)：在经典线性回归模型(CLRM)假设中再增加干扰项
`\(u_i\)`服从正态性的相关假设。

- 均值为0：
`\(E(u|X_i)=0\)`

- 同方差：
`\(Var(u_i|X_i) \equiv \sigma^2\)`

- 无自相关：
`\(E(u_i,u_j|X_i)=0\)`

- 正态性分布：
`\(u_i \sim N(0, \sigma^2)\)`

以上几条也可以统写为：
`\(u_i \sim iid. \  N(0, \sigma^2)\)`

其中，iid表示独立同分布(Independent Identical Distribution, iid)。

---

## N-CLRM假设下OLS估计量的性质

在N-CLRM假设下，OLS估计量有如下统计性质：

- 性质1：无偏性

- 性质2：有效性（方差最小）

- 性质3：一致性（收敛到它们的总体参数上）

---

## N-CLRM假设下OLS估计量的性质

- 性质4：估计量
`\(\hat{\beta}_2\)`是正态分布的：

`$$\begin{align}
\hat{\beta}_2 &amp; \sim N(\mu_{\hat{\beta}_2}, \sigma^2_{\hat{\beta}_2}) \\
\mu_{\hat{\beta}_2} &amp; = E(\hat{\beta}_2) = \beta_2 \\
\sigma^2_{\hat{\beta}_2} &amp; = \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

随机变量
`\(Z_2\)`服从标准正态分布：

`$$\begin{align}
Z_2 &amp;=\frac{\hat{\beta}_2- \beta_2}{\sigma_{\hat{\beta}_2}} \sim N(0,1)\\
\mu(Z_2) &amp; = E(\hat{\beta}_2- \beta_2) =0 \\
\sigma_{Z_2}^2 &amp; = Var \left( \frac{\hat{\beta}_2- \beta_2 }{\sigma_{\hat{\beta}_2}} \right)= \frac{Var(\hat{\beta}_2)}{\sigma^2_{\hat{\beta}_2}} =1
\end{align}$$`

---

## N-CLRM假设下OLS估计量的性质

- 性质5：估计量
`\(\hat{\beta}_1\)`是正态分布的：

`$$\begin{align}
\hat{\beta}_1 &amp; \sim N(\mu_{\hat{\beta}_1}, \sigma^2_{\hat{\beta}_1}) \\
\mu_{\hat{\beta}_1} &amp; = E(\hat{\beta}_1) = \beta_1 \\
\sigma^2_{\hat{\beta}_1} &amp; = \frac{\sum{X_i^2}}{n} \cdot  \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

随机变量
`\(Z_1\)`服从标准正态分布：


`$$\begin{align}
Z_1 &amp;=\frac{\hat{\beta}_1- \beta_1}{\sigma_{\hat{\beta}_1}} \sim N(0,1)\\
\mu(Z_1) &amp; = E(\hat{\beta}_1- \beta_1) =0 \\
\sigma_{Z_1}^2 &amp; = Var \left( \frac{\hat{\beta}_1- \beta_1 }{\sigma_{\hat{\beta}_1}} \right)= \frac{Var(\hat{\beta}_1)}{\sigma^2_{\hat{\beta}_1}} =1
\end{align}$$`

---

## N-CLRM假设下OLS估计量的性质

&lt;img src="pic/chpt3-N-CLRM.png" width="2043" style="display: block; margin: auto;" /&gt;

---

## N-CLRM假设下OLS估计量的性质

- 性质6：
`\(X \equiv (n-2)\hat{\sigma^2}/\sigma^2\)`服从自由度为
`\((n-2)\)`的卡方分布。

`$$\begin{align}
X &amp; \equiv (n-2)\hat{\sigma}^2/\sigma^2 \\
X &amp; \sim \chi^2(n-2)
\end{align}$$`

- 性质7：随机变量
`\((\hat{\beta}_2, \hat{\beta}_1)\)`的分布独立于随机变量
`\(\hat{\sigma}^2\)`

- 性质8：估计量
`\((\hat{\beta}_2, \hat{\beta}_1)\)`在所有无偏估计中，无论是线性还是非线性，都有最小的方差。也即，它们是最有无偏估计量（**B**est **U**nbiased **E**stimators, **BUE**）。


---

### 小结与讨论

本节**内容小结**：

- 除了关心参数估计量的性质（是否BLUE），我们还需要关注：参数估计量作为一个**随机变量**，会服从什么概率分布？这样才会为后面的**假设检验**提供基础！它将成为完成“由样本推断总体”逻辑分析的最后一个台阶，也是最重要的一个环节之一！

- N-CLRM假设体系，是在CLRM假设基础之上，额外再增加了一条关于随机干扰项服从**正态分布**的假设。基于此，我们可以推断得到回归系数估计量也将服从正态分布，从而进一步可以构造出很多有用的**样本统计量**，例如后面要学的**t统计量**、**F统计量**等。

**思考与讨论**：

- 你能说出现实中，随机干扰项
`\(u_i\)`服从其他概率分布的情形？

&gt; 参考答案：显然，现实社会现象中有很多不服从正态分布的情形，比如t分布、二项分布等。

- 如果随机干扰项确实**不服从**正态分布，OLS方法+CLRM假设还能那么“天生丽质”、那么“无往不利”么？

&gt; 参考答案：事实上，我们并不需要随机干扰项服从正态分布这条**强假设**。即时不服从正态分布，**中心极限定理**和**大数定理**也照样能保证OLS方法的有效性！


---
layout: false
class: inverse, center, middle, duke-softblue
name: ML

# 3.8 极大似然估计法(ML)

---
layout: true
  
&lt;div class="my-header"&gt;&lt;/div&gt;

&lt;div class="my-footer"&gt;&lt;span&gt;huhuaping@   第03章 一元回归：参数估计   
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;&amp;emsp;
3.8 极大似然估计法(ML) &lt;/span&gt;&lt;/div&gt; 

---

## 极大似然估计法(ML)

**极大似然估计法**(maximum likelihood, ML):

- 是由Fisher提出的一种参数估计方法基本思想：设总体分布的函数形式已知，但有未知参数
`\(\Theta\)`，
`\(\Theta\)`可以取很多值，在
`\(\Theta\)`的一切可能取值中选一个使样本观察值出现的概率为最大的
`\(\hat{\Theta}\)`值作为
`\(\Theta\)`的估计值，并称估计值
`\(\hat{\Theta}\)`为参数
`\(\Theta\)`的极大似然估计值。这种求估计量的方法称为极大似然估计法。

似然函数表达式：

- 设总体
`\(X_i\)`的概率密度函数
`\(f(X_i; \Theta)\)`为,其中
`\(\Theta\)`为待估计参数。对于从总体中取得的样本观测值
`\((X_1; X_2, \cdots , X_n)\)` ， 其联合密度函数为
`\(\prod{ f(X_i; \Theta)}\)` ，它是参数
`\(\Theta\)`的函数，称之为的似然函数，记为
`\(L(\Theta)\)`：

`$$\begin{align}
L(\Theta) = \prod{ f(X_i;\Theta)}
\end{align}$$`

---

## ML估计法与OLS估计法的关系

极大似然估计法(ML)比较复杂，我们仅需知道。在随机干扰项正态性假设下（N-CLRM）：

- 回归系数
`\(\beta_i\)`的ML估计量和OLS估计量是相同的——无论是一元回归还是多元回归!

- 对于
`\(\sigma^2\)`的估计，其ML估计量为
`\(\sum{e_i^2}/n\)`，是**有偏**的；其OLS估计量是
`\(\sum{e_i^2}/(n-2)\)`，是**无偏**的。

- 关于
`\(\sigma^2\)`的两种估计量，随着样本容量n的增大，两者将趋于相等！

启示：OLS方法真好！

---
layout:false
background-image: url("pic/thank-you-gif-funny-gentle.gif")
class: inverse,center
# 本章结束
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
