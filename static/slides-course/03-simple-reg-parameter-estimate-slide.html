<!DOCTYPE html>
<html>
  <head>
    <title>03-simple-reg-parameter-estimate-slide.utf8.md</title>
    <meta charset="utf-8">
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/duke-blue.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge-duke.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <script src="libs/kePrint/kePrint.js"></script>
    <link rel="stylesheet" href="libs\cc-fonts.css" type="text/css" />
    <link rel="stylesheet" href="libs\figure-captions.css" type="text/css" />
    <link rel="stylesheet" href="libs\animate.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

background-image: url("pic/slide-front-page.jpg")
class: center,middle

# 计量经济学(Econometrics)

### 胡华平

### 西北农林科技大学

### 经济管理学院数量经济教研室

### huhuaping01@hotmail.com

### 2019-03-05





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 22px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

---
class: center, middle,inverse,animated, slideInRight
# 第3章：一元回归：参数估计

## 内容目录

普通最小二乘法（OLS）

经典线性回归模型(CLRM)

最小二乘估计的精度或标准误差

最小二乘估计的性质：BLUE

判定系数和简单相关系数

一个数值例子

说明性例子

经典正态线性回归模型（CNLRM）

---
class: inverse, middle, center
# 普通最小二乘法（OLS）


---
class: animated, bounceInDown
# 普通最小二乘法（OLS）

估计回归函数中的系数，有多种方法

- 图解法

- 最小二乘法(order lease squares, OLS)：最常用的方法

- 最大似然法(maximum likelihood, ML)

- 矩估计方法(Moment method, MM)

---
## 样本回归与总体回归的比较

.pull-left[
总体回归函数PRF:

`$$\begin{align}
E(Y|X_i) &amp;= \beta_1 +\beta_2X_i 
\end{align}$$`

总体回归模型PRM:

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i 
\end{align}$$`

]

.pull-right[
样本回归函数SRF:

`$$\begin{align}
\hat{Y}_i =\hat{\beta}_1 + \hat{\beta}_2X_i 
\end{align}$$`

样本回归模型SRM:

`$$\begin{align}
Y_i &amp;= \hat{\beta}_1 + \hat{\beta}_2X_i +e_i 
\end{align}$$`

]

--

思考：

- PRF无法直接观测，只能用SRF近似替代

- 估计值与观测值之间存在偏差

- SRF又是怎样决定的呢?

---

## 普通最小二乘法的原理

认识普通最小二乘法的原理：一个图示

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-demo.png" alt="最小二乘法的原理" width="507" /&gt;
&lt;p class="caption"&gt;最小二乘法的原理&lt;/p&gt;
&lt;/div&gt;

---
## 普通最小二乘法的原理

OLS的基本原理：残差平方和最小化。

`$$\begin{align}
e_i  &amp;= Y_i - \hat{Y}_i \\
     &amp;= Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) 
\end{align}$$`


`$$\begin{align}
Q  &amp;= \sum{e_i^2} \\
   &amp;= \sum{(Y_i - \hat{Y}_i)^2} \\
   &amp;= \sum{\left( Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) \right)^2} \\
   &amp;\equiv f(\hat{\beta}_1,\hat{\beta}_2)
\end{align}$$`


`$$\begin{align}
Min(Q)  &amp;= Min \left ( f(\hat{\beta}_1,\hat{\beta}_2) \right)
\end{align}$$`

---

## 普通最小二乘法的原理

认识普通最小二乘法的原理：一个数值试验。假设存在下面所示的4组观测值
`\((X_i, Y_i)\)`：

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-compare1.png" alt="数值试验：数据" width="2504" /&gt;
&lt;p class="caption"&gt;数值试验：数据&lt;/p&gt;
&lt;/div&gt;

---

## 普通最小二乘法的原理

假设随便猜想了如下两个SRF，完成下表计算，并分析哪个SRF给出的
`\((\hat{\beta}_1, \hat{\beta}_2)\)`要更好？

`$$\begin{align}
SRF1：\hat{Y}_{1i} &amp; = \hat{\beta}_1 +\hat{\beta}_2X_i = 1.572 + 1.357X_i \\
SRF2：\hat{Y}_{2i} &amp; = \hat{\beta}_1 +\hat{\beta}_2X_i = 3.0 + 1.0X_i 
\end{align}$$`


&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-OLS-compare2.png" alt="数值试验：计算" width="2509" /&gt;
&lt;p class="caption"&gt;数值试验：计算&lt;/p&gt;
&lt;/div&gt;

---
## 回归参数的OLS点估计

最小化求解：

`$$\begin{align}
Min(Q)  &amp;= Min \left ( f(\hat{\beta}_1,\hat{\beta}_2) \right)\\
 &amp;= Min\left(\sum{\left( Y_i - (\hat{\beta}_1 +\hat{\beta}_2X_i) \right)^2} \right) \\
  &amp;= Min \sum{\left( Y_i - \hat{\beta}_1 - \hat{\beta}_2X_i \right)^2}
\end{align}$$`


方程组变形，得到**正规方程组**：

`$$\begin{align}
\left \{
  \begin{split}
   \sum{\left[ \hat{\beta}_1 - (Y_i -\hat{\beta}_2X_i) \right]}  &amp;=0 \\
   \sum{\left[ X_i^2\hat{\beta}_2 - (Y_i-\hat{\beta}_1 )X_i \right ] }&amp;=0 
   \end{split}
\right. 
\end{align}$$`

`$$\begin{align}
\left \{
  \begin{split}
   \sum{Y_i} - n\hat{\beta}_1- (\sum{X_i})\hat{\beta}_2 &amp;=0 \\
   \sum{X_iY_i}-(\sum{X_i})\hat{\beta}_1 -  (\sum{X_i^2})\hat{\beta}_2 &amp;=0 
   \end{split}
\right.
\end{align}$$`

---
## 回归参数的OLS点估计

进而得到回归系数的计算公式1（Favorite Five，FF）：

`$$\begin{align}
  \left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
  \hat{\beta_1} &amp;=\frac{n\sum{X_i^2Y_i}-\sum{X_i}\sum{X_iY_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}
  \end{split} 
  \right.
  &amp;&amp;\text{(FF solution)}
\end{align}$$`

---
## 回归参数的OLS点估计

此外我们也可以得到如下的离差公式(favorite five，ff)

`$$\begin{align}
\left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{\sum{x_iy_i}}{\sum{x_i^2}}\\
  \hat{\beta_1} &amp;=\bar{Y}_i-\hat{\beta}_2\bar{X}_i
  \end{split} 
\right.  
  &amp;&amp; \text{(ff solution)}
\end{align}$$`

其中离差计算
`\(x_i=X_i-\bar{X};\  y_i=Y_i - \bar{Y}\)`。

--

课堂测试：

`$$\begin{align}
\left\{
  \begin{split}
    \sum{x_iy_i} &amp;= \sum{\left[ (X_i-\bar{X})(Y_i-\bar{Y})\right]} 
    &amp;&amp;= \sum{X_iY_i} - \frac{1}{n}\sum{X_i}\sum{Y_i} \\
    \sum{x_i^2} &amp;= \sum{(X_i- \bar{X})^2} 
    &amp;&amp;= \sum{X_i^2} -\frac{1}{n} \left( \sum{X_i} \right)^2
  \end{split}
\right.
\end{align}$$`

---
## 随机干扰项参数的OLS点估计

PRM公式变形：

`$$\begin{alignedat}{2}
&amp;\left.
  \begin{split}
   Y_i &amp;&amp;= \beta_1 - &amp;&amp;\beta_2X_i +u_i  \ &amp;&amp; \text{(PRM)} \Rightarrow \\
   \hat{Y} &amp;&amp;= \beta_1 - &amp;&amp;\beta_2\bar{X} +\bar{u} &amp;&amp; \\   
  \end{split}
\right \} \Rightarrow \\
 &amp; y_i = \beta_2x_i +(u_i- \bar{u})  
\end{alignedat}$$`

残差公式变形：

`$$\begin{alignedat}{2}
 &amp;\left. 
  \begin{split}
    &amp; e_i = y_i - \hat{\beta}_2x_i \\
    &amp; e_i = \beta_2x_i +(u_i- \bar{u}) -\hat{\beta}_2x_i 
  \end{split}
\right \}  \Rightarrow \\
&amp; e_i =-(\hat{\beta}_2- \beta_2)x_i + (u_i- \hat{u})
\end{alignedat}$$`

---
## 随机干扰项参数的OLS点估计

求解残差平方和：

`$$\begin{alignedat}{2}
  &amp; \sum{e_i^2} &amp;&amp; = (\hat{\beta}_2 - \beta_2)^2\sum{x_i^2} + \sum{(u-\bar{u})^2} - 2(\hat{\beta}_2 - \beta_2)\sum{x_i(u-\bar{u})}  
\end{alignedat}$$`

求残差平方和的期望：

`$$\begin{align}
E(\sum{e_i^2}) &amp;= 
 \sum{x_i^2 E \left[ (\hat{\beta}_2 - \beta_2)^2 \right ]}+ E\left[ \sum{(u-\bar{u})^2} \right ]\\
&amp;+ 2E \left[ (\hat{\beta}_2 - \beta_2)\sum{x_i(u-\bar{u})} \right ] \\
&amp; \equiv   A + B + C \\
&amp; = \sigma^2 + (n-1)\sigma^2 -2\sigma^2 \\
&amp; = (n-2)\sigma^2 
\end{align}$$`

---
## 随机干扰项参数的OLS点估计

**回归误差方差**（Deviation of Regression Error）：

- 采用OLS方法下，总体回归模型PRM中随机干扰项
`\(u_i\)`的总体方差的无偏估计量，记为
`\(E(\sigma^2) \equiv \hat{\sigma}^2\)`，简单地记为
`\(\hat{\sigma}^2\)`。

`$$\begin{align}
\hat{\sigma}^2=\frac{\sum{e_i^2}}{n-2}
\end{align}$$`

--

**回归误差标准差**（Standard Deviation of Regression Error）：

- 采用OLS方法下，总体回归模型PRM中随机干扰项
`\(u_i\)`的总体标准差的无偏估计量，记为
`\(E(\sigma) \equiv \hat{\sigma}\)`，代数表达式一般简单地记为
`\(\hat{\sigma}\)`，有时候也记为**se**。

`$$\begin{align}
\hat{\sigma}=\sqrt{\frac{\sum{e_i^2}}{n-2}}
\end{align}$$`

---
## 随机干扰项参数的OLS点估计

详细证明过程如下：

- A过程说明：

`$$\begin{align}
A &amp; = \sum{x_i^2 E \left[ (\hat{\beta}_2 - \beta_2)^2 \right ]} \\
  &amp; = \sum{ \left[ x_i^2 \cdot var(\hat{\beta}_2) \right] } \\
  &amp; = var(\hat{\beta}_2) \cdot \sum{x_i^2}  \\
  &amp; = \frac{\sigma^2}{\sum{ x_i^2}} \cdot \sum{ x_i^2}  \\
  &amp; = \sigma^2
\end{align}$$`

---
## 随机干扰项参数的OLS点估计

- B过程说明：

`$$\begin{align}
B &amp; = E \left[ \sum{(u-\bar{u})^2} \right ] \\
  &amp; = E(\sum{u_i^2}) - 2E \left[ \sum{(u_i\bar{u})} \right] +nE(\bar{u}^2) \\
  &amp; = n \cdot Var(u_i) - 2E \left[ \sum{(u_i \cdot \frac{\sum{u_i}}{n} )}  \right]  + nE(\frac{\sum{u_i}}{n})^2 \\
  &amp; = n \sigma^2 - 2E \left[ \frac{\sum{u_i}}{n} \sum{u_i} \right] + E\left[ \frac{(\sum{u_i})^2}{n} \right]\\
  &amp; = n \sigma^2- E\left[ \frac{(\sum{u_i})^2}{n} \right] \\
  &amp; = n \sigma^2  -  \frac{E(u_i^2) + E(u_2^2) + \cdots +  E(u_n^2) )}{n} \\
  &amp; =  n \sigma^2 -  \frac{nVar{u_i}}{n} \\
  &amp; =  n \sigma^2 -  \sigma^2 \\
  &amp; =  (n-1) \sigma^2
\end{align}$$`

---
### 随机干扰项参数的OLS点估计

- C过程说明：

`$$\begin{align}
C &amp;= - 2E \left[ (\hat{\beta}_2 - \beta_2)\sum{x_i(u_i-\bar{u})} \right ] \\
  &amp;= - 2E \left[ \frac{\sum{x_iu_i}}{\sum{x_i^2}} \left( \sum{x_iu_i}-\bar{u}\sum{x_i} \right) \right ] \\
  &amp;= - 2E \left[ \frac{ \left( \sum{x_iu_i} \right)^2}{\sum{x_i^2}}  \right ]  \\
  &amp;= -2E \left[(\hat{\beta}_2 - \beta_2)^2 \right] = -2\sigma^2
\end{align}$$`

--

- 其中：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} = \sum{k_i(\beta_1 +\beta_2X_i +u_i)}  \\
&amp; = \beta_1\sum{k_i} +\beta_2 \sum{k_iX_i}+\sum{k_iu_i}  
= \beta_2 +\sum{k_iu_i} \\
\hat{\beta}_2 - \beta_2 &amp; = \sum{k_iu_i} = \frac{ \sum{x_iu_i} }{\sum{x_i^2}}
\end{align}$$`

---
## 随机干扰项参数的OLS点估计

估计出总体参数
`\(\sigma^2\)`：

`$$\begin{align}
\hat{\sigma}^2 = \frac{\sum{e_i^2}}{(n-2)}
\end{align}$$`

`$$\begin{align}
\hat{\sigma} = \sqrt{\frac{\sum{e_i^2}}{(n-2)}}
\end{align}$$`

---
## OLS方法讨论：“估计值”与“估计量”

理解OLS方法下的“估计值”与“估计量”

回归系数的计算公式1（Favorite Five，FF）：

`$$\begin{align}
  \left \{
  \begin{split}
  \hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
  \hat{\beta_1} &amp;=\frac{n\sum{X_i^2Y_i}-\sum{X_i}\sum{X_iY_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}
  \end{split} 
  \right.
  &amp;&amp;\text{(FF solution)}
\end{align}$$`


- 如果给出的参数估计结果是由一个具体样本资料计算出来的，它是一个“估计值”，或者“点估计”，是参数估计量的一个具体数值；

- 如果把上式看成参数估计的一个表达式，那么，则它是
`\((X_i,Y_i)\)`的函数，而$Y_i$是随机变量，所以参数估计也是随机变量，在这个角度上，称之为“估计量”。  

---
## OLS方法下SRF和SRM的特征

OLS估计量是纯粹由可观测的(即样本)量(指X和Y)表达的，因此它们很容易计算。

它们是点估计量(point estimators)，即对于给定样本，每个估计量仅提供有关总体参数的一个(点)值。[我们以后还将考虑区间估计量(interval Estimators)]

一旦从样本数据得到OLS估计值，便容易画出样本回归线。

---
## OLS方法下SRF和SRM的特征

- 特征1：样本回归线一定会经过样本均值点
`\((\bar{X}, \bar{Y})\)`：

`$$\begin{align}
\bar{Y} = \hat{\beta}_1 +\hat{\beta}_2\bar{X}
\end{align}$$`

- 特征2：
`\(Y_i\)`的**估计值**(
`\(\hat{Y}_i\)`)的均值(
`\(\bar{\hat{Y_i}}\)`)等于Y的样本均值(
`\(\bar{Y}\)`)

`$$\begin{align}
\hat{Y_i} &amp;= \hat{\beta}_1 +\hat{\beta}_2\bar{X} \\
&amp; =(\bar{Y} - \hat{\beta}_2\bar{X}) + \hat{\beta_2}X_i \\
&amp; = \bar{Y} - \hat{\beta}_2(X_i - \bar{X}) 
\end{align}$$`

`$$\begin{align}
&amp;\Rightarrow  1/n\sum{\hat{Y_i}} =  1/n\sum{\bar{Y} - \hat{\beta}_2(X_i - \bar{X})} \\
&amp;\Rightarrow  \bar{\hat{Y_i}}  = \bar{Y}
\end{align}$$`

---
## OLS方法下SRF和SRM的特征

- 特征3：残差的均值(
`\(\bar{e_i}\)`)为零：

`$$\begin{align}
\sum{\left[ \hat{\beta}_1 - (Y_i -\hat{\beta}_2X_i) \right]}  &amp;=0 \\
\sum{\left[ Y_i- \hat{\beta}_1 - \hat{\beta}_2X_i) \right]}  &amp;=0 \\
\sum{( Y_i- \hat{Y}_i )} &amp;=0 \\
\sum{e_i}  &amp;=0 \\
\bar{e_i} &amp;=0
\end{align}$$`

---
## OLS方法下SRF和SRM的特征

- 特征4：SRM和SRF可以写成离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i + e_i \\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X}) + e_i \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i +e_i \  &amp;&amp;\text{(SRM-dev)}
\end{align}$$`

`$$\begin{align}
&amp; \left.
  \begin{split}
  \hat{Y}_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i\\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; \hat{Y}_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X})  \Rightarrow  \\
&amp; \hat{y}_i=\hat{\beta_2}x_i \  &amp;&amp;\text{(SRF-dev)} 
\end{align}$$`

---
## OLS方法下SRF和SRM的特征

- 特征5：残差(
`\(e_i\)`)和
`\(Y_i\)`的拟合值(
`\(\hat{Y_i}\)`)不相关

`$$\begin{align}
Cov(e_i, \hat{Y_i}) &amp;= Cov \left[ \left( e_i-E(e_i)\right )\cdot \left( \hat{Y_i}-E(\hat{Y_i})\right ) \right]\\
&amp; = Cov(e_i \cdot \hat{y_i}) \\
&amp; = \sum(e_i \cdot \hat{\beta_2}x_i) \\
&amp; = \sum{ \left[ (y_i-\hat{\beta_2}x_i) \cdot \hat{\beta_2}x_i \right]} \\
&amp; = \hat{\beta_2}\sum \left[ (y_i-\hat{\beta_2}x_i)\cdot x_i \right]\\
&amp; = \hat{\beta_2}\sum \left[ (y_ix_i-\hat{\beta_2}x_i^2)  \right]\\
&amp; = \hat{\beta_2}\sum{x_iy_i}-\hat{\beta}_2^2\sum{x_i^2}  &amp;&amp; \Leftarrow \hat{\beta_2} = \frac{\sum{x_iy_i}}{x_i^2} \\
&amp; = \hat{\beta}_2^2\sum{x_i^2}-  \hat{\beta_2}^2\sum{x_i^2}   = 0
\end{align}$$`


- 特征6：残差(
`\(e_i\)`)和自变量(
`\(X_i\)`)不相关

---
## OLS方法下的离差公式总结

- 离差定义与符号：

`$$\begin{align}
x_i &amp;= X_i - \bar{X} \\
y_i &amp;= Y_i - \bar{Y} \\
\hat{y}_i &amp;= \hat{Y}_i - \bar{\hat{Y}}_i = \hat{Y}_i - \bar{Y}
\end{align}$$`

- PRM及其离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \beta_1 + \beta_2X_i + u_i \\
  \bar{Y} &amp;&amp;= \beta_1 + \beta_2\bar{X} + \bar{u}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\beta_2x_i + (u_i- \bar{u}) \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i + (u_i- \bar{u})  \  &amp;&amp;\text{(PRM-dev)}
\end{align}$$`

---
## OLS方法下的离差公式总结

--
.pull-left[
- SRM及其离差形式：
`$$\begin{align}
&amp; \left.
  \begin{split}
  Y_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i + e_i \\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; Y_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X}) + e_i \Rightarrow  \\
&amp; y_i=\hat{\beta_2}x_i +e_i 
\end{align}$$`
]

--
.pull-right[
- SRF及其离差形式：

`$$\begin{align}
&amp; \left.
  \begin{split}
  \hat{Y}_i &amp;&amp; = \hat{\beta}_1 + \hat{\beta}_2X_i\\
  \bar{Y} &amp;&amp;= \hat{\beta}_1 + \hat{\beta}_2\bar{X}
  \end{split}
\right \} \Rightarrow \\
&amp; \hat{Y}_i - \bar{Y} =\hat{\beta_2}(X_i - \bar{X})  \Rightarrow  \\
&amp; \hat{y}_i=\hat{\beta_2}x_i \   
\end{align}$$`
]

--

- 残差的离差形式：

`$$\begin{align}
 y_i=\hat{\beta_2}x_i +e_i  &amp;&amp;\text{(SRM-dev)} \ \Rightarrow  \\
 e_i =y_i - \hat{\beta_2}x_i \  &amp;&amp;\text{(residual-dev)}
\end{align}$$`

---
# 经典线性回归模型(CLRM)假设

经典线性回归模型(classical linear regression model, CLRM)：

- 又称为高斯或标准线性回归模型

- 成为大部分计量经济学理论的基石，有7个基本假设

- 本章以双变量回归模型为讨论基础：

---
## 关于模型的假设

**CLRM假设1**：模型是正确设置的。（这里大有学问，也是一切计量分析问题的根本来源）


**CLRM假设2**：模型应该是参数线性的。也即模型中**参数**必须线性，变量可以不是线性

`$$\begin{align}
Y_i  = \beta_1 + \beta_2X_i + u_i 
\end{align}$$`

---
## 关于自变量X的假设

**CLRM假设3**：X是固定的（给定的）或独立于误差项。也即自变量X**不是**随机变量。

`$$\begin{align}
Cov(X_i, u_i)= 0\\ 
E(X_i, u_i)= 0
\end{align}$$`

X是固定的（给定的）是什么含义：

- 其方差（
`\(Var(X)\)`）是有限的正数。例如X取值不能全部相同（此时
`\(Var(X)=0\)`）；又例如回归系数估计值公式中分母为0，无法求解！

- X变量没有异常值(outlier)，即没有一个X值对于其他值过大或过小。

--

课堂思考：

- X值固定不变现实么？

- 为什么要假设这种情形？

- 如果X是随机变量，仍旧坚持用OLS方法，得到的结果还是不变么？

---
## 关于自变量X的假设



扁数据形态：“非标准”数据形态（但很直观）

<div id="htmlwidget-3c5e2a85c9a286135dfa" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-3c5e2a85c9a286135dfa">{"x":{"filter":"none","caption":"<caption>60个家庭的收入和支出情况：假设的总体<\/caption>","data":[["1","2","3","4","5","6","7","8"],["X","Y1","Y2","Y3","Y4","Y5","Y6","Y7"],[80,55,60,65,70,75,null,null],[100,65,70,74,80,85,88,null],[120,79,84,90,94,98,null,null],[140,80,93,95,103,108,113,115],[160,102,107,110,116,118,125,null],[180,110,115,120,130,135,140,null],[200,120,136,140,144,145,null,null],[220,135,137,140,152,157,160,162],[240,137,145,155,165,175,189,null],[260,150,152,175,178,180,185,191]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Mark<\/th>\n      <th>G1<\/th>\n      <th>G2<\/th>\n      <th>G3<\/th>\n      <th>G4<\/th>\n      <th>G5<\/th>\n      <th>G6<\/th>\n      <th>G7<\/th>\n      <th>G8<\/th>\n      <th>G9<\/th>\n      <th>G10<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":9,"dom":"tip","columnDefs":[{"className":"dt-right","targets":[2,3,4,5,6,7,8,9,10,11]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[9,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---
## 关于自变量X的假设

长数据形态：标准数据形态（但不直观）

<div id="htmlwidget-a4293ea63caa39102f0c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-a4293ea63caa39102f0c">{"x":{"filter":"none","caption":"<caption>60个家庭的收入和支出情况：假设的总体<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60"],[1,1,1,1,1,2,2,2,2,2,2,3,3,3,3,3,4,4,4,4,4,4,4,5,5,5,5,5,5,6,6,6,6,6,6,7,7,7,7,7,8,8,8,8,8,8,8,9,9,9,9,9,9,10,10,10,10,10,10,10],[80,80,80,80,80,100,100,100,100,100,100,120,120,120,120,120,140,140,140,140,140,140,140,160,160,160,160,160,160,180,180,180,180,180,180,200,200,200,200,200,220,220,220,220,220,220,220,240,240,240,240,240,240,260,260,260,260,260,260,260],[55,60,65,70,75,65,70,74,80,85,88,79,84,90,94,98,80,93,95,103,108,113,115,102,107,110,116,118,125,110,115,120,130,135,140,120,136,140,144,145,135,137,140,152,157,160,162,137,145,155,165,175,189,150,152,175,178,178,185,191]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>group<\/th>\n      <th>X<\/th>\n      <th>Y<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":7,"dom":"tip","columnDefs":[{"className":"dt-right","targets":[1,2,3]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[7,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>




---
## 关于自变量X的假设

- X为**固定值**情形下，两份随机样本：

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; var &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n4 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n5 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n6 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n7 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n8 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n9 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n10 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Y1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 60 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 85 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 115 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 110 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 162 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 165 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 178 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 80 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Y2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 55 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 74 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 90 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 95 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 107 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 115 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 189 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 152 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## 关于自变量X的假设

- X为**随机变量**时，两份随机样本：

&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; var &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n1 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n2 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n3 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n4 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n5 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n6 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n7 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n8 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n9 &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; n10 &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 180 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 260 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Y3 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 70 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 93 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 108 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 125 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 135 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 155 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 152 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 150 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 178 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; X4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 120 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 160 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 200 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 220 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 240 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; Y4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 84 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 98 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 116 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 144 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 140 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 145 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 157 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 162 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 189 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 137 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---
## 关于随机干扰项的假设

### 随机干扰项均值为零

**CLRM假设4**：假设随机干扰项均值为零。也即给定
`\(X_i\)`的情形下，假定随机干扰项
`\(u_i\)`的**条件期望**为零。

`$$\begin{align}
E(u|X_i)= 0
\end{align}$$`


&lt;img src="pic/chpt3-CLRM-mean0.jpg" width="454" style="display: block; margin: auto;" /&gt;

---
## 关于随机干扰项的假设

课堂讨论：

- 讨论1：这条假设是多余的么？——因为前面已经假定
`\(X_i\)`与$u_i$不相关（也即，
`\(E(X_i, u_i)=0\)`见**CLRM假设3**），是不是就必然
`\(E(u|X_i)= 0\)`？

&gt;&gt;
答案：如果
`\(X_i\)`与
`\(u_i\)`**相关**（也即
`\(E(X_i, u_i) \neq 0\)`），则随机干扰项的条件期望往往不等于零（也即
`\(E(u|X_i) \neq 0\)`）；同时，如果
`\(X_i\)`与
`\(u_i\)`**不相关**（也即
`\(E(X_i, u_i) = 0\)`），则随机干扰项的条件期望也可能等于零（也即
`\(E(u|X_i) = 0\)`）。因此这一条假设是必须的。
&gt;&gt;

- 讨论2：若自变量
`\(X_i\)`为**随机变量**，我们还应当假定随机干扰项
`\(u_i\)`的**无条件期望**为零么？也即能否假定
`\(E(u_i)=0\)`？

---
### 随机干扰项的方差为同方差

**CLRM假设5**：随机干扰项的方差为同方差。也即给定$X_i$的情形下，随机干扰项$u_i$的方差，处处都是相等的。记为：

`$$\begin{align}
Var(u_i|X_i) &amp; = E \left[ \left( u_i -E(u_i) \right)^2|X_i \right] \\
&amp; = E(u_i^2|X_i) \\
&amp; = E(u_i^2) \\
&amp; \equiv \sigma^2
\end{align}$$`

---
### 随机干扰项的方差为同方差

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-homoscedasticity.png" alt="随机干扰项的方差处处相等" width="608" /&gt;
&lt;p class="caption"&gt;随机干扰项的方差处处相等&lt;/p&gt;
&lt;/div&gt;

---
### 随机干扰项的方差为同方差

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-heteroscedasiticity.png" alt="随机干扰项的方差随X取值不同而不同" width="599" /&gt;
&lt;p class="caption"&gt;随机干扰项的方差随X取值不同而不同&lt;/p&gt;
&lt;/div&gt;

---
### 随机干扰项的方差为同方差

- 同方差性(homoscedasticity) ：
`\(Var(u_i|X_i) \equiv \sigma^2\)`

- 异方差性(heteroscedasticity) ：
`\(Var(u_i|X_i) \equiv \sigma_i^2\)`


课堂讨论：

- 讨论1: 如果
`\(Var(u_i|X_1) &lt; Var(u_2|X_2)\)`，是否意味着来自
`\(X=X_1\)`的总体，相比来自
`\(X=X_2\)`的总体，更靠近总体回归线PRL?

- 讨论2：如何看待**随机样本**的质量？或者，那些离均值较近的Y总体的随机样本，与远为分散的Y总体的随机样本，前者是不是质量更好?

- 讨论3：如果出现异方差，会对OLS估计产生什么后果？

---
### 随机干扰项之间无自相关

**CLRM假设6**：各个随机干扰之间无自相关。也即给定两个不同的自变量取值（
`\(X_i,X_j;i \neq j\)`）情形下，随机干扰项
`\(u_i,u_j\)`的相关系数为0。或者说
`\(u_i,u_j\)`最好是相互独立的。记为：

在
`\(X_i\)`为给定情形下，且
`\(i,j \in (1, 2, \cdots, n); i \neq j\)`，假定：

`$$\begin{align}
Cov(u_i, u_j) &amp; = E \left[ \left( u_i -E(u_i) \right)\left( u_i -E(u_i) \right) \right]  \\
&amp; = E(u_iu_j) \\
&amp; \equiv 0
\end{align}$$`


重要概念区别：

- 无**序列相关**(no serial correlatìon)：时间序列数据问题

- 无**自相关**(no autocorrelation)：截面数据（问题不大）

`$$\begin{align}
Y_i &amp;= \beta_1 + \beta_2X_i + u_i \\
Y_t &amp; = \beta_1 + \beta_2X_{2t} + \beta_2X_{3t} + u_t 
\end{align}$$`

---
### 随机干扰项之间无自相关

.pull-left[
&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-CLRM-correlation.png" alt="随机干扰项之间的相关情形" width="486" /&gt;
&lt;p class="caption"&gt;随机干扰项之间的相关情形&lt;/p&gt;
&lt;/div&gt;
]

--
.pull-right[
课堂讨论：

- 讨论1：该假设的目的和用处是什么？

- 讨论2：如果出现自相关，会对OLS估计产生什么影响？

]

---
## 关于样本数的要求

**CLRM假设7**：观测次数n，要大于待估计参数个数。否则方程无法解出，参数不能估计出来。

---

## 关于CLRM假设的大讨论

--

- 所有这些假设有多真实?

&gt;&gt;
“假定无关紧要论”——弗里德曼


--

- 上述说有假设都是针对PRF，而不是SRF！

&gt;&gt;
例如：PRF中随机干扰项有无自相关的假设
`\(Cov(u_i, u_j)=0\)`；但是在SRF中，可能就会出现
`\(Cov(e_i, e_j) \neq 0\)`

--

- 前面提到的OLS方法正是试图“复制”CLRM的假设！

&gt;&gt;
OLS方法中，
`\(\sum{e_iX_i}=0\)`，就类似于自变量X与随机干扰项不相关的假设（也即
`\(Cov(u_i,X_i)=0\)`）。

&gt;&gt;
OLS方法中，
`\(\sum{e_i}=0,(\bar{e_i}=0)\)`，就类似于随机干扰项期望值为0的假设（也即
`\(E(u|X_i)=0\)`）

---
class: animated, bounceInDown, middle, center, inverse
# OLS估计量的精度


---
## 斜率系数的方差和样本方差

.pull-left[
斜率系数（
`\(\hat{\beta}_2\)`）的**总体方差**（
`\(\sigma^2_{\hat{\beta}_2}\)`）和**总体标准差**（
`\(\sigma_{\hat{\beta}_2}\)`）：

`$$\begin{align}
Var(\hat{\beta}_2) \equiv \sigma_{\hat{\beta}_2}^2  &amp; =\frac{\sigma^2}{\sum{x_i^2}} \\
\sigma_{\hat{\beta}_2} &amp;=\sqrt{\frac{\sigma^2}{\sum{x_i^2}}} 
\end{align}$$`

- 其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。

]

.pull-right[
斜率系数（
`\(\hat{\beta}_2\)`）的**样本方差**（
`\(S^2_{\hat{\beta}_2}\)`）和**样本标准差**（
`\(S_{\hat{\beta}_2}\)`）：

`$$\begin{align}
S_{\hat{\beta}_2}^2 &amp;=\frac{\hat{\sigma}^2}{\sum{x_i^2}} \\
S_{\hat{\beta}_2} &amp;=\sqrt{\frac{\hat{\sigma}^2}{\sum{x_i^2}}}
\end{align}$$`

- 其中，
`\(E(\sigma^2) = \hat{\sigma}^2 = \frac{\sum{e_i^2}}{n-2}\)`表示对随机干扰项（
`\(u_i\)`）的总体方差的**无偏估计量**。
]

---
## 斜率系数的方差和样本方差

证明过程：（**步骤1**）
`\(\hat{\beta}_2\)`的变形：

`$$\begin{align}
\hat{\beta}_2 &amp;=\frac{\sum{x_iy_i}}{\sum{x_i^2}}= \frac{\sum{\left[ x_i (Y_i -\bar{Y}) \right]} }{\sum{x_i^2}}  \\
&amp; = \frac{\sum{ x_iY_i}- \sum{ x_i \bar{Y} } }{\sum{x_i^2}}    \\
&amp; = \frac{\sum{x_iY_i}- \bar{Y}\sum{x_i} }{\sum{x_i^2}}  &amp;&amp; \leftarrow \left[ \sum{x_i}=\sum{(X_i -\bar{X})} = 0 \right]  \\
&amp; = \sum{ \left(\frac{\sum{x_i}}{\sum{x_i^2}} \cdot Y_i \right) }   &amp;&amp; \leftarrow  \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sum{k_iY_i}
\end{align}$$`

- 其中，
`\(k_i =\frac{x_i}{\sum{x_i^2}}\)`。

---
## 斜率系数的方差和样本方差：推导过程

证明过程（续）：（**步骤2**）：计算
`\(\hat{\beta}_2\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_2}\)`）：

`$$\begin{align}
\sigma^2_{\hat{\beta}_2} &amp; \equiv Var(\hat{\beta}_2) 
 = Var(\sum{k_iY_i} ) \\
&amp; = \sum{\left( k_i^2Var(Y_i) \right)} \\
&amp; = \sum{\left( k_i^2Var(\beta_1 +\beta_2X_i +u_i) \right)} \\
&amp; = \sum{ \left( k_i^2Var(u_i) \right)}  &amp;&amp; \leftarrow \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sum{ \left( \left(\frac{x_i}{\sum{x_i^2}} 
                 \right)^2 \cdot \sigma^2 
          \right)} \\
&amp; = \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。

---
## 截距系数的方差和样本方差

.pull-left[
截距系数（
`\(\hat{\beta}_1\)`）的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）和**总体标准差**（
`\(\sigma_{\hat{\beta}_1}\)`）：


`$$\begin{align}
Var(\hat{\beta}_1) \equiv \sigma_{\hat{\beta}_1}^2  &amp;=\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}} \\
\sigma_{\hat{\beta}_1} &amp; =\sqrt{\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}}
\end{align}$$`


- 其中，
`\(Var(u_i) \equiv \sigma^2\)`表示随机干扰项
`\(u_i\)`的总体方差。
]

.pull-right[
截距系数（
`\(\hat{\beta}_1\)`）的**样本方差**（
`\(S^2_{\hat{\beta}_1}\)`）和**样本标准差**（
`\(S_{\hat{\beta}_1}\)`）：

`$$\begin{align}
S_{\hat{\beta}_1}^2 &amp;=\frac{\sum{X^2_i}}{n} \cdot \frac{\hat{\sigma}^2}{\sum{x_i^2}} \\
S_{\hat{\beta}_1} &amp;=\sqrt{\frac{\sum{X^2_i}}{n} \cdot \frac{\hat{\sigma}^2}{\sum{x_i^2}}}
\end{align}$$`

- 其中，
`\(E(\sigma^2) = \hat{\sigma}^2 = \frac{\sum{e_i^2}}{n-2}\)`表示对随机干扰项（
`\(u_i\)`）的总体方差的**无偏估计量**。
]

---
## 截距系数的方差和样本方差

证明过程：（**步骤1**）
`\(\hat{\beta}_2\)`的变形：

`$$\begin{align}
\hat{\beta_1} &amp; = \bar{Y}_i-\hat{\beta}_2\bar{X}_i &amp;&amp; \leftarrow \left[ \hat{\beta}_2= \sum{k_iY_i} \right] \\
&amp; = \frac{1}{n} \sum{Y_i} - \sum{\left( k_iY_i \cdot \bar{X} \right)} \\
&amp; = \sum{\left( (\frac{1}{n} - k_i\bar{X}) \cdot Y_i  \right)}   &amp;&amp; \leftarrow \left[ w_i = \frac{1}{n} - k_i\bar{X} \right]\\     
&amp; = \sum{w_iY_i}
\end{align}$$`

- 其中：令
`\(w_i = \frac{1}{n} - k_i\bar{X}\)`

---
## 截距系数的方差和样本方差

证明过程：（**步骤2**）计算
`\(\hat{\beta}_1\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）：

`$$\begin{align}
\sigma^2_{\hat{\beta}_1} &amp; \equiv  Var(\hat{\beta_1})  = Var(\sum{w_iY_i}) \\
&amp; = \sum{\left( w_i^2Var(\beta_1 +\beta_2X_i + u_i) \right)} &amp;&amp; \leftarrow \left[w_i = \frac{1}{n} - k_i\bar{X} \right]\\
&amp; = \sum{\left( 
            \left( \frac{1}{n} - k_i\bar{X} \right)^2Var(u_i) 
         \right)} \\
&amp; = \sigma^2 \cdot \sum{ \left( \frac{1}{n^2} - \frac{2 \bar{X} k_i}{n} + k_i^2 \bar{X}^2 \right) }  &amp;&amp; \leftarrow \left[ \sum{k_i} = \sum{\left( \frac{x_i}{\sum{x_i^2}} \right)= \frac{\sum{x_i}} {\sum{x_i^2}}}=0 \right] \\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2\sum{k_i^2} \right)  &amp;&amp; \leftarrow \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right]\\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2\sum{ \left( \frac{x_i}{\sum{x_i^2}} \right) ^2} \right) 
\end{align}$$`

---
## 截距系数的方差和样本方差

证明过程：（**步骤2**）计算
`\(\hat{\beta}_1\)`的**总体方差**（
`\(\sigma^2_{\hat{\beta}_1}\)`）：

`$$\begin{align}
&amp; = \sigma^2 \cdot \left( \frac{1}{n} + \bar{X}^2  \frac{\sum{x_i^2}}{\left( \sum{x_i^2} \right)^2}  \right) \\
&amp; = \sigma^2 \cdot \left( \frac{1}{n} +   \frac{ \bar{X}^2 } { \sum{x_i^2} }  \right) \\
&amp; =  \frac{\sum{x_i^2} + n\bar{X}^2} {n\sum{x_i^2}} \cdot \sigma^2 &amp;&amp; \leftarrow  \left[ \sum{x_i^2} + n\bar{X}^2 = \sum{(X_i-\bar{X})^2} + n\bar{X}^2 = \sum{X_i^2}\right]\\
&amp; = \frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

---
class: inverse, middle, center, animated, bounceInDown
# 最小二乘估计量的性质


---

## 估计量好坏的评价指标

如何评价估计量的好坏？一个图形说明：

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-BLUE-demo-manu.png" alt="估计量的性质的一个图形说明" width="555" /&gt;
&lt;p class="caption"&gt;估计量的性质的一个图形说明&lt;/p&gt;
&lt;/div&gt;

---
## 估计量好坏的评价指标

某种参数**估计方法**（如OLS方法），得到的**估计量**（如
`\(\hat{\beta}_2,\hat{\beta}_1, \hat{\sigma}^2\)`）是总体**参数**（如
`\(\beta_2, \beta_1, \sigma^2\)`）的**最优线性无偏估计量**（**B**est **L**inear **U**nbiased **E**stimate，**BLUE**）需要满足如下三个条件：

- 线性的(Linear)：估计量是因变量
`\(Y_i\)`的线性函数。

- 无偏的(Unbiased)：**估计量**的均值或期望值（
`\(E(\hat{\beta}_i)\)`）等于**参数**的真值（
`\(\beta_i\)`）。

- 方差最小的（Best）：也即估计量是最有效的(Efficient)，是所有线性无偏估计量中有最小方差的那个估计量。

---
## 高斯-马尔可夫定理(Gauss-Markov Theorem)：

**高斯-马尔可夫定理**(Gauss-Markov Theorem)：在给定经典线性回归模型(CLRM)的假定下，最小二乘(OLS)**估计量**（如
`\(\hat{\beta}_2,\hat{\beta}_1,\hat{\sigma}^2\)`），在无偏线性估计量一类中，有最小方差，就是说它们是**总体参数**（如
`\(\beta_2, \beta_1, \sigma^2\)`）的**最优线性无偏估计量**(BLUE)。

--

课堂讨论：

- 讨论1：为什么最小二乘法（OLS）被计量学家奉为神明？还有其他选择吗？

- 讨论2：OLS得到的BLUE为到底有什么值得你称赞？

- 讨论3：OLS得到BLUE还需要CLRM假设以外的更多假设吗？(正态性？？)

---
## OLS方法最优线性无偏估计性质的证明

不同估计方法下两个估计量的抽样分布

.pull-left[

&lt;img src="pic/chpt3-BLUE-demo.png" width="304" style="display: block; margin: auto;" /&gt;
]

.pull-right[

- 图(a) **OLS方法**下估计量
`\(\hat{\beta}_2\)`是总体参数
`\(\beta_2\)`的一个**线性无偏估计量**

- 图(b) **其他某种方法**下估计量
`\(\hat{\beta}_2^{\ast}\)`也是总体参数
`\(\beta_2\)`的一个**线性无偏估计量**

- 图(c)  那么哪一个估计量（
`\(\hat{\beta}_2\)`还是
`\(\hat{\beta}_2^{\ast}\)`）更能为我们所接受呢？

课程讨论：

- 讨论1：什么是抽样分布？

- 讨论2：怎样获得估计量分布？

- 讨论3：没有比OLS估计量更好的估计量了吗？
]

---
### 性质1：线性性

**线性性**（Linearity）：是指
`\(\hat{\beta}_2\)`和
`\(\hat{\beta}_1\)`对
`\(Y_i\)`是线性的。

具体证明过程如下：

**步骤1**：证明斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} &amp;&amp; \leftarrow  \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right] \\
\end{align}$$`

又因为
`\(k_i =\frac{x_i}{\sum{x_i^2}}\)`是不全为0的（为什么？），所以斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

---
### 性质1：线性性

详细证明（反证法）：

- 假设
`\(H_0\)`：
`\(k_i =\frac{x_i}{\sum{x_i^2}}= 0\)`，也即全为零。

- 则有：
`\(x_i= X_i-\bar{X}=0\)`，

- 则有：
`\(X_i\)`处处等于
`\(\bar{X}\)`，

- 也就意味着：
`\(x_i\)`是一个不变的量（只有一个取值）

- 因此，这是明显违背CLRM假设中关于自变量
`\(X_i\)`的设定（见前面）。

- 因此，
`\(H_0\)`是显然不成立的，认为
`\(k_i\)`不能全为零。[证明完毕]


---
### 性质1：线性性

**步骤2**：证明截距系数估计量
`\(\hat{\beta}_1\)`对
`\(Y_i\)`是线性的。

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{w_iY_i} &amp;&amp; \leftarrow \left[ w_i = \frac{1}{n} - k_i\bar{X} \right]
\end{align}$$`

又因为
`\(w_i = \frac{1}{n} - k_i\bar{X}\)`是不全为0的（为什么？），所以斜率系数估计量
`\(\hat{\beta}_2\)`对
`\(Y_i\)`是线性的。

---
### 性质1：线性性

详细证明（反证法）：

- 假设
`\(H_0\)`：
`\(w_i = \frac{1}{n} - k_i\bar{X}= 0\)`，也即全为零。

- 则有：
`\(\sum{w_i} = \sum{(\frac{1}{n} - k_i\bar{X})}=0\)`，

- 则有：
`\(1-\bar{X}\sum{k_i}=0\)`，

- 又因为：
`\(\sum{k_i} = \sum{\frac{x_i}{\sum{x_i^2}}=\frac{\sum{x_i}}{\sum{x_i^2}} }=0\)`，

- 因此有：
`\(1-0=0\)`，也即
`\(1=0\)`

- 因此，这显然是错误的。

- 因此
`\(H_0\)`是显然不成立的，认为
`\(w_i\)`不能全为零。[证明完毕]

---
### 性质2：无偏性

**无偏性**(Unbias)：**估计量**期望值（
`\(E(\hat{\beta}_i)\)`）等于**参数**的真值（
`\(\beta_i\)`）。

**步骤1**：证明斜率系数估计量
`\(\hat{\beta}_2\)`是无偏的，也即
`\(E(\hat{\beta}_2)= \beta_2\)`。

容易有：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{k_iY_i} &amp;&amp; \leftarrow  \left[ k_i =\frac{x_i}{\sum{x_i^2}} \right] \\
&amp; = \sum{k_i(\beta_1 +\beta_2X_i +u_i)} \\
&amp; = \beta_1\sum{k_i}+\beta_2\sum{k_iX_i} + \sum{k_iu_i}\\
&amp; = 0 + \beta_2 +\sum{k_iu_i}
\end{align}$$`


---
### 性质2：无偏性

其中：

`$$\begin{align}
\sum{k_i}  &amp;= \sum{\frac{x_i}{\sum{x_i^2}}=\frac{\sum{x_i}}{\sum{x_i^2}} }=0 \\
 \sum{k_iX_i} 
&amp;= \sum{\left[ \frac{x_i}{\sum{x_i^2} }\cdot X_i \right]} 
= \frac{ \sum{x_iX_i}} {\sum{x_i^2}} 
= \frac{ \sum{x_i(x_i+ \bar{X})}} {\sum{x_i^2}} \\
&amp;= \frac{ \sum{x_i^2+x_i \bar{X}}} {\sum{x_i^2}} 
= \frac{ \sum{x_i^2}+\bar{X}\sum{x_i} } {\sum{x_i^2}} 
= 1
\end{align}$$`

所以有：

`$$\begin{align}
\hat{\beta}_2 &amp; =  \beta_2 +\sum{k_iu_i} \\
E(\hat{\beta}_2) &amp; =  E(\beta_2 +\sum{k_iu_i}) \\
&amp; =  \beta_2 +E(\sum{k_iu_i}) \\
&amp; =  \beta_2 +\sum{\left[ k_iE(u_i) \right]} \\
&amp; =  \beta_2
\end{align}$$`

**[证明完毕]**！

---
### 性质2：无偏性

**步骤2**：证明截距系数估计量
`\(\hat{\beta}_1\)`是无偏的，也即
`\(E(\hat{\beta}_1)= \beta_1\)`。

容易有：

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{w_iY_i} &amp;&amp; \leftarrow \left[ w_i = \frac{1}{n} - k_i\bar{X} \right] \\
&amp; = \sum{w_i(\beta_1 +\beta_2X_i +u_i)} \\
&amp; = \beta_1\sum{w_i} + \beta_2\sum{w_iX_i} + \sum{w_iu_i}\\
&amp; = \beta_1 + 0 +\sum{k_iu_i}
\end{align}$$`

---
### 性质2：无偏性

其中：

`$$\begin{align}
&amp;\sum{w_i}  = \sum{ \left[ \frac{1}{n} - k_i\bar{X} \right]}
= 1- \bar{X}\sum{k_i} 
= 1 \\
&amp; \sum{w_iX_i} 
= \sum{\left[ \left( \frac{1}{n} - k_i\bar{X} \right) \cdot X_i \right] }
= \sum{\left( \frac{X_i}{n} - \bar{X} k_i X_i \right) }
= \bar{X} -\bar{X}\sum{( k_i X_i) }
= 0
\end{align}$$`

所以有：

`$$\begin{align}
\hat{\beta}_1 &amp; =  \beta_2 +\sum{w_iu_i} \\
E(\hat{\beta}_1) &amp; =  E(\beta_1 +\sum{k_iu_i}) \\
&amp; =  \beta_1 +E(\sum{k_iu_i}) \\
&amp; =  \beta_1 +\sum{\left[ k_iE(u_i) \right]} \\
&amp; =  \beta_1
\end{align}$$`

**[证明完毕]**！

---
### 性质3：方差最小性

**方差最小性**（Best）：也即估计量是最有效的(Efficient)，是所有线性无偏估计量中，方差为最小的那个估计量。

证明：

已知估计量
`\(\hat{\beta}_2\)`和
`\(\hat{\beta}_1\)`的**总体方差**分别是：

`$$\begin{align}
Var(\hat{\beta}_2) \equiv \sigma_{\hat{\beta}_2}^2  &amp;=\frac{\sigma^2}{\sum{x_i^2}} \\
Var(\hat{\beta}_1) \equiv \sigma_{\hat{\beta}_1}^2  &amp;=\frac{\sum{X_i^2}}{n} \cdot \frac{\sigma^2}{\sum{x_i^2}}  
\end{align}$$`

---
### 性质3：方差最小性

假设存在用其他方法估计的线性无偏估计量
`\(\hat{\beta}_2^{\ast}\)`和
`\(\hat{\beta}_1^{\ast}\)`：

`$$\begin{align}
\hat{\beta}_2^{\ast} = \sum{\left[ (k_i + d_i)Y_i \right]} = \sum{c_iY_i}\\
\hat{\beta}_1^{\ast} = \sum{\left[ (w_i + g_i)Y_i \right]} = \sum{h_iY_i}
\end{align}$$`

其中，
`\(d_i\)`和
`\(g_i\)`为不全为零的常数（证明略），则可以证明（此处略）：

`$$\begin{align}
Var(\hat{\beta}_2^{\ast} ) \geq Var(\hat{\beta}_2)  \\
Var(\hat{\beta}_1^{\ast}) \geq  Var(\hat{\beta}_1) 
\end{align}$$`

因此，方差最小性得以证明！

---
class: inverse, middle, center
# 拟合优度


---
## Y变异的分解

&lt;img src="pic/chpt2-1-PRL-SRL.png" width="601" style="display: block; margin: auto;" /&gt;


`$$\begin{alignedat}{2}
&amp;&amp;(Y_i - \bar{Y}) &amp;&amp;= (\hat{Y}_i - \bar{Y}) &amp;&amp;+ (Y_i - \hat{Y}_i ) \\
&amp;&amp;y_i &amp;&amp;= \hat{y}_i &amp;&amp;+ e_i 
\end{alignedat}$$`

---
## 平方和分解

`$$\begin{alignedat}{2}
&amp;&amp;(Y_i - \bar{Y}) &amp;&amp;= (\hat{Y}_i - \bar{Y}) &amp;&amp;+ (Y_i - \hat{Y}_i ) \\
&amp;&amp;y_i &amp;&amp;= \hat{y}_i &amp;&amp;+ e_i \\
&amp;&amp;\sum{y_i^2} &amp;&amp;= \sum{\hat{y}_i^2} &amp;&amp;+ \sum{e_i^2} \\
&amp;&amp;TSS &amp;&amp;=ESS &amp;&amp;+RSS
\end{alignedat}$$`

- 其中：
`\(TSS\)`表示**总离差平方和**;
`\(ESS\)`表示**回归平方和**;
`\(RSS\)`表示**残差差平方和**

`$$\begin{align}
\sum{y_i^2} &amp;= \sum{(\hat{y}_i e_i)^2} \\
&amp;= \sum{(\hat{y}_i^2 +2\hat{y}_ie_i +e_i^2)}\\
&amp;= \sum{\hat{y}_i^2 } +2\sum{\hat{y}_ie_i} + \sum{e_i^2}\\
&amp;= \sum{\hat{y}_i^2 } +2\sum{\left( \hat{(\beta_2}x_i)e_i \right)} + \sum{e_i^2}\\
&amp;= \sum{\hat{y}_i^2 } +2\hat{\beta_2}\sum{\left( x_ie_i \right)} + \sum{e_i^2} &amp;&amp; \leftarrow \left[ \sum{x_ie_i} =0 \right]\\ 
&amp;= \sum{\hat{y}_i^2} + \sum{e_i^2}
\end{align}$$`

---
## 拟合优度的度量


**拟合优度**（Goodness of fit）：判断样本回归线对一组数据拟合优劣水平的度量。

**判定系数**（coefficient of determination）：一种利用平方和分解，考察样本回归线对数据拟合效果的总度量。一元回归中，一般记为
`\(r^2\)`；多元回归中，一般记为
`\(R^2\)`。

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt3-fitness-venn.png" alt="维恩图看拟合优度" width="763" /&gt;
&lt;p class="caption"&gt;维恩图看拟合优度&lt;/p&gt;
&lt;/div&gt;

---
## 拟合优度的度量

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt2-1-PRL-SRL.png" alt="平方和分解看拟合优度" width="651" /&gt;
&lt;p class="caption"&gt;平方和分解看拟合优度&lt;/p&gt;
&lt;/div&gt;

---
## 拟合优度的度量

判定系数
`\(r^2\)`计算公式1：

`$$\begin{align}
r^2 &amp;=\frac{ESS}{TSS} = \frac{\sum{(\hat{Y}_i - \bar{Y})^2}}{\sum{(Y_i - \bar{Y})^2}} 
\end{align}$$`

判定系数
`\(r^2\)`计算公式2：

`$$\begin{align}
r^2 &amp;=1- \frac{RSS}{TSS} = 1- \frac{\sum{e_i^2}}{\sum{(Y_i - \bar{Y})^2}} \\
\end{align}$$`

---
## 拟合优度的度量

判定系数
`\(r^2\)`计算公式3：

`$$\begin{align}
r^2 &amp;=\frac{ESS}{TSS} 
= \frac{\sum{\hat{y}_i^2}}{\sum{y_i^2}} 
= \frac{\sum{(\hat{\beta}_2x_i)^2}}{\sum{y_i^2}} 
= \hat{\beta}_2^2\frac{\sum{x_i^2}}{\sum{y_i^2}} 
= \hat{\beta}_2^2 \frac{S_{X_i}^2}{S_{Y_i}^2}
\end{align}$$`

判定系数
`\(r^2\)`计算公式4：

`$$\begin{align}
r^2 &amp;= \hat{\beta}_2^2 \cdot \frac{\sum{x_i^2}}{\sum{y_i^2}} 
= \left( \frac{\sum{x_iy_i}}{\sum{x_i^2}} \right)^2 \cdot \left( \frac{\sum{x_i^2}}{\sum{y_i^2}} \right)
= \frac{(\sum{x_iy_i})^2}{\sum{x_i^2 }\sum{y_i^2}}
\end{align}$$`

课堂讨论：

- 讨论1： 
`\(r^2\)`是一个非负量。为什么？

- 讨论2：
`\(0 \leq r^2 \leq 1\)`，两个端值分别意味什么？

---

## 判定系数和简单相关系数的区别与联系

**总体相关系数**：是变量
`\(X_i\)`与变量
`\(Y_i\)`总体相关关系的参数，一般记为
`\(\rho\)`。

`$$\begin{align}
\rho &amp;=\frac{Cov(X,Y)}{\sqrt{Var(X_i)Var(Y_i)}}
=\frac{E(X_i-EX)(Y_i-EY)}{\sqrt{E(X_i-EX)^2E(Y_i-EY)^2}}
\end{align}$$`

**样本相关系数**：是从总体中抽取随机样本，获得变量
`\(X_i\)`与变量
`\(Y_i\)`样本相关关系的统计量度量，一般记为
`\(r\)`。

`$$\begin{align}
r &amp;=\frac{S_{XY}^2}{S_X\ast S_Y}
=\frac{\sum{(X_i-\bar{X})(Y_i-\bar{Y})}}{\sqrt{\sum{(X_i-\bar{X}})^2\sum{(Y_i-\bar{Y})^2}}}
= \frac{\sum{x_iy_i}}{\sqrt{\sum{x_i^2 }\sum{y_i^2}}}
\end{align}$$`

---

## 判定系数和相关系数的区别与联系

判定系数和简单相关系数的联系:

- 在一元回归中，判定系数
`\(r^2\)`等于样本相关系数
`\(r\)`的平方。

判定系数和简单相关系数的区别：

- 判定系数
`\(r^2\)`表明因变量变异由解释变量所解释的比例，而相关系数
`\(r\)`只能表明变量间的线性关联强度。

- 在多元回归中，这种区别会更加凸显！因为那时的相关系数r出现了偏相关的情形(交互关联)！

---
class: inverse, middle, center
# 一个数值例子

受教育程度（
`\(X_i\)`，年）与时均工资(
`\(Y_i\)`，美元/小时)。


---
## 计算表FF和ff



&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; obs &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(X_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(Y_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(X_iY_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(X_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(Y_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(x_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(y_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(x_iy_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(x_i^2\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(y_i^2\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.46 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 26.74 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 19.86 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -6.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -4.22 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 25.31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 17.79 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.77 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 40.39 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 49.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 33.29 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -5.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -2.90 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 14.52 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 25.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.44 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.98 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 47.83 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 64.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 35.74 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -4.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -2.70 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.78 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.27 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.33 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 65.99 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 81.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 53.75 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -3.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.34 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.03 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.80 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.32 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 73.18 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 100.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 53.56 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -2.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.36 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.71 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.84 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.58 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 72.43 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 121.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 43.35 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -2.09 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.09 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.37 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.82 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 93.82 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 144.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 61.12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.86 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.73 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.84 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 101.86 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 169.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 61.39 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.84 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.84 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.70 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 14.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.02 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 154.31 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 196.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 121.49 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.35 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.70 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.51 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 15.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.67 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 160.11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 225.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 113.93 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 3.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.00 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.84 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 173.38 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 256.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 117.42 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.16 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.65 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 16.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.67 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 17.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.62 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 231.46 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 289.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 185.37 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.94 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 24.70 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 25.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 24.41 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 18.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.53 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 243.56 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 324.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 183.09 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.86 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 29.14 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 36.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 23.58 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sum &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 156.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 112.77 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1485.04 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2054.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1083.38 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 131.79 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 182.00 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 105.12 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

---

## 计算回归系数

公式1: （Favorite Five，FF形式）

`$$\begin{align}
\hat{\beta}_2 &amp;=\frac{n\sum{X_iY_i}-\sum{X_i}\sum{Y_i}}{n\sum{X_i^2}-\left ( \sum{X_i} \right)^2}\\
&amp;=\frac{13\ast1485.04-156\ast112.771}{13\ast2054-156^2}=0.7241
\end{align}$$`


`$$\begin{align}
\hat{\beta_1} &amp;= \bar{Y} - \hat{\beta}_2 \bar{X}
=8.6747-0.7241\ast12=-0.0145
\end{align}$$`

---

## 计算回归系数

公式2：（离差形式，favorite five，ff形式）


`$$\begin{align}
\hat{\beta}_2 =\frac{\sum{x_iy_i}}{\sum{x_i^2}}
=\frac{131.786}{182}=0.7241
\end{align}$$`


`$$\begin{align}
\hat{\beta_1} = \bar{Y} - \hat{\beta}_2 \bar{X}
=8.6747-0.7241\ast12=-0.0145
\end{align}$$`

---
## 样本回归方程SRF

`$$\begin{align}
\hat{Y}_i= \hat{\beta}_1 + \hat{\beta}_2 X_i
=-0.0145+0.7241X_i
\end{align}$$`

---
## 样本回归线SRL

&lt;img src="03-simple-reg-parameter-estimate-slide_files/figure-html/unnamed-chunk-26-1.png" style="display: block; margin: auto;" /&gt;

---
## 样本回归线SRL

&lt;img src="03-simple-reg-parameter-estimate-slide_files/figure-html/unnamed-chunk-27-1.png" style="display: block; margin: auto;" /&gt;

---
## 计算得到拟合值和残差


&lt;table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;"&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; obs &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(X_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(Y_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(\hat{Y}_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(e_i\)` &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; `\(e_i^2\)` &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.4567 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 4.3301 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.1266 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0160 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.7700 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.0542 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.7158 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5123 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 3 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.9787 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 5.7783 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.2004 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0402 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.3317 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.5024 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8293 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.6877 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 5 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.3182 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.2265 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0917 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0084 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 6 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 6.5844 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.9506 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.3662 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.8665 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 7 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.8182 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 8.6747 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.8565 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.7336 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 7.8351 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.3988 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -1.5637 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 2.4452 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 9 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 14.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.0223 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.1229 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8994 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.8089 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 10 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 15.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.6738 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.8470 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.1732 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0300 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 11 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 16.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 10.8361 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 11.5711 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; -0.7350 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5402 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 17.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.6150 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 12.2952 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.3198 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 1.7419 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; 13 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 18.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.5310 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 13.0193 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.5117 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.2618 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; sum &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 156.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 112.7712 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 112.7712 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 0.0000 &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; 9.6928 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
.pull-right[
`$$\begin{align}
\hat{Y}_i &amp;=\hat{\beta}_1 +\hat{\beta}_2X_i\\
e_i &amp;= Y_i - \hat{Y}_i
\end{align}$$`
]

???
根据以上样本回归方程，可以计算得到
`\(Y_i\)`的回归拟合值
`\(\hat{Y}_i\)`，以及回归残差
`\(e_i\)`。

---
## 回归误差方差和标准差

回归误差方差
`\(\hat{\sigma}^2\)`

`$$\begin{align}
\hat{\sigma}^2= \frac{\sum{e_i^2}} {(n-2)}
=\frac{9.693}{11}=0.8812
\end{align}$$`

回归误差标准差
`\(\hat{\sigma}\)`：

`$$\begin{align}
\hat{\sigma}=\sqrt{\frac{\sum{e_i^2}}{(n-2)}}
=\sqrt{0.8812}=0.8812
\end{align}$$`

---
## 计算回归系数的样本方差

`$$\begin{align}
S^2_{\hat{\beta}_2} &amp;= \frac{\hat{\sigma}^2} {\sum{x_i^2}}
=\frac{0.8812}{182}=0.0048\\
S_{\hat{\beta}_2} &amp;= \sqrt{\frac{\hat{\sigma}^2} {\sum{x_i^2}}}
=\sqrt{0.0048}=0.0696
\end{align}$$`

`$$\begin{align}
S^2_{\hat{\beta}_1} &amp;= \frac{\sum{X_i^2}} {n} \frac{\hat{\sigma}^2} {\sum{x_i^2}}
=\frac{2054}{13}\frac{0.8812}{182}=0.765\\
S_{\hat{\beta}_1} &amp;= \sqrt{\frac{\sum{X_i^2}}{n}\frac{\hat{\sigma}^2} {\sum{x_i^2}}}
=\sqrt{0.765}=0.8746
\end{align}$$`

---
## 计算平方和分解

`$$\begin{align}
TSS &amp;= \sum{Y_i- \bar{Y}}=105.1183\\
RSS &amp;= \sum{Y_i- \hat{Y}_i}=9.693\\
ESS &amp;= \sum{\hat{Y}_i- \bar{Y}}=95.4253
\end{align}$$`

---
## 相关系数和判定系数

样本相关系数
`\(r\)`：

`$$\begin{align}
r =\frac{S_{XY}^2}{S_X\ast S_Y}=\frac{10.9821}{3.8944\ast2.9597} =0.9528\\
\end{align}$$`

回归方程的判定系数
`\(r^2\)`：

`$$\begin{align}
r^2 &amp;= 1- \frac{RSS}{TSS}=1-\frac{9.693}{105.1183} =0.9078\\
\end{align}$$`

二者关系

---
class: inverse, center, middle
# 经典正态线性回归模型（CNLRM）


---
## 随机干扰项的正态性假设

CLRM假设下：OLS方法得到的已经是BLUE了

CLRM假设下：随机干扰项
`\(u_i\)`的期望值为零（
`\(Eu_i=0\)`），
`\(u_i, u_j\)`不相关（
`\(E(u_i,u_j)=0\)`），且有一个不变方差（
`\(Var(u_i) \equiv \sigma^2\)`）。

总体回归模型PRM:

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i 
\end{align}$$`

样本回归模型SRM:

`$$\begin{align}
Y_i &amp;= \hat{\beta}_1 + \hat{\beta}_2X_i +e_i 
\end{align}$$`

--

讨论：

- 一个样本怎样才推断总体PRF？

- 多份样本而言，OLS估计量（
`\(\hat{\beta}_i,\hat{\sigma}^2\)`）因为样本变化而变化(随机变量)

---
## 随机干扰项的正态性假设

随机干扰项
`\(u_i\)`对于估计SRF
`\(\rightarrow\)`推断PRF的重要性：

`$$\begin{align}
\hat{\beta}_2 &amp; = \sum{ \left( \frac{x_i}{\sum{x_i^2}} \cdot Y_i \right)} = \sum{k_iY_i}  = \sum{ \left( k_i(\beta_1 +\beta_2X_i +u_i) \right) } 
\end{align}$$`

`$$\begin{align}
\hat{\beta_1} &amp; = \sum{\left( \left( \frac{1}{n} - k_i\bar{X} \right) \cdot Y_i \right)} = \sum{w_iY_i} = \sum{ \left() w_i(\beta_1 +\beta_2X_i +u_i) \right) } 
\end{align}$$`

`$$\begin{align}
\hat{\sigma}^2=\frac{\sum{e_i^2}}{n-2}
\end{align}$$`


---
## 经典正态线性回归模型假设（N-CLRM）

**经典正态线性回归模型**(classical normal linear regression model , N-CLRM)：在经典线性回归模型(CLRM)假设中再增加干扰项
`\(u_i\)`服从正态性的相关假设。

- 均值为0：
`\(E(u|X_i)=0\)`

- 同方差：
`\(Var(u_i) \equiv \sigma^2\)`

- 无自相关：
`\(E(u_i,u_j)=0\)`

- 正态性分布：
`\(u_i \sim N(0, \sigma^2)\)`

以上几条也可以统写为：
`\(u_i \sim iid. \  N(0, \sigma^2)\)`

其中，iid表示独立同分布(Independent Identical Distribution, iid)。

---
## N-CLRM假设下OLS估计量的性质

在N-CLRM假设下，OLS估计量有如下统计性质：

- 性质1：无偏性

- 性质2：有效性（方差最小）

- 性质3：一致性（收敛到它们的总体参数上）

---
## N-CLRM假设下OLS估计量的性质

- 性质4：估计量
`\(\hat{\beta}_2\)`是正态分布的：

`$$\begin{align}
\hat{\beta}_2 &amp; \sim N(\mu_{\hat{\beta}_2}, \sigma^2_{\hat{\beta}_2}) \\
\mu_{\hat{\beta}_2} &amp; = E(\hat{\beta}_2) = \beta_2 \\
\sigma^2_{\hat{\beta}_2} &amp; = \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

随机变量
`\(Z_2\)`服从标准正态分布：

`$$\begin{align}
Z_2 &amp;=\frac{\hat{\beta}_2- \beta_2}{\sigma_{\hat{\beta}_2}} \sim N(0,1)\\
\mu(Z_2) &amp; = E(\hat{\beta}_2- \beta_2) =0 \\
\sigma_{Z_2}^2 &amp; = Var \left( \frac{\hat{\beta}_2- \beta_2 }{\sigma_{\hat{\beta}_2}} \right)= \frac{Var(\hat{\beta}_2)}{\sigma^2_{\hat{\beta}_2}} =1
\end{align}$$`

---
## N-CLRM假设下OLS估计量的性质

- 性质5：估计量
`\(\hat{\beta}_1\)`是正态分布的：

`$$\begin{align}
\hat{\beta}_1 &amp; \sim N(\mu_{\hat{\beta}_1}, \sigma^2_{\hat{\beta}_1}) \\
\mu_{\hat{\beta}_1} &amp; = E(\hat{\beta}_1) = \beta_1 \\
\sigma^2_{\hat{\beta}_1} &amp; = \frac{\sum{X_i^2}}{n} \cdot  \frac{\sigma^2}{\sum{x_i^2}}
\end{align}$$`

随机变量
`\(Z_1\)`服从标准正态分布：


`$$\begin{align}
Z_1 &amp;=\frac{\hat{\beta}_1- \beta_1}{\sigma_{\hat{\beta}_1}} \sim N(0,1)\\
\mu(Z_1) &amp; = E(\hat{\beta}_1- \beta_1) =0 \\
\sigma_{Z_1}^2 &amp; = Var \left( \frac{\hat{\beta}_1- \beta_1 }{\sigma_{\hat{\beta}_1}} \right)= \frac{Var(\hat{\beta}_1)}{\sigma^2_{\hat{\beta}_1}} =1
\end{align}$$`

---
## N-CLRM假设下OLS估计量的性质

&lt;img src="pic/chpt3-N-CLRM.png" width="2043" style="display: block; margin: auto;" /&gt;

---
## N-CLRM假设下OLS估计量的性质

- 性质6：
`\(X \equiv (n-2)\hat{\sigma^2}/\sigma^2\)`服从自由度为
`\((n-2)\)`的卡方分布。

`$$\begin{align}
X &amp; \equiv (n-2)\hat{\sigma}^2/\sigma^2 \\
X &amp; \sim \chi^2(n-2)
\end{align}$$`

- 性质7：随机变量
`\((\hat{\beta}_2, \hat{\beta}_1)\)`的分布独立于随机变量
`\(\hat{\sigma}^2\)`

- 性质8：估计量
`\((\hat{\beta}_2, \hat{\beta}_1)\)`在所有无偏估计中，无论是线性还是非线性，都有最小的方差。也即，它们是最有无偏估计量（**B**est **U**nbiased **E**stimators, **BUE**）。

---
class: inverse, center, middle
# 极大似然估计法(ML)

---
## 极大似然估计法(ML)

**极大似然估计法**(maximum likelihood, ML):

- 是由Fisher提出的一种参数估计方法基本思想：设总体分布的函数形式已知，但有未知参数
`\(\Theta\)`，
`\(\Theta\)`可以取很多值，在
`\(\Theta\)`的一切可能取值中选一个使样本观察值出现的概率为最大的
`\(\hat{\Theta}\)`值作为
`\(\Theta\)`的估计值，并称估计值
`\(\hat{\Theta}\)`为参数
`\(\Theta\)`的极大似然估计值。这种求估计量的方法称为极大似然估计法。

似然函数表达式：

- 设总体
`\(X_i\)`的概率密度函数
`\(f(X_i; \Theta)\)`为,其中
`\(\Theta\)`为待估计参数。对于从总体中取得的样本观测值
`\((X_1; X_2, \cdots , X_n)\)` ， 其联合密度函数为
`\(\prod{ f(X_i; \Theta)}\)` ，它是参数
`\(\Theta\)`的函数，称之为的似然函数，记为
`\(L(\Theta)\)`：

`$$\begin{align}
L(\Theta) = \prod{ f(X_i;\Theta)}
\end{align}$$`

---
## ML估计法与OLS估计法的关系

极大似然估计法(ML)比较复杂，我们仅需知道。在随机干扰项正态性假设下（N-CLRM）：

- 回归系数
`\(\beta_i\)`的ML估计量和OLS估计量是相同的——无论是一元回归还是多元回归!

- 对于
`\(\sigma^2\)`的估计，其ML估计量为
`\(\sum{e_i^2}/n\)`，是**有偏**的；其OLS估计量是
`\(\sum{e_i^2}/(n-2)\)`，是**无偏**的。

- 关于
`\(\sigma^2\)`的两种估计量，随着样本容量n的增大，两者将趋于相等！

启示：OLS方法真好！

---
background-image: url("pic/thank-you-gif-funny-gentle.gif")
class: inverse,center
# 本章结束
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
