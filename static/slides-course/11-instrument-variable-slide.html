<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>11-instrument-variable-slide.utf8.md</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/duke-blue.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge-duke.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="libs\cc-fonts.css" type="text/css" />
    <link rel="stylesheet" href="libs\figure-captions.css" type="text/css" />
    <link rel="stylesheet" href="libs\animate.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

background-image: url("pic/slide-front-page.jpg")
class: center,middle

# 计量经济学(Econometrics)

### 胡华平

### 西北农林科技大学

### 经济管理学院数量经济教研室

### huhuaping01@hotmail.com

### 2019-04-28





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 24px;
    padding: 1em 4em 1em 4em;
}

.page-font-16 {
  font-size: 16px;
}

.page-font-26 {
  font-size: 26px;
}

&lt;/style&gt;

---
class: center, middle,inverse
# 第11章：内生自变量问题

### 简单回归模型中的遗漏变量

### 内生变量法下的估计问题

### 多元回归模型的IV估计

### 工具变量法的一些讨论

---
class: center, middle,inverse
# 简单回归模型中的遗漏变量

---
## 一个简单示例

假定工资水平的**“真实模型”**为：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+\beta_{2} abil_i+u_i
\end {align}$$`

--

然而，因为能力变量（
`\(abil_i\)`）无法观测得到，所以我们经常用智商水平变量（
`\(IQ_i\)`）来替换，并构建如下**“代理变量模型”**：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+\beta_{2} IQ_i+u_i^{\ast}
\end {align}$$`

--

进一步地，如果拿不到智力水平（
`\(IQ_i\)`）数据，我们很可能构建出如下**遗漏重要变量**的**“偏误模型”**：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+v_i
\end {align}$$`

--

- 此时，我们可以认为重要变量
`\(abil_i\)`被遗漏，从而进入到随机干扰项
`\(v_i\)`中。

--

- 又因为，一般情况下我们认为教育水平（
`\(educ_i\)`）与能力（
`\(abil_i\)`）是相关的，从而可以认为
自变量
`\(educ_i\)`与随机干扰项
`\(v_i\)`是相关的。而这是违背CLRM假设的（违背了哪一条？）。

---

## 知识回顾

对于总体回归模型：

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i &amp;&amp; \text{(PRM)}
\end{align}$$`

- 在CLRM假设下：**CLRM假设3**——X是固定的（给定的）或独立于误差项。也即自变量X**不是**随机变量。此时，我们可以使用OLS方法，并得到**BLUE**。

`$$\begin{align}
Cov(X_i, u_i)= 0\\ 
E(X_i u_i)= 0
\end{align}$$`

--

- 如果违背上述假设，也即自变量X与随机干扰项相关。此时使用OLS估计将不再能得到**BLUE**，而应该采用**工具变量法**（IV）进行估计。

`$$\begin{align}
Cov(X_i, u_i)= 0\\ 
E(X_i u_i)= 0
\end{align}$$`

--

事实上，无论
`\(X_i\)`与
`\(u_i\)`是否相关，我们都可以采用**IV法**得到**BLUE**。

---

## 代理变量


**代理变量**（proxy variable）：一般因为某些原因，某个变量
`\(X_i\)`不能直接观测得到（数据不可得），那么常常会找一个能够观测到到的、并与
`\(X_i\)`高度相关的变量
`\(D_i\)`作为替代。

--

- 假定工资水平的**“真实模型”**为：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+\beta_{2} abil_i+u_i
\end {align}$$`

--

- 然而，因为能力变量（
`\(abil_i\)`）无法观测得到，所以我们经常用智商水平变量（
`\(IQ_i\)`）来替换，并构建如下**“代理变量模型”**：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+\beta_{2} IQ_i+u_i^{\ast}
\end {align}$$`

此时，智力水平
`\(IQ_i\)`就可以认为是变量
能力
`\(abil_i\)`的**代理变量**。

---

## 工具变量


**代理变量**（proxy variable）：一般因为某些原因，某个变量
`\(X_i\)`不能直接观测得到（数据不可得），那么常常会找一个能够观测到到的、并与
`\(X_i\)`高度相关的变量
`\(D_i\)`作为替代。

**工具变量**（instrument variable）：一般记为
`\(Z_i\)`，是指一个具备如下**两个性质**的可观测的变量：

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i &amp;&amp; \text{(PRM)}
\end{align}$$`

`$$\begin{align}
Cov(Z_i, u_i) &amp; = 0 &amp;&amp; \text{(instrumental exogeneity)}\\ 
Cov(Z_i, X_i) &amp; \neq 0 &amp;&amp; \text{(instrumental relevance)}
\end{align}$$`

- 工具外生性（instrumental exogeneity）：工具变量对于总体回归模型是外生的。


- 工具相关性（instrumental relevance）：工具变量对于解释
`\(X_i\)`的变异时有很重要的作用。

**思考**： 我们很容易就能检验出模型是不是具有工具外生性或工具相关性么？为什么？

???

从遗漏变量角度来看，工具变量
`\(Z_i\)`对
`\(Y_i\)`没有多余的影响，并且
`\(Z_i\)`与遗漏变量也不相关。

`$$\begin{align}
X_i &amp;=  \pi_1 +\pi_2Z_i + v_i \\
\pi_2 &amp; = \frac{cov(X_i,Z_i)}{var(Z_i)}
\end{align}$$`

---
## 对示例的进一步扩展

对于**“真实模型”**：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+\beta_{2} abil_i+u_i
\end {align}$$`

我们很可能构建出如下**遗漏重要变量**的**“偏误模型”**：

`$$\begin {align} 
\log (wage_i)=\beta_{0}+\beta_{1} educ_i+v_i
\end {align}$$`

因此，对于教育
`\(edu_i\)`的工具变量
`\(Z_i\)`而言：

- 工具变量
`\(Z_i\)`必须与能力
`\(abil_i\)`不相关，但又必须与教育
`\(edu_i\)`相关。

--

- 选择方案1：家庭背景教育如母亲的教育
`\(MotherEdu_i\)`就可能是工具变量备选方案之一。因为我们通常可以认为**母亲教育水平**与孩子教育是正相关的。

--

- 选择方案2：另一个工具变量的备选方案是家庭中**兄弟姐妹的数量**
`\(sibs\)`。因为我们通常可以认为家庭中兄弟姐妹数量会导致较低的平均教育水平（也即负相关）。

**提问**：哪一个方案更好呢？为什么？

???

对于能力
`\(abil_i\)`的工具变量
`\(Z_i\)`：

- 工具变量
`\(Z_i\)`必须与教育
`\(edu_i\)`不相关，但又必须与能力
`\(abil_i\)`相关。

- 智商水平
`\(IQ_i\)`就是一个符合以上条件的、较理想的工具变量。

---
class: middle, center, duke-orange

# 学习成绩与逃课次数的例子


---
## 模型设定

假设**“真实模型”**是：

`$$\begin {align} 
score_i=\alpha_{1}+\alpha_{2}skipped_i+ \alpha_3 abil_i + \alpha_4 mot_i + \alpha_5 income_i +u_i
\end {align}$$`

一个遗漏了重要变量的**“偏误模型”**是：

`$$\begin {align} 
score_i=\beta_{1}+\beta_{2}skipped_i+v_i
\end {align}$$`


- 学习成绩受到逃课次数的影响，但是我们也很担心以上模型中
`\(skipped_i\)`与
`\(v_i\)`中的某些因素相关，例如越有能力
`\(abil_i\)`、越积极
`\(mot_i\)`的学生，逃课也越少。

- 因为自变量
`\(skipped_i\)`可能与随机干扰项
`\(v_i\)`相关。此时，对于以上简单的回归，可能得不出可靠的估计。

---
## 模型设定

`$$\begin {align} 
score_i=\beta_{1}+\beta_{2}skipped_i+v_i
\end {align}$$`

逃课次数
`\(skipped_i\)`的工具变量
`\(Z_i\)`有哪些可供备选的呢？

--

- 宿舍跟上课地点的距离
`\(distance\)`。我们一般认为，它与逃课次数相关
`\(skipped_i\)`，但是它与
`\(v_i\)`中的某些因素也会相关么？

--

- 如果收入水平
`\(income\)`确实影响了学习成绩，但是模型却没有引入收入水平
`\(income\)`变量，也就意味着
`\(v_i\)`中包含了遗漏的重要变量——收入水平
`\(income\)`。此时，距离
`\(distance\)`就会与收入水平
`\(income\)`相关，进而与
`\(v_i\)`相关。——因为收入少的学生，更倾向于在外租房（合租）；收入多的学生，更倾向于住校。

???

- 此外，如果能找到能力
`\(abil_i\)`的一个合适的工具变量
`\(Z_{2i}\)`，那么也可以减弱我们对一元回归模型估计问题的担忧。例如，累计学积分
`\(GPA\)`。

---
class: middle, center, inverse

# 内生变量法下的估计问题

---

## 工具变量法下系数的估计过程

把上述**“偏误模型”**记为：

`$$\begin {align} 
score_i &amp; =\beta_{1}+\beta_{2}skipped_i+u_i \\
Y_i &amp; = \beta_{1}+\beta_{2}X_i+u_i
\end {align}$$`

假设我们找到了理想的工具变量
`\(Z_i\)`，并构建如下的**工具变量模型**：

`$$\begin {align} 
Y_i &amp; = \alpha_{1}+\alpha_{2}Z_i+v_i
\end {align}$$`

`$$\begin {align} 
cov(Z_i, Y_i) &amp; = \alpha_{2}cov(Z_i,X_i)+ cov(Z_i,u_i)  &amp;&amp; \leftarrow [cov(Z_i,u_i)=0] \\
\alpha_2|_{IV}^{plim} &amp; = \frac{cov(Z_i, Y_i)}{cov(Z_i, X_i)} \\
&amp; = \frac{\sum{z_iy_i}}{\sum{z_ix_i}} &amp;&amp; \leftarrow [if \quad X_i=Z_i] \\
&amp; = \frac{\sum{x_iy_i}}{\sum{x_i^2}} =\beta_2
\end {align}$$`

这将意味着工具变量法IV会得到最小二乘法OLS下的估计结果。

---

## 工具变量法下系数的真实方差

.pull-left[

对于**“偏误模型”**和**工具变量模型**：

`$$\begin{align}
Y_i &amp;=  \beta_1 +\beta_2X_i + u_i &amp;&amp; \text{(PRM)} \\
Y_i &amp;=  \alpha_1 +\alpha_2Z_i + v_i &amp;&amp; \text{(IV)}
\end{align}$$`

如果如下三个条件成立：

`$$\begin{align}
Cov(Z_i, u_i) &amp; = 0 \\ 
Cov(Z_i, X_i) &amp; \neq 0 \\
E(v_i^2|Z_i) &amp; \equiv \sigma^2 \equiv var(u_i)
\end{align}$$`

]

--

.pull-right[

可以证明斜率系数
`\(\alpha_2\)`的渐近方差为：

`$$\begin{align}
var(\alpha_2) \simeq \frac{\sigma^2}{n \sigma^2_{X_i} \rho^2_{(X_i,Z_i)}} 
\end{align}$$`


其中：

- `\(\sigma^2\)`是
`\(v_i\)`的**总体方差**，也即
`\(var(v_i) \equiv \sigma^2\)`。

- `\(\sigma^2_{X_i}\)`是
`\(X_i\)`的**总体方差**，也即
`\(var(X_i) \equiv \sigma^2_{X_i}\)`。

- `\(\rho^2_{(X_i,Z_i)}\)`是
`\(X_i\)`和
`\(Z_i\)`的**总体相关系数**的平方，也即
`\(\rho^2_{(X_i,Z_i)} \equiv \frac{[cov(X_i,Z_i)]^2}{var(X_i)var(Z_i)}\)`；
]

---

## 工具变量法下系数的样本方差

对于给定的样本数据，我们可以计算出

`$$\begin{align}
var(\alpha_2) \simeq \frac{\sigma^2}{n \sigma^2_{X_i} \rho^2_{(X_i,Z_i)}}  
\simeq \frac{\hat{\sigma}^2}{n S^2_{X_i} R^2_{(X_i,Z_i)}} 
\end{align}$$`

其中：

- `\(\sigma^2_{X_i} \simeq S^2_{X_i}=\frac{\sum{(X_i-\bar{X})^2}}{n-1}\)`。

- `\(\rho^2_{(X_i,Z_i)}\simeq R^2\)`，其中
`\(R^2\)`为通过做
`\(X_i\)`对
`\(Z_i\)`的回归来获得的**判定系数**。

`$$\begin{align}
X_i = \hat{\pi}_1 +\hat{\pi}_2 Z_i + \epsilon_i 
\end{align}$$`

- `\(\hat{\sigma}^2 = \frac{\sum{e_i^2}}{n-2}\)`，是来自对**工具变量回归**的残差计算。

`$$\begin{align}
Y_i = \hat{\alpha}_1 +\hat{\alpha}_2 Z_i + e_i 
\end{align}$$`

---
class: middle, center, duke-orange

# 已婚女性的教育回报案例




---

## 变量说明

研究者关注428名已婚女性**时均工资**
`\(wage\)`与其**受教育年数**
`\(educ\)`之间的关系，并考虑如下变量：

<div id="htmlwidget-53e4ae83f58976e88665" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-53e4ae83f58976e88665">{"x":{"filter":"none","caption":"<caption>变量说明<\/caption>","data":[["wage","educ","exper","fatheduc","motheduc","inlf","hours","kidslt6","kidsge6","age","repwage","hushrs","husage","huseduc","huswage","faminc","mtr","unem","city","nwifeinc","lwage","expersq"],[["时均工资"],["受教育年数"],["就业次数"],["父亲的受教育年数"],["母亲的受教育年数"],["是否是劳动力"],["工作时长"],["六岁以下孩子数"],["6-18岁孩子数"],["年龄"],["回访年的时均工资"],["丈夫工作时长"],["丈夫年龄"],["丈夫的受教育年数"],["丈夫的时均工资"],["家庭收入"],["征收税率"],["所在地区失业率"],["居住地是否为SMSA"],["扣除妇女工资的家庭收入/1000"],["工资的对数"],["就业次数的平方"]]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>vars<\/th>\n      <th>mark<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":6,"dom":"tip","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[6,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>

---

## 原始数据


<div id="htmlwidget-c6c6ec535000dcfa9b77" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-c6c6ec535000dcfa9b77">{"x":{"filter":"none","caption":"<caption>已婚女性的教育回报数据n=(428)<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428"],[3.35400009155273,1.3889000415802,4.54549980163574,1.09650003910065,4.59180021286011,4.74209976196289,8.33329963684082,7.84310007095337,2.12619996070862,4.6875,4.06300020217896,4.59180021286011,2.08330011367798,2.26679992675781,3.67969989776611,1.34720003604889,3.21429991722107,5.17500019073486,2,7.5528998374939,3.50519990921021,3.57139992713928,3.25,3.25,2.15450000762939,3.7878999710083,4,4.72690010070801,7.25589990615845,5.86709976196289,1.53849995136261,2.45900011062622,5.85109996795654,3.57139992713928,3.80679988861084,2.46379995346069,2.37529993057251,4.53509998321533,5.61829996109009,14.6309995651245,2.67860007286072,3.91939997673035,2.57290005683899,4.53749990463257,2,3.4721999168396,2.01609992980957,4.57159996032715,2.27270007133484,2.63750004768372,2.28990006446838,1.09889996051788,1.1764999628067,1.60000002384186,1.87619996070862,4.04370021820068,9.63539981842041,8.04090023040771,4.59899997711182,2.14289999008179,4.40000009536743,3.53539991378784,2.71740007400513,6.25,11.9329996109009,3.59310007095337,6.9443998336792,2.91669988632202,3.07690000534058,3.75,5.72590017318726,3.67569994926453,5.16480016708374,8.22369956970215,4.33650016784668,4.98190021514893,0.357100009918213,2.96740007400513,1,2.55539989471436,0.860199987888336,1,2.92610001564026,3.54609990119934,1.62639999389648,8.33329963684082,3.0952000617981,2.70000004768372,5.25209999084473,1.41540002822876,4.79860019683838,1.66670000553131,1.12170004844666,0.5,0.714299976825714,2.79609990119934,4.85830020904541,1.74349999427795,2.46309995651245,2.42129993438721,1.53450000286102,2.88179993629456,2.40689992904663,5.23260021209717,3.75,1.3889000415802,4,3.23130011558533,3.40140008926392,1.33329999446869,9.30230045318604,4.5,4.62349987030029,3.9556999206543,5.81400012969971,0.5,4.08160018920898,6,3.66669988632202,3.86129999160767,2.76290011405945,2.93099999427795,4.38840007781982,5.41669988632202,9.86110019683838,0.16159999370575,0.382600009441376,3.63639998435974,2.37470006942749,4.66669988632202,1.85189998149872,5.19999980926514,3.2985999584198,8.53330039978027,2.06349992752075,2.56410002708435,2.1875,6.25,3.33330011367798,4.4443998336792,6.63350009918213,8.42240047454834,4.39559984207153,2.44569993019104,1.22449994087219,1.625,3,4.72690010070801,1.12929999828339,7.40240001678467,4.45949983596802,2.47250008583069,1.88240003585815,4,8.1899995803833,7.09679985046387,1.66670000553131,3.4449999332428,4.23470020294189,2.7778000831604,1.88919997215271,5.03520011901855,1.25,2.85710000991821,4.11670017242432,1.77779996395111,13.5539999008179,4.56269979476929,2.12770009040833,2.98909997940063,2.56539988517761,5.6121997833252,2.80539989471436,1.60699999332428,2.25,2.03250002861023,5.53200006484985,1.58449995517731,3.7878999710083,3,8.65380001068115,4.21049976348877,4.6875,4.0984001159668,25,2.6331000328064,6,5.41260004043579,0.664200007915497,1.25,2.27539992332458,3.4614999294281,4.16669988632202,4.46869993209839,1.75,3.66939997673035,6.58260011672974,2.59999990463257,4.86509990692139,5.78700017929077,4.54080009460449,9.50570011138916,10.6379995346069,1.1110999584198,4.05410003662109,2.68709993362427,2.97620010375977,3.17280006408691,3.54999995231628,17.9069995880127,3.41739988327026,3.33330011367798,3.88599991798401,2.31110000610352,1.71080005168915,2.1143000125885,9.9330997467041,3.02769994735718,1.86049997806549,0.128199994564056,6.63269996643066,5.63910007476807,1.59899997711182,2.66669988632202,7.92080020904541,5.33979988098145,4,6.04489994049072,6.25,2.97620010375977,4.23390007019043,3.49160003662109,4.96449995040894,2.76920008659363,3.65849995613098,5.39349985122681,0.6564000248909,4.76879978179932,8.555100440979,10.4589996337891,2.63750004768372,6.84929990768433,5.08720016479492,0.961499989032745,4.30660009384155,7.06669998168945,2.52530002593994,7.894700050354,4.14890003204346,8.17459964752197,9.59710025787354,2.01640009880066,7.6217999458313,3.19749999046326,1.60000002384186,4.0984001159668,1.4815000295639,3.63639998435974,1,2.60419988632202,1.75,4.80000019073486,5.53000020980835,4.0984001159668,1.25,1.66670000553131,3.79010009765625,2.36360001564026,10.2040004730225,6.81820011138916,7.21460008621216,2.44840002059937,1.19869995117188,1.64100003242493,1.7820999622345,2.94120001792908,4.96890020370483,1.86049997806549,8.03569984436035,3.97160005569458,3.04159998893738,2.90700006484985,3.06119990348816,4.89659976959229,4.01609992980957,5.5556001663208,1.22269999980927,2.68390011787415,2.67379999160767,9.32940006256104,3.13809990882874,0.542599976062775,8.61110019683838,3.6686999797821,2.31850004196167,2.88199996948242,3.16659998893738,3.64580011367798,6.25,10.2489995956421,3.20959997177124,7.65220022201538,1.97239995002747,4.69799995422363,2.12339997291565,2.33330011367798,2.38960003852844,1.25639998912811,1.09379994869232,3.75,3.30719995498657,5.13520002365112,6.63269996643066,4.5644998550415,11.8479995727539,3.75,4.36509990692139,3.93330001831055,3.33330011367798,3.31590008735657,3.56159996986389,1.60000002384186,2.22550010681152,4.78719997406006,5.80649995803833,2.3585000038147,2,1.89999997615814,5.1230001449585,5.49450016021729,6.32180023193359,7.14289999008179,2.375,2.54290008544922,2.17980003356934,2.59999990463257,3.72939991950989,4.375,4.44329977035522,4.28770017623901,1.66670000553131,3.2558000087738,5.41130018234253,2.20499992370605,4.0625,0.64819997549057,5.38259983062744,0.170900002121925,23.4669990539551,9.57849979400635,3.69479990005493,2.2221999168396,1.74899995326996,1.15939998626709,6.99009990692139,3.96830010414124,21.4290008544922,0.476200014352798,2.15050005912781,1.85780000686646,4.32950019836426,8.9286003112793,2.7778000831604,2.65650010108948,2.5,18.2670001983643,0.819000005722046,2,15.3850002288818,6.47749996185303,8.33329963684082,4.55030012130737,2.49580001831055,4.47970008850098,2.23239994049072,2.07100009918213,1.67599999904633,3.40910005569458,2.5,3.96090006828308,6.22749996185303,3.92860007286072,2.90000009536743,4.08160018920898,2.84999990463257,7.01529979705811,2.93880009651184,1.92309999465942,6.875,3.90019989013672,2,4.97629976272583,1.20190000534058,22.5,6.86770009994507,3.56130003929138,1.97920000553131,5.3713002204895,1.7441999912262,5.09479999542236,2.5,3.82500004768372,1,3.07170009613037,1.71630001068115,4.02089977264404,5.45849990844727,25,2.38969993591309,3.21339988708496,3.37700009346008,1.77779996395111,3.16330003738403,2.70269989967346,1.69270002841949,0.213699996471405,6.76690006256104,1.74070000648499,2.5,4.48589992523193,2.57200002670288,3.46000003814697,4.78259992599487,2.31180000305176,5.3060998916626,5.86749982833862,3.40910005569458,4.08160018920898],[12,12,12,12,14,12,16,12,12,12,12,11,12,12,10,11,12,12,12,12,16,12,13,12,12,17,12,12,17,12,11,16,13,12,16,11,12,10,14,17,12,12,16,12,12,12,16,12,12,12,12,12,12,8,10,16,14,17,14,12,14,12,8,12,12,8,17,12,12,12,12,12,9,10,12,12,12,17,15,12,6,14,12,14,9,17,13,9,15,12,12,12,12,12,12,12,12,13,12,13,12,12,12,16,12,13,11,12,12,12,17,14,16,17,12,11,12,12,17,10,13,11,12,16,17,12,16,12,16,8,12,12,12,13,11,12,12,14,12,12,12,17,14,12,9,12,12,12,14,16,17,15,12,16,17,17,12,16,13,12,11,16,14,16,12,9,17,14,12,12,11,12,12,10,12,5,17,11,12,12,14,11,12,14,12,10,16,13,12,12,12,11,12,9,13,12,12,12,13,16,12,16,17,12,12,9,12,12,13,12,12,12,12,10,12,16,12,11,12,10,12,12,12,12,16,17,12,17,12,12,12,8,12,13,12,12,8,12,17,17,12,13,12,12,12,12,9,10,12,16,13,8,16,13,12,11,13,12,12,10,12,17,15,16,10,11,12,12,14,16,14,8,7,12,12,14,12,12,12,14,16,12,12,12,13,13,10,12,12,12,12,14,17,10,9,12,12,16,12,17,12,17,11,16,11,13,11,8,11,12,10,17,12,12,17,14,12,12,12,12,12,12,9,10,12,12,12,12,12,17,12,17,12,10,12,12,12,12,12,12,16,13,13,12,16,17,12,14,12,17,12,14,12,12,17,16,16,12,9,12,12,16,14,12,12,11,12,16,17,17,14,12,14,12,10,12,13,16,12,7,16,14,12,10,12,16,10,12,14,12,6,15,12,17,14,13,6,16,14,15,14,8,14,12,12,12,12,12,12,8,12,17,12,12,14,13,17,8,12,11,12,12,17,10,12,13,12,12],[7,7,7,7,14,7,7,3,7,7,3,7,16,10,7,10,7,12,7,7,16,10,3,7,7,14,7,7,12,12,7,3,10,14,12,3,3,3,7,17,12,9,16,3,7,7,16,10,7,7,7,3,7,7,3,12,7,17,7,7,3,12,7,7,7,12,16,7,7,7,12,10,9,0,10,14,7,3,12,12,7,17,3,7,7,12,7,7,12,10,0,12,10,7,7,7,3,12,7,12,7,7,10,14,7,12,7,7,10,7,12,7,7,17,7,7,7,10,10,12,7,12,7,10,7,10,7,7,7,7,7,7,16,12,7,3,7,7,7,12,7,12,12,14,7,7,7,12,12,14,10,12,7,16,7,17,3,10,9,7,3,16,12,7,7,7,12,3,7,7,7,7,10,10,7,12,17,10,7,7,12,7,12,7,7,7,7,14,7,12,7,7,12,3,7,12,12,7,7,14,12,17,17,7,7,10,7,7,7,3,0,7,12,7,7,12,7,7,12,7,7,3,7,7,10,12,7,12,7,7,7,7,12,7,7,7,7,7,14,17,7,10,7,7,12,7,7,9,7,14,7,3,16,3,16,7,16,12,7,7,12,12,12,16,7,9,7,12,12,10,7,12,7,7,3,10,17,7,3,3,12,7,7,7,10,7,10,7,9,9,12,12,12,7,9,12,7,12,7,12,12,14,7,14,10,12,7,7,12,7,7,12,12,12,12,14,10,7,7,7,7,7,7,7,7,7,12,12,7,14,10,12,7,7,12,7,10,12,10,7,14,7,12,12,7,16,0,12,7,17,7,12,3,7,14,7,12,12,7,7,14,7,12,12,7,7,7,7,3,12,7,7,14,10,10,10,10,12,3,7,16,7,7,0,7,7,10,7,12,7,7,7,7,16,12,10,7,7,16,7,7,7,12,10,10,10,7,10,12,12,7,17,12,16,10,16,7,7,9,3,7,7,7,7,7,7,16,12]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>wage<\/th>\n      <th>educ<\/th>\n      <th>fatheduc<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":8,"dom":"tip","scrollX":true,"scrollCollapse":true,"columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[8,10,25,50,100],"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>

---

## 散点图1

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="11-instrument-variable-slide_files/figure-html/unnamed-chunk-4-1.png" alt="受教育年数与时均工资的散点图"  /&gt;
&lt;p class="caption"&gt;受教育年数与时均工资的散点图&lt;/p&gt;
&lt;/div&gt;

---

## 散点图2

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="11-instrument-variable-slide_files/figure-html/unnamed-chunk-5-1.png" alt="考虑父亲受教育年数的散点图"  /&gt;
&lt;p class="caption"&gt;考虑父亲受教育年数的散点图&lt;/p&gt;
&lt;/div&gt;

---

## OLS回归




如果直接构建如下的**“偏误模型”**，并坚持采用OLS估计：

`$$\begin{equation} \begin{alignedat}{999} &amp;log(wage)=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} educ&amp;&amp;+u_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(wage)}=&amp;&amp;-0.19&amp;&amp;+0.11educ\\ &amp;\text{(t)}&amp;&amp;(-0.9998)&amp;&amp;(7.5451)\\&amp;\text{(se)}&amp;&amp;(0.1852)&amp;&amp;(0.0144)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.1179;&amp;&amp; \bar{R^2}=0.1158\\&amp; &amp;&amp; F^{\ast}=56.93;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---

## 工具变量法回归（IV）:手工分步计算

采用工具变量法的第一阶段回归：

`$$\begin{equation} \begin{alignedat}{999} &amp;educ=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} fatheduc&amp;&amp;+u_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{educ}=&amp;&amp;+10.24&amp;&amp;+0.27fatheduc\\ &amp;\text{(t)}&amp;&amp;(37.0993)&amp;&amp;(9.4255)\\&amp;\text{(se)}&amp;&amp;(0.2759)&amp;&amp;(0.0286)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.1726;&amp;&amp; \bar{R^2}=0.1706\\&amp; &amp;&amp; F^{\ast}=88.84;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

采用工具变量法的第二阶段回归：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(wage)}=&amp;&amp;+0.44&amp;&amp;+0.06educ.hat\\ &amp;\text{(t)}&amp;&amp;(0.9443)&amp;&amp;(1.6081)\\&amp;\text{(se)}&amp;&amp;(0.4671)&amp;&amp;(0.0368)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.0060;&amp;&amp; \bar{R^2}=0.0037\\&amp; &amp;&amp; F^{\ast}=2.59;&amp;&amp; p=0.1086 \end{alignedat} \end{equation}$$`

---

## 工具变量法回归（IV）：R软件自动计算

采用R包`AER`的工具变量回归函数`ivreg()`，可以得到如下回归结果：

.pull-left[


```r
require("AER")
lm.tsls &lt;- ivreg(log(wage)~educ|fatheduc,data=mroz)
summary(lm.tsls)
```

```

Call:
ivreg(formula = log(wage) ~ educ | fatheduc, data = mroz)

Residuals:
    Min      1Q  Median      3Q     Max 
-3.0870 -0.3393  0.0525  0.4042  2.0677 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)  
(Intercept)   0.4411     0.4461    0.99    0.323  
educ          0.0592     0.0351    1.68    0.093 .
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.689 on 426 degrees of freedom
Multiple R-Squared: 0.0934,	Adjusted R-squared: 0.0913 
Wald test: 2.84 on 1 and 426 DF,  p-value: 0.0929 
```

]

.pull-right[

**工具变量回归模型**：

`$$\begin{align}
log(wage) = \lambda_1 + \lambda_2educ|fatheduc + \epsilon_i
\end{align}$$`

**提问**：

- 手工分步计算与软件自动计算有哪些不同？


- 判定系数和系数标准误差为什么会不同？

]

---

## 工具变量法回归（IV）：EViews软件自动计算

EViews软件下工具变量法的实现：

&lt;img src="pic/chpt11-eq-iv-estimation.png" width="470" style="display: block; margin: auto;" /&gt;

---

## 工具变量法回归（IV）：EViews软件自动计算

EViews软件下工具变量法的结果：

&lt;img src="pic/chpt11-eq-iv-eviews.png" width="590" style="display: block; margin: auto;" /&gt;

---
class: middle, center, inverse

# 多元回归模型的IV估计

---
## 新的符号表达体系

对于多元回归模型，我们可以记为：

`$$\begin {align} 
\log (wage) &amp; = \beta_{1}+\beta_{2} educ+\lambda_{1} exper+u_{i} \\
Y_{1i} &amp; =\beta_{1}+\beta_{2} Y_{2i}+\lambda_{1} Z_{1i}+u_{i} \\
E(u_i)&amp;=0;  \quad cov(Y_{2i}, u_i) \neq 0 ;\\
cov(Z_{1i},u_i)&amp;=0;\quad cov(Y_{2i},Z_{1i})=0
\end {align}$$`

- **内生变量**（endogenous variable）：用符号
`\(Y_i\)`表达。例如，因变量（工资水平）
`\(Y_{1i}\)`显然是内生变量；而其中的一个自变量（教育年数）
`\(Y_{2i}\)`我们在这里认为也是内生的——也即允许它跟随机干扰项
`\(u_i\)`**相关**。

- **外生变量**（exogenous variable）：用符号
`\(Z_i\)`表达。例如另一个自变量（工作经历）
`\(Z_{1i}\)`则认为它是外生的——也即它跟随机干扰项
`\(u_i\)`**不相关**。


---
## 约简方程

`$$\begin {align} 
\log (wage) &amp; = \beta_{1}+\beta_{2} educ+\lambda_{1} exper+u_{i} \\
Y_{1i} &amp; =\beta_{1}+\beta_{2} Y_{2i}+\lambda_{1} Z_{1i}+u_{i} \\
\end {align}$$`

如果我们给内生变量
`\(Y_{2i}\)`找到一个理想的工具变量
`\(Z_{2i}\)`，则可以构建如下的**约简方程**：

`$$\begin {align} 
Y_{2i} &amp; =\pi_{0}+\pi_{1} Z_{1i}+\pi_{2} Z_{2i}+v_{i} \\
E(v_i)&amp;=0; \quad cov(Y_{2i},Z_{1i})=0;\\
cov(Z_{1i},v_i) &amp;=0; \quad cov(Z_{2i}, v_i) = 0
\end {align}$$`

- **约简方程**（reductive equation）：是指一个**内生变量**对全部**外生变量**的回归方程。

- 因为
`\(Z_{1i};Z_{2i}\)`为外生变量都为**外生变量**，所以满足CLRM假设，上述**偏相关分析模型**可以直接使用OLS方法而得到**BLUE**。**偏相关分析模型**可以用于检验
`\(Y_{2i}\)`与
`\(Z_{2i}\)`是否相关，也即检验：
`\(H_0:\pi_2=0; \quad H_1:\pi_2 \neq 0\)`。具体可以采用通常的**t检验**方法。


**提问1**：我们能不能检验
`\(Z_{2i}\)`与
`\(u_i\)`相关？能不能检验
`\(Z_{1i}\)`与
`\(u_i\)`相关？

**提问2**：还能不能构造别的**简约方程**？

---
## 两阶段最小二乘法（2SLS）

`$$\begin {align} 
\log (wage) &amp; = \beta_{1}+\beta_{2} educ+\lambda_{1} exper+u_{i} \\
Y_{1i} &amp; =\beta_{1}+\beta_{2} Y_{2i}+\lambda_{1} Z_{1i}+u_{i} \\
\end {align}$$`

第一阶段OLS回归：对约简方程进行回归。

`$$\begin {align} 
Y_{2i} &amp; =\pi_{0}+\pi_{1} Z_{1i}+\pi_{2} Z_{2i}+v_{i}  
&amp;&amp; \leftarrow \left[ \pi_2 \neq 0 \right]\\
\hat{Y}_{2i} &amp; =\pi_{0}+\pi_{1} Z_{1i}+\pi_{2} Z_{2i}
\end {align}$$`

第二阶段OLS回归：基于估计值进行回归。

`$$\begin {align} 
Y_{1i} &amp; =\alpha_{0}+\alpha_{1} Z_{1i}+\beta_{2} \hat{Y}_{2i}+\epsilon_{i}
\end {align}$$`

---
class: middle, center, duke-orange

# 已婚女性的教育回报案例

---

## 原始数据


<div id="htmlwidget-ee396f37047a96eff3d4" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ee396f37047a96eff3d4">{"x":{"filter":"none","caption":"<caption>已婚女性的教育回报数据n=(428)<\/caption>","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54","55","56","57","58","59","60","61","62","63","64","65","66","67","68","69","70","71","72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87","88","89","90","91","92","93","94","95","96","97","98","99","100","101","102","103","104","105","106","107","108","109","110","111","112","113","114","115","116","117","118","119","120","121","122","123","124","125","126","127","128","129","130","131","132","133","134","135","136","137","138","139","140","141","142","143","144","145","146","147","148","149","150","151","152","153","154","155","156","157","158","159","160","161","162","163","164","165","166","167","168","169","170","171","172","173","174","175","176","177","178","179","180","181","182","183","184","185","186","187","188","189","190","191","192","193","194","195","196","197","198","199","200","201","202","203","204","205","206","207","208","209","210","211","212","213","214","215","216","217","218","219","220","221","222","223","224","225","226","227","228","229","230","231","232","233","234","235","236","237","238","239","240","241","242","243","244","245","246","247","248","249","250","251","252","253","254","255","256","257","258","259","260","261","262","263","264","265","266","267","268","269","270","271","272","273","274","275","276","277","278","279","280","281","282","283","284","285","286","287","288","289","290","291","292","293","294","295","296","297","298","299","300","301","302","303","304","305","306","307","308","309","310","311","312","313","314","315","316","317","318","319","320","321","322","323","324","325","326","327","328","329","330","331","332","333","334","335","336","337","338","339","340","341","342","343","344","345","346","347","348","349","350","351","352","353","354","355","356","357","358","359","360","361","362","363","364","365","366","367","368","369","370","371","372","373","374","375","376","377","378","379","380","381","382","383","384","385","386","387","388","389","390","391","392","393","394","395","396","397","398","399","400","401","402","403","404","405","406","407","408","409","410","411","412","413","414","415","416","417","418","419","420","421","422","423","424","425","426","427","428"],[3.35400009155273,1.3889000415802,4.54549980163574,1.09650003910065,4.59180021286011,4.74209976196289,8.33329963684082,7.84310007095337,2.12619996070862,4.6875,4.06300020217896,4.59180021286011,2.08330011367798,2.26679992675781,3.67969989776611,1.34720003604889,3.21429991722107,5.17500019073486,2,7.5528998374939,3.50519990921021,3.57139992713928,3.25,3.25,2.15450000762939,3.7878999710083,4,4.72690010070801,7.25589990615845,5.86709976196289,1.53849995136261,2.45900011062622,5.85109996795654,3.57139992713928,3.80679988861084,2.46379995346069,2.37529993057251,4.53509998321533,5.61829996109009,14.6309995651245,2.67860007286072,3.91939997673035,2.57290005683899,4.53749990463257,2,3.4721999168396,2.01609992980957,4.57159996032715,2.27270007133484,2.63750004768372,2.28990006446838,1.09889996051788,1.1764999628067,1.60000002384186,1.87619996070862,4.04370021820068,9.63539981842041,8.04090023040771,4.59899997711182,2.14289999008179,4.40000009536743,3.53539991378784,2.71740007400513,6.25,11.9329996109009,3.59310007095337,6.9443998336792,2.91669988632202,3.07690000534058,3.75,5.72590017318726,3.67569994926453,5.16480016708374,8.22369956970215,4.33650016784668,4.98190021514893,0.357100009918213,2.96740007400513,1,2.55539989471436,0.860199987888336,1,2.92610001564026,3.54609990119934,1.62639999389648,8.33329963684082,3.0952000617981,2.70000004768372,5.25209999084473,1.41540002822876,4.79860019683838,1.66670000553131,1.12170004844666,0.5,0.714299976825714,2.79609990119934,4.85830020904541,1.74349999427795,2.46309995651245,2.42129993438721,1.53450000286102,2.88179993629456,2.40689992904663,5.23260021209717,3.75,1.3889000415802,4,3.23130011558533,3.40140008926392,1.33329999446869,9.30230045318604,4.5,4.62349987030029,3.9556999206543,5.81400012969971,0.5,4.08160018920898,6,3.66669988632202,3.86129999160767,2.76290011405945,2.93099999427795,4.38840007781982,5.41669988632202,9.86110019683838,0.16159999370575,0.382600009441376,3.63639998435974,2.37470006942749,4.66669988632202,1.85189998149872,5.19999980926514,3.2985999584198,8.53330039978027,2.06349992752075,2.56410002708435,2.1875,6.25,3.33330011367798,4.4443998336792,6.63350009918213,8.42240047454834,4.39559984207153,2.44569993019104,1.22449994087219,1.625,3,4.72690010070801,1.12929999828339,7.40240001678467,4.45949983596802,2.47250008583069,1.88240003585815,4,8.1899995803833,7.09679985046387,1.66670000553131,3.4449999332428,4.23470020294189,2.7778000831604,1.88919997215271,5.03520011901855,1.25,2.85710000991821,4.11670017242432,1.77779996395111,13.5539999008179,4.56269979476929,2.12770009040833,2.98909997940063,2.56539988517761,5.6121997833252,2.80539989471436,1.60699999332428,2.25,2.03250002861023,5.53200006484985,1.58449995517731,3.7878999710083,3,8.65380001068115,4.21049976348877,4.6875,4.0984001159668,25,2.6331000328064,6,5.41260004043579,0.664200007915497,1.25,2.27539992332458,3.4614999294281,4.16669988632202,4.46869993209839,1.75,3.66939997673035,6.58260011672974,2.59999990463257,4.86509990692139,5.78700017929077,4.54080009460449,9.50570011138916,10.6379995346069,1.1110999584198,4.05410003662109,2.68709993362427,2.97620010375977,3.17280006408691,3.54999995231628,17.9069995880127,3.41739988327026,3.33330011367798,3.88599991798401,2.31110000610352,1.71080005168915,2.1143000125885,9.9330997467041,3.02769994735718,1.86049997806549,0.128199994564056,6.63269996643066,5.63910007476807,1.59899997711182,2.66669988632202,7.92080020904541,5.33979988098145,4,6.04489994049072,6.25,2.97620010375977,4.23390007019043,3.49160003662109,4.96449995040894,2.76920008659363,3.65849995613098,5.39349985122681,0.6564000248909,4.76879978179932,8.555100440979,10.4589996337891,2.63750004768372,6.84929990768433,5.08720016479492,0.961499989032745,4.30660009384155,7.06669998168945,2.52530002593994,7.894700050354,4.14890003204346,8.17459964752197,9.59710025787354,2.01640009880066,7.6217999458313,3.19749999046326,1.60000002384186,4.0984001159668,1.4815000295639,3.63639998435974,1,2.60419988632202,1.75,4.80000019073486,5.53000020980835,4.0984001159668,1.25,1.66670000553131,3.79010009765625,2.36360001564026,10.2040004730225,6.81820011138916,7.21460008621216,2.44840002059937,1.19869995117188,1.64100003242493,1.7820999622345,2.94120001792908,4.96890020370483,1.86049997806549,8.03569984436035,3.97160005569458,3.04159998893738,2.90700006484985,3.06119990348816,4.89659976959229,4.01609992980957,5.5556001663208,1.22269999980927,2.68390011787415,2.67379999160767,9.32940006256104,3.13809990882874,0.542599976062775,8.61110019683838,3.6686999797821,2.31850004196167,2.88199996948242,3.16659998893738,3.64580011367798,6.25,10.2489995956421,3.20959997177124,7.65220022201538,1.97239995002747,4.69799995422363,2.12339997291565,2.33330011367798,2.38960003852844,1.25639998912811,1.09379994869232,3.75,3.30719995498657,5.13520002365112,6.63269996643066,4.5644998550415,11.8479995727539,3.75,4.36509990692139,3.93330001831055,3.33330011367798,3.31590008735657,3.56159996986389,1.60000002384186,2.22550010681152,4.78719997406006,5.80649995803833,2.3585000038147,2,1.89999997615814,5.1230001449585,5.49450016021729,6.32180023193359,7.14289999008179,2.375,2.54290008544922,2.17980003356934,2.59999990463257,3.72939991950989,4.375,4.44329977035522,4.28770017623901,1.66670000553131,3.2558000087738,5.41130018234253,2.20499992370605,4.0625,0.64819997549057,5.38259983062744,0.170900002121925,23.4669990539551,9.57849979400635,3.69479990005493,2.2221999168396,1.74899995326996,1.15939998626709,6.99009990692139,3.96830010414124,21.4290008544922,0.476200014352798,2.15050005912781,1.85780000686646,4.32950019836426,8.9286003112793,2.7778000831604,2.65650010108948,2.5,18.2670001983643,0.819000005722046,2,15.3850002288818,6.47749996185303,8.33329963684082,4.55030012130737,2.49580001831055,4.47970008850098,2.23239994049072,2.07100009918213,1.67599999904633,3.40910005569458,2.5,3.96090006828308,6.22749996185303,3.92860007286072,2.90000009536743,4.08160018920898,2.84999990463257,7.01529979705811,2.93880009651184,1.92309999465942,6.875,3.90019989013672,2,4.97629976272583,1.20190000534058,22.5,6.86770009994507,3.56130003929138,1.97920000553131,5.3713002204895,1.7441999912262,5.09479999542236,2.5,3.82500004768372,1,3.07170009613037,1.71630001068115,4.02089977264404,5.45849990844727,25,2.38969993591309,3.21339988708496,3.37700009346008,1.77779996395111,3.16330003738403,2.70269989967346,1.69270002841949,0.213699996471405,6.76690006256104,1.74070000648499,2.5,4.48589992523193,2.57200002670288,3.46000003814697,4.78259992599487,2.31180000305176,5.3060998916626,5.86749982833862,3.40910005569458,4.08160018920898],[12,12,12,12,14,12,16,12,12,12,12,11,12,12,10,11,12,12,12,12,16,12,13,12,12,17,12,12,17,12,11,16,13,12,16,11,12,10,14,17,12,12,16,12,12,12,16,12,12,12,12,12,12,8,10,16,14,17,14,12,14,12,8,12,12,8,17,12,12,12,12,12,9,10,12,12,12,17,15,12,6,14,12,14,9,17,13,9,15,12,12,12,12,12,12,12,12,13,12,13,12,12,12,16,12,13,11,12,12,12,17,14,16,17,12,11,12,12,17,10,13,11,12,16,17,12,16,12,16,8,12,12,12,13,11,12,12,14,12,12,12,17,14,12,9,12,12,12,14,16,17,15,12,16,17,17,12,16,13,12,11,16,14,16,12,9,17,14,12,12,11,12,12,10,12,5,17,11,12,12,14,11,12,14,12,10,16,13,12,12,12,11,12,9,13,12,12,12,13,16,12,16,17,12,12,9,12,12,13,12,12,12,12,10,12,16,12,11,12,10,12,12,12,12,16,17,12,17,12,12,12,8,12,13,12,12,8,12,17,17,12,13,12,12,12,12,9,10,12,16,13,8,16,13,12,11,13,12,12,10,12,17,15,16,10,11,12,12,14,16,14,8,7,12,12,14,12,12,12,14,16,12,12,12,13,13,10,12,12,12,12,14,17,10,9,12,12,16,12,17,12,17,11,16,11,13,11,8,11,12,10,17,12,12,17,14,12,12,12,12,12,12,9,10,12,12,12,12,12,17,12,17,12,10,12,12,12,12,12,12,16,13,13,12,16,17,12,14,12,17,12,14,12,12,17,16,16,12,9,12,12,16,14,12,12,11,12,16,17,17,14,12,14,12,10,12,13,16,12,7,16,14,12,10,12,16,10,12,14,12,6,15,12,17,14,13,6,16,14,15,14,8,14,12,12,12,12,12,12,8,12,17,12,12,14,13,17,8,12,11,12,12,17,10,12,13,12,12],[14,5,15,6,7,33,11,35,24,21,15,14,0,14,6,9,20,6,23,9,5,11,18,15,4,21,31,9,7,7,32,11,16,14,27,0,17,28,24,11,1,14,6,10,6,4,10,22,16,6,12,32,15,17,34,9,37,10,35,6,19,10,11,15,12,12,14,11,9,24,12,13,29,11,13,19,2,24,9,6,22,30,10,6,29,29,36,19,8,13,16,11,15,6,13,22,24,2,6,2,2,14,9,11,9,6,19,26,19,3,7,28,13,9,15,20,29,9,1,8,19,23,3,13,8,17,4,15,11,7,0,0,10,8,2,4,6,18,3,22,33,28,23,27,11,6,11,14,17,17,14,11,7,8,6,8,4,25,24,11,19,9,19,14,22,6,23,15,6,11,2,22,10,14,12,9,13,18,8,11,9,9,14,9,2,12,15,11,7,9,19,11,8,13,4,7,19,14,14,3,9,7,7,14,29,19,14,16,10,12,24,6,9,14,26,7,4,15,23,1,29,9,6,11,17,6,7,2,24,4,11,25,11,2,19,7,2,20,10,19,17,12,11,6,10,4,2,13,21,9,4,2,19,4,9,14,6,24,1,13,3,10,16,9,19,4,10,5,7,3,38,16,13,1,7,15,10,2,19,25,25,7,15,11,25,19,4,14,19,18,14,11,4,29,21,24,19,31,28,15,27,13,4,10,8,4,18,3,11,8,10,33,19,35,21,7,18,4,12,16,14,3,1,27,12,6,9,2,6,9,16,22,26,11,11,15,13,6,20,17,8,13,15,14,14,6,24,10,2,9,23,12,8,16,10,7,19,2,9,14,9,16,7,6,22,9,9,14,17,12,13,8,10,16,1,6,4,8,4,15,7,14,16,15,23,19,4,12,12,25,14,14,11,7,18,4,37,13,14,17,5,2,0,3,21,20,19,4,19,11,14,8,13,24,1,1,3,4,21,10,13,9,14,2,21,22,14,7],[7,7,7,7,14,7,7,3,7,7,3,7,16,10,7,10,7,12,7,7,16,10,3,7,7,14,7,7,12,12,7,3,10,14,12,3,3,3,7,17,12,9,16,3,7,7,16,10,7,7,7,3,7,7,3,12,7,17,7,7,3,12,7,7,7,12,16,7,7,7,12,10,9,0,10,14,7,3,12,12,7,17,3,7,7,12,7,7,12,10,0,12,10,7,7,7,3,12,7,12,7,7,10,14,7,12,7,7,10,7,12,7,7,17,7,7,7,10,10,12,7,12,7,10,7,10,7,7,7,7,7,7,16,12,7,3,7,7,7,12,7,12,12,14,7,7,7,12,12,14,10,12,7,16,7,17,3,10,9,7,3,16,12,7,7,7,12,3,7,7,7,7,10,10,7,12,17,10,7,7,12,7,12,7,7,7,7,14,7,12,7,7,12,3,7,12,12,7,7,14,12,17,17,7,7,10,7,7,7,3,0,7,12,7,7,12,7,7,12,7,7,3,7,7,10,12,7,12,7,7,7,7,12,7,7,7,7,7,14,17,7,10,7,7,12,7,7,9,7,14,7,3,16,3,16,7,16,12,7,7,12,12,12,16,7,9,7,12,12,10,7,12,7,7,3,10,17,7,3,3,12,7,7,7,10,7,10,7,9,9,12,12,12,7,9,12,7,12,7,12,12,14,7,14,10,12,7,7,12,7,7,12,12,12,12,14,10,7,7,7,7,7,7,7,7,7,12,12,7,14,10,12,7,7,12,7,10,12,10,7,14,7,12,12,7,16,0,12,7,17,7,12,3,7,14,7,12,12,7,7,14,7,12,12,7,7,7,7,3,12,7,7,14,10,10,10,10,12,3,7,16,7,7,0,7,7,10,7,12,7,7,7,7,16,12,10,7,7,16,7,7,7,12,10,10,10,7,10,12,12,7,17,12,16,10,16,7,7,9,3,7,7,7,7,7,7,16,12],[12,7,12,7,12,14,14,3,7,7,12,14,16,10,7,16,10,12,7,12,10,12,7,7,12,16,3,3,12,12,7,3,12,7,12,10,3,10,7,14,12,9,14,3,12,12,14,10,7,12,7,7,12,7,7,12,7,17,17,12,14,12,7,7,7,12,12,12,7,12,12,10,7,0,7,12,7,3,10,7,12,12,7,7,7,7,7,7,7,10,7,12,10,12,7,7,7,14,7,12,12,7,7,14,12,10,7,7,7,7,12,7,12,10,10,7,7,7,12,7,7,12,14,12,7,10,7,7,12,10,7,7,12,10,7,12,7,7,7,7,3,12,16,7,3,12,7,12,12,16,12,12,7,14,7,10,7,14,7,7,12,12,17,7,7,3,12,7,7,7,3,7,10,10,12,7,14,10,7,7,10,12,12,7,7,7,12,7,12,12,12,10,12,10,12,12,12,7,12,12,12,12,16,7,16,7,7,10,12,10,0,7,12,12,10,12,3,7,12,10,7,7,7,7,12,12,7,12,7,10,10,7,12,17,7,7,7,7,12,14,7,12,7,7,16,7,10,12,7,16,10,3,16,7,12,7,7,7,12,12,7,10,14,16,7,10,7,14,14,12,7,7,3,7,7,7,12,10,7,3,12,7,12,7,10,7,0,7,10,9,12,12,12,3,9,12,12,14,7,12,12,12,7,12,10,12,7,7,3,12,7,16,12,12,7,14,7,7,12,10,7,3,7,7,10,7,7,12,12,12,10,14,7,7,14,10,10,7,7,7,14,7,12,14,14,14,0,16,7,12,7,10,7,10,10,14,7,12,7,7,12,14,12,7,7,7,12,16,3,16,7,16,7,10,10,10,10,12,10,7,16,7,7,7,7,7,10,10,12,7,7,7,7,14,14,7,7,7,16,12,7,7,12,12,12,10,3,12,12,12,7,16,12,10,10,12,7,7,7,7,7,7,7,7,7,7,12,12]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>wage<\/th>\n      <th>educ<\/th>\n      <th>exper<\/th>\n      <th>fatheduc<\/th>\n      <th>motheduc<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":8,"dom":"tip","scrollX":true,"scrollCollapse":true,"columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[8,10,25,50,100],"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>

---

## 偏误OLS回归：精炼报告




如果认为工作经历
`\(exper\)`是**外生变量**变量；且认为已婚女性的受教育年数
`\(educ\)`为**内生变量**。直接构建如下的**“偏误模型”**，并坚持采用OLS方法，估计结果为：

`$$\begin{equation} \begin{alignedat}{999} &amp;log(wage)=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} educ&amp;&amp; + \beta_{3} exper&amp;&amp; + \beta_{4} I(exper^2)&amp;&amp;+u_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(wage)}=&amp;&amp;-0.52&amp;&amp;+0.11educ&amp;&amp;+0.04exper&amp;&amp;-0.00I(exper^2)\\ &amp;\text{(t)}&amp;&amp;(-2.6282)&amp;&amp;(7.5983)&amp;&amp;(3.1549)&amp;&amp;(-2.0628)\\&amp;\text{(se)}&amp;&amp;(0.1986)&amp;&amp;(0.0141)&amp;&amp;(0.0132)&amp;&amp;(0.0004)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.1568;&amp;&amp; \bar{R^2}=0.1509\\&amp; &amp;&amp; F^{\ast}=26.29;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---

## 偏误OLS回归：EViews报告

下面给出EViews软件分析报告：

&lt;img src="pic/chpt11-mroz2-eq-ols.png" width="564" style="display: block; margin: auto;" /&gt;

---

## 两阶段回归法（2SLS）:第一阶段

假设**父亲受教育年数**
`\(fatheduc\)`和**母亲受教育年数**
`\(motheduc\)`都是已婚女性**受教育年数**
`\(educ\)`的理想**工具变量**。

采用工具变量法的对如下**约简方程**：

`$$\begin{equation}
\begin{alignedat}{999} &amp;educ=&amp;&amp; + \pi_{1} &amp;&amp; + \pi_{2} exper&amp;&amp; + \pi_{3} fatheduc&amp;&amp; + \pi_{4} motheduc&amp;&amp;+v_i\\ \end{alignedat}
\end{equation}$$`

对以上**约简方程**进行第一阶段OLS回归，估计结果为：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{educ}=&amp;&amp;+9.10&amp;&amp;+0.05exper&amp;&amp;-0.00I(exper^2)&amp;&amp;+0.19fatheduc&amp;&amp;+0.16motheduc\\ &amp;\text{(t)}&amp;&amp;(21.3396)&amp;&amp;(1.1236)&amp;&amp;(-0.8386)&amp;&amp;(5.6152)&amp;&amp;(4.3906)\\&amp;\text{(se)}&amp;&amp;(0.4266)&amp;&amp;(0.0403)&amp;&amp;(0.0012)&amp;&amp;(0.0338)&amp;&amp;(0.0359)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.2115;&amp;&amp; \bar{R^2}=0.2040\\&amp; &amp;&amp; F^{\ast}=28.36;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---

## 两阶段回归法（2SLS）:第二阶段

利用前述第一阶段回归得到的
`\(\widehat{educ}\)`以及原来的**外生变量**
`\(exper\)`，我们可以构建如下的第二阶段回归模型：

`$$\begin {align} 
log(wage) &amp; =\alpha_{1}+\alpha_{2} exper_{i}+\alpha_{3} exper_{i}^2 +\beta_{1} \widehat{educ}_{i}+\epsilon_{i}
\end {align}$$`

采用OLS方法对以上模型进行估计，得到如下结果：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(wage)}=&amp;&amp;+0.05&amp;&amp;+0.06educ.hat&amp;&amp;+0.04exper&amp;&amp;-0.00I(exper^2)\\ &amp;\text{(t)}&amp;&amp;(0.1146)&amp;&amp;(1.8626)&amp;&amp;(3.1361)&amp;&amp;(-2.1344)\\&amp;\text{(se)}&amp;&amp;(0.4198)&amp;&amp;(0.0330)&amp;&amp;(0.0141)&amp;&amp;(0.0004)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.0498;&amp;&amp; \bar{R^2}=0.0431\\&amp; &amp;&amp; F^{\ast}=7.40;&amp;&amp; p=0.0001 \end{alignedat} \end{equation}$$`

---

## 两阶段最小二乘法（2SLS,不调整方差）：EViews实现

EViews软件的具体设置为：

&lt;img src="pic/chpt11-mroz2-eq-2sls-specif.png" width="661" style="display: block; margin: auto;" /&gt;

---

## 两阶段最小二乘法（2SLS,不调整方差）：EViews结果

EViews软件的分析结果：

&lt;img src="pic/chpt11-mroz2-eq-2sls.png" width="596" style="display: block; margin: auto;" /&gt;

---

## 两阶段最小二乘法（2SLS,怀特矫正）：EViews实现

EViews软件设置：

&lt;img src="pic/chpt11-mroz2-eq-2sls-adj-specif.png" width="660" style="display: block; margin: auto;" /&gt;

---

## 两阶段最小二乘法（2SLS,怀特矫正）：EViews结果

EViews软件分析结果：

&lt;img src="pic/chpt11-mroz2-eq-2sls-adj.png" width="570" style="display: block; margin: auto;" /&gt;

---
class: middle, center, inverse
# 工具变量法的一些讨论

---
class: duke-green, page-font-26
## 工具变量法回归中的判定系数

--

1. 与OLS中的怙况不同，由于IV的SSR实际上可能大亍SST。所以IV估计中的
`\(R^2\)`可能为负。尽管报告IV估计的
`\(R^2\)`也不会有什么害处，但也不是很有用。

--

2. 当自变量
`\(X_i\)`与随机干扰项
`\(v_i\)`**相关**时，因变量
`\(Y_i\)`的方差分解成
`\(\lambda_2^2var(X_i) + var(v_i)\)`，因此**判定系数**
`\(R^2\)`没有合理的解释。进一步地，工具变量回归下
`\(R^2\)`也**不能**用于联合约束的**F检验**。

--

3. 如果目标是为了得到最大的
`\(R^2\)`，我们将总是使用**OLS**。如果采用工具变量法（IV），拟合优度
`\(R^2\)`已经不是其考虑的方面了。

--

4. **两阶段最小二乘法**（2SLS）是GLS方法的一种，它是需要利用额外的信息（工具变量）。很多时候2SLS还需要考虑对**系数方差矩阵**的矫正——**怀特矫正**（White方法）或**HAC矫正**（Neway-West方法）。

---
## 低劣工具变量条件下IV的性质

`$$\begin {align} 
Y_i &amp; = \beta_{1}+\beta_{2}X_i+u_i \\
Y_i &amp; = \alpha_{1}+\alpha_{2}Z_i+v_i
\end {align}$$`

最小二乘法（OLS）估计下：

`$$\begin {align} 
\hat{\beta}_2|_{OLS}^{plim} &amp; = {\beta}_2 +corr(u_i, X_i) \cdot \frac{\sigma_{u_i}}{\sigma_{X_i}}
\end {align}$$`

工具变量法（IV）估计下：

`$$\begin {align} 
\hat{\alpha}_2|_{IV}^{plim} &amp; = {\alpha}_2 +\frac{corr(Z_i, v_i)}{corr(Z_i, X_i)} \cdot \frac{\sigma_{v_i}}{\sigma_{X_i}}
\end {align}$$`

- 如果
`\(corr(Z_i,X_i)=0.2\)`，要使得IV比OLS具有更小的渐近偏误，
`\(corr(Z_i,v_i)\)`必须小于
`\(corr(X_i,u_i)\)`的1/5.

---
background-image: url("pic/thank-you-gif-funny-little-yellow.gif")
class: inverse,center
# 本章结束
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
