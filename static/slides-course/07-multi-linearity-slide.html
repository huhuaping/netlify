<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>07-multi-linearity-slide.utf8.md</title>
    <meta charset="utf-8" />
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <link href="libs/remark-css/duke-blue.css" rel="stylesheet" />
    <link href="libs/remark-css/hygge-duke.css" rel="stylesheet" />
    <script src="libs/htmlwidgets/htmlwidgets.js"></script>
    <script src="libs/jquery/jquery.min.js"></script>
    <link href="libs/datatables-css/datatables-crosstalk.css" rel="stylesheet" />
    <script src="libs/datatables-binding/datatables.js"></script>
    <link href="libs/dt-core/css/jquery.dataTables.min.css" rel="stylesheet" />
    <link href="libs/dt-core/css/jquery.dataTables.extra.css" rel="stylesheet" />
    <script src="libs/dt-core/js/jquery.dataTables.min.js"></script>
    <link href="libs/crosstalk/css/crosstalk.css" rel="stylesheet" />
    <script src="libs/crosstalk/js/crosstalk.min.js"></script>
    <link rel="stylesheet" href="libs\cc-fonts.css" type="text/css" />
    <link rel="stylesheet" href="libs\figure-captions.css" type="text/css" />
    <link rel="stylesheet" href="libs\animate.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">

background-image: url("pic/slide-front-page.jpg")
class: center,middle

# 计量经济学(Econometrics)

### 胡华平

### 西北农林科技大学

### 经济管理学院数量经济教研室

### huhuaping01@hotmail.com

### 2019-03-31





&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 22px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

---
class: center, middle,inverse
# 第7章：放宽基本假设：多重共线性

多重共线性的性质

出现完全多重共线性时的估计问题

"高度"但"不完全"多重共线性时的估计问题

多重共线性:多重共线性的理论后果

多重共线性的实际后果

说明性的例子

多重共线性的侦察

补救措施

多重共线性一定是坏事吗?

一个引申的例:朗利数据


---
class: center, middle,inverse
# 多重共线性的性质

---
## 什么是模型多重共线性？（引子1）

**消费案例**：

`$$\begin {align} 
 Y_{i} &amp;=\hat{\beta}_{1}+\hat{\beta}_{2} X_{2 i}+\hat{\beta}_{3} X_{3 i}+e_{i} 
\end {align}$$`

&lt;img src="pic/chpt7-multi-linearity-demo0.png" width="1413" style="display: block; margin: auto;" /&gt;

- 
`\(Y_i\)`表示消费

- 
`\(X_{2i}\)`表示收入；
`\(X_{3i}\)`表示财富。

---
## 什么是模型多重共线性？（引子2）


**慈善捐款**案例：

**回归模型**：

`$$\begin {align} 
 Y_{i} &amp;=\hat{\beta}_{1}+\hat{\beta}_{2} X_{2 i}+\hat{\beta}_{3} X_{3 i}+e_{i} 
\end {align}$$`

- 
`\(Y_i\)`表示慈善捐款

- 
`\(X_{2i}\)`表示城市总人口；
`\(X_{3i}\)`表示城市GDP。

**现实情况**：城市GDP=城市总人口*人均GDP

**多重共线性来源**：自变量间表现为某种因果关系


---
## 什么是模型多重共线性？（引子3）

**消费者忠诚度**案例：

**回归模型**：

`$$\begin {align} 
 Y_{i} &amp;=\hat{\beta}_{1}+\hat{\beta}_{2} X_{2 i}+\hat{\beta}_{3} X_{3 i}+e_{i} 
\end {align}$$`

- 
`\(Y_i\)`表示消费者忠诚度

- 
`\(X_{2i}\)`表示产品满意度；
`\(X_{3i}\)`体验满意度。

**现实情况**：收入水平与满意度有关（奢侈品、拼多多）

**多重共线性来源**：自变量背后有共同的潜在因素

---
## 什么是模型多重共线性？

`$$\begin{align}
Y_i&amp;=\beta_1+\beta_2X_{2i}+\beta_3X_{3i}+\cdots+\beta_kX_{ki}+u_i 
&amp;&amp; \text{(PRM)}\\
Y_i&amp;=\hat{\beta}_1+\hat{\beta}_2X_{2i}+\hat{\beta}_3X_{3i}+\cdots+\hat{\beta}_kX_{ki}+e_i 
&amp;&amp; \text{(SRM)}
\end{align}$$`

**多重共线性**：在多元线性回归模型中，各解释变量
`\(\{X_2,X_3,\cdots,X_k\}\)`之间有交互相关，但又非完全相关的现象。正式地：

`$$\begin{align}
\lambda_2X_{2i}+\lambda_3X_{3i}+\cdots+\lambda_kX_{ki}+v_i=0 
\end{align}$$`

其中，
`\(v_i\)`为随机误差项。

我们称总体回归模型存在**多重共线性**，此时
`\(\lambda_1;\lambda_2;\cdots;\lambda_k\)`不全为0，且
`\(v_i \neq 0\)`。

我们称总体回归模型存在**完全多重共线性**，此时
`\(\lambda_1;\lambda_2;\cdots;\lambda_k\)`不全为0，且
`\(v_i = 0\)`。

---
## 什么是模型多重共线性？

下面用一个直观图进行说明：

&lt;img src="pic/chpt7-multi-linearity-demo1.png" width="681" style="display: block; margin: auto;" /&gt;

---
## 什么是模型多重共线性？


- 简单相关r(Pearson Relationship)

- 多重共线性(Multicollinearity)

&lt;img src="pic/chpt7-multi-linearity-demo2.png" width="395" style="display: block; margin: auto;" /&gt;

---
## 引起多重共线性的原因

- 数据采集所用的方法: 回归元限于一个范围

- 模型或从中取样的总体受到约束：如做电力消费（Y）对收入（X2）和住房面积（X3）回归时，可能X2高的X3也大

- 模型设定: 如在模型中加入多项式；数据范围小等

- 一个过度决定的模型：回归元个数大于观测次数。医学研究中可能只有少数病人

- 相同的时间趋势。消费支出(Y)对收入、财富和人口的回归

---
## 完全多重共线性时的估计问题

在完全多重共线性的情况下，回归系数是不确定的。

`$$\begin {align} 
 Y_{i} &amp;=\hat{\beta}_{1}+\hat{\beta}_{2} X_{2 i}+\hat{\beta}_{3} X_{3 i}+e_{i} 
\end {align}$$`

`$$\begin {align} 
\hat{\beta}_{1} &amp;=\overline{Y}-\hat{\beta}_{2} \overline{X}-\hat{\beta}_{3} \overline{X}_{3} \\ 
\hat{\beta}_{2} &amp;=\frac{\left(\sum y_{i} x_{2 i}\right)\left(\sum x_{3 i}^{2}\right)-\left(\sum y_{i} x_{3 i}\right)\left(\sum x_{2 i} x_{3 i}\right)}{\left(\sum x_{2 i}^{2}\right)\left(\sum x_{3 i}^{2}\right)-\left(\sum x_{2 i} x_{3 i}\right)^{2}} \\ 
\hat{\beta}_{3} &amp;=\frac{\left(\sum y_{i} x_{3 i}\right)\left(\sum x_{2 i}^{2}\right)-\left(\sum y_{i} x_{2 i}\right)\left(\sum x_{2 i} x_{3 i}\right)}{\left(\sum x_{2 i}^{2}\right)\left(\sum x_{3 i}^{2}\right)-\left(\sum x_{2 i} x_{3 i}\right)^{2}} 
\end {align}$$`

---
## 完全多重共线性时的估计问题

在完全多重共线性的情况下，回归系数是不确定的。

假设**完全共线性**情形下，
`\(X_{3 i}=\lambda X_{2 i}\)`，则容易发现：

`$$\begin {align} 
\hat{\beta}_{2}=\frac{\left(\sum y_{i} x_{2 i}\right)\left(\lambda^{2} \sum x_{2 i}^{2}\right)-\left(\lambda \sum y_{i} x_{2 i}\right)\left(\lambda \sum x_{2 i}^{2}\right)}{\left(\sum x_{2 i}^{2}\right)\left(\lambda^{2} \sum x_{2 i}^{2}\right)-\lambda^{2}\left(\sum x_{2 i}^{2}\right)^{2}}=\frac{0}{0}
\end {align}$$`


---
## 完全多重共线性时的估计问题

在完全多重共线性的情况下，回归系数是不确定的。

假定**不完全多重共线性**下，
`\(x_{3 i}=\lambda x_{2 i}+v_{i}\)`，则有：

`$$\begin {align} 
\hat{\beta}_{2}=\frac{\sum\left(y_{i} x_{2 i}\right)\left(\lambda^{2} \sum x_{2 i}^{2}+\sum v_{i}^{2}\right)-\left(\lambda \sum y_{i} x_{2 i}+\sum y_{i} v_{i}\right)\left(\lambda \sum x_{2 i}^{2}\right)}{\sum x_{2 i}^{2}\left(\lambda^{2} \sum x_{2 i}^{2}+\sum v_{i}^{2}\right)-\left(\lambda \sum x_{2 i}^{2}\right)^{2}}
\end {align}$$`

如果
`\(v_i\)`足够小，以至于接近于零，则上式将表示**完全共线性**情形。

---
## 多重共线性的理论后果

如果模型出现多重共线性问题，在N-CLRM假设下，OLS估计量仍然是最优线性无偏估计量（BLUE）：

- 只要不是**完全共线性**，在近似多重共线性的情形下，OLS估计量仍然是无偏的

- 只要不是**完全共线性**，在近似多重共线性的情形下，OLS估计量的方差一定是小的

- 多重共线性本质上是一种样本现象。即使总体中X变量间不存在共线性，由于抽样方法或小样本问题，也可能带来多重共线性问题


---
## 多重共线性的实际后果

**实际后果1**：更大的方差和协方差，估计精度大大下降。
`\(\hat{\beta}_2\)`和
`\(\hat{\beta}_3\)`的**真实方差**分别为：

`$$\begin {align} 
\sigma_{\hat{\beta}_{2}}^{2} 
 =\frac{\sigma^{2}}{\sum x_{2 i}^{2}\left(1-r_{23}^{2}\right)} 
 \equiv \frac{\sigma^{2}}{\sum x_{2 i}^{2} \cdot TOL} 
 \equiv \frac{\sigma^{2}}{\sum x_{2 i}^{2}} \cdot VIF 
\end {align}$$`

`$$\begin {align} 
\sigma_{\hat{\beta}_{3}}^{2} 
 =\frac{\sigma^{2}}{\sum x_{3 i}^{2}\left(1-r_{23}^{2}\right)} 
 \equiv \frac{\sigma^{2}}{\sum x_{3 i}^{2} \cdot TOL} 
 \equiv \frac{\sigma^{2}}{\sum x_{3 i}^{2}} \cdot VIF 
\end {align}$$`

`$$\begin {align} 
\operatorname{cov}\left(\hat{\beta}_{2}, \hat{\beta}_{3}\right) &amp; =\frac{-r_{23} \sigma^{2}}{\left(1-r_{23}^{2}\right) \sqrt{\sum x_{2 i}^{2}} \sqrt{\sum x_{3 i}^{2}}} &amp;&amp; \leftarrow \left[ r_{23}^{2}=\frac{\left(\sum x_{2 i} x_{3 i}\right)^{2}}{\sum x_{2 i}^{2} \sum x_{3 i}^{2}} \right] 
\end {align}$$`

`$$\begin {align} 
r_{23}^{2}  \equiv \frac{\left(\sum x_{2 i} x_{3 i}\right)^{2}}{\sum x_{2 i}^{2} \sum x_{3 i}^{2}};  \quad
TOL  \equiv \left(1-r_{23}^{2}\right);  \quad
VIF  \equiv \frac{1}{\left(1-r_{23}^{2}\right)} 
\end {align}$$`

随着
`\(r_{23}\)`的增大，方差和协方差的绝对值也增大。

---
## 多重共线性的实际后果

方差增大的速度用**方差膨胀因子**(VIF, variance-inflating factor)衡量：

`$$\begin {align} 
VIF=\frac{1}{\left(1-r_{23}^{2}\right)}
\end {align}$$`

**容忍度**(tolerance , TOL)定义为VIF的倒数：

`$$\begin {align} 
\mathrm{TOL}_{\mathrm{j}}=\frac{1}{VIF_{\mathrm{j}}}=1-R_{j}^{2}
\end {align}$$`

`\(R^2_j\)`表示
`\(X_j\)`对其余(k-2)个回归元进行回归的判定系数

- 当
`\(R_j^2=1\)`，即完全共线性时，
`\(TOL_j=0\)`; 

- 当
`\(R_j^2=0\)`，即不存在共线性时，
`\(TOL_j=1\)`; 

- 由于VIF 和TOL 之间有密切关系，所以可以将它们互换使用。

&gt; 注意: （古扎拉蒂）在k个变量的回归模型中有是k-1个回归元。


---
## 多重共线性的实际后果

**实际后果2**：置信区间变宽，系数的检验倾向于不显著！即更倾向接受原假设H0，认为系数为零。

- 标准误增大，则有关总体参数的置信区间随之变大。

`$$\begin{align}
\hat{\beta}_{1} \pm t_{1- \alpha / 2}(n-k) \cdot S_{\hat{\beta}_{1}}\\
\hat{\beta}_{2} \pm t_{1- \alpha / 2}(n-k) \cdot S_{\hat{\beta}_{2}}\\
\hat{\beta}_{3} \pm t_{1- \alpha / 2}(n-k) \cdot S_{\hat{\beta}_{3}}
\end{align}$$`



---
## 多重共线性的实际后果

**实际后果3**：系数的t值倾向于统计上不显著，但
`\(R^2\)`却会很高。

`$$\begin{align} 
t^{\ast}_{\hat{\beta}_1}&amp;=\frac{\hat{\beta}_{1}}{S_{\hat{\beta}_{1}}} \\
t^{\ast}_{\hat{\beta}_2}&amp;=\frac{\hat{\beta}_{2}}{S_{\hat{\beta}_{2}}}
\\
t^{\ast}_{\hat{\beta}_3}&amp;=\frac{\hat{\beta}_{3}}{S_{\hat{\beta}_{3}}}
\end{align}$$`


将
`\(t^{\ast}_{\hat{\beta}_i}\)`值和临界t值相比较。高度共线性使估计的标准误增加很快，t值迅速变小。

因而，在高度多重共线性的情形下，增加了接受错误假设的概率（第二类错误）

---
## 多重共线性的实际后果

**实际后果3**：系数的t值倾向于统计上不显著，但
`\(R^2\)`却会很高。

- 在高度共线性情形中，有可能会发现一个或多个偏斜率系数基于t检验不是个别统计显著的，然而这时
`\(R^2\)`却高达(比如说)0.9以上，从而根据F检验，可令人信服地拒绝原假设：

`$$H_{0} : \beta_{2}=\beta_{3}=\cdots=\beta_{k}=0$$`
- 但是，个别偏回归系数的t检验可能并不显著——这就是多重共线性的一个信号

- 这里的真正问题在于估计量之间的协方差，而这些协方差是同回归元之间的相关性有关系的。

---
## 多重共线性的实际后果

**实际后果4**：OLS估计量及其标准误对数据的微小变化非常敏感。



---
### 说明性例子（数据）




&lt;div class="figure" style="text-align: center"&gt;
<div id="htmlwidget-95d23df90af6ac33853a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-95d23df90af6ac33853a">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16","17","18","19","20","21","22","23","24","25","26","27","28","29","30","31","32","33","34","35","36","37","38","39","40","41","42","43","44","45","46","47","48","49","50","51","52","53","54"],[1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962,1963,1964,1965,1966,1967,1968,1969,1970,1971,1972,1973,1974,1975,1976,1977,1978,1979,1980,1981,1982,1983,1984,1985,1986,1987,1988,1989,1990,1991,1992,1993,1994,1995,1996,1997,1998,1999,2000],[976.4,998.1,1025.3,1090.9,1107.1,1142.4,1197.2,1221.9,1310.4,1348.8,1381.8,1393,1470.7,1510.8,1541.2,1617.3,1684,1784.8,1897.6,2006.1,2066.2,2184.2,2264.8,2317.5,2405.2,2550.5,2675.9,2653.7,2710.9,2868.9,2992.1,3124.7,3203.2,3193,3236,3275.5,3454.3,3640.6,3820.9,3981.2,4113.4,4279.5,4393.7,4474.5,4466.6,4594.5,4748.9,4928.1,5075.6,5237.5,5423.9,5683.7,5968.4,6257.8],[1035.2,1090,1095.6,1192.7,1227,1266.8,1327.5,1344,1433.8,1502.3,1539.5,1553.7,1623.8,1664.8,1720,1803.5,1871.5,2006.9,2131,2244.6,2340.5,2448.2,2524.3,2630,2745.3,2874.3,3072.3,3051.9,3108.5,3243.5,3360.7,3527.5,3628.6,3658,3741.1,3791.7,3906.9,4207.6,4347.8,4486.6,4582.5,4784.1,4906.5,5014.2,5033,5189.3,5261.3,5397.2,5539.1,5677.7,5854.5,6168.6,6320,6539.2],[5166.815,5280.757,5607.351,5759.515,6086.056,6243.864,6355.613,6797.027,7172.242,7375.18,7315.286,7869.975,8188.054,8351.757,8971.872,9091.545,9436.097,10003.4,10562.81,10522.04,11312.07,12145.41,11672.25,11650.04,12312.92,13499.92,13080.96,11868.79,12634.36,13456.78,13786.31,14450.5,15340,15964.95,15964.99,16312.51,16944.85,17526.75,19068.35,20530.04,21235.69,22331.99,23659.8,23105.13,24050.21,24418.2,25092.33,25218.6,27439.73,29448.19,32664.07,35587.02,39591.26,38167.72],[-10.35094,-4.719804,1.044063,0.407346,-5.283152,-0.277011,0.561137,-0.138476,0.261997,-0.736124,-0.260683,-0.57463,2.295943,1.511181,1.296432,1.395922,2.057616,2.026599,2.111669,2.020251,1.212616,1.054986,1.732154,1.166228,-0.712241,-0.155737,1.413839,-1.042571,-3.533585,-0.656766,-1.190427,0.113048,1.70421,2.298496,4.703847,4.449027,4.690972,5.848332,4.330504,3.768031,2.819469,3.287061,4.317956,3.595025,1.802757,1.007439,0.62479,2.206002,3.333143,3.083201,3.12,3.583909,3.245271,3.57597]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>Year<\/th>\n      <th>Y<\/th>\n      <th>X2<\/th>\n      <th>X3<\/th>\n      <th>X4<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"dom":"tip","columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
&lt;p class="caption"&gt;消费支出与收入和财富的数据(n=54)&lt;/p&gt;
&lt;/div&gt;

- `\(Y_i\)`表示人均消费支出

- `\(X_{2i}\)`表示人均收入；
`\(X_{3i}\)`表示财富；
`\(X_{4i}\)`表示利率


---
### 说明性例子（精简分析报告）

我们可以构建如下的回归模型：

`$$\begin{equation} \begin{alignedat}{999} &amp;log(Y)=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} log(X2)&amp;&amp; + \beta_{3} log(X3)&amp;&amp; + \beta_{4} X4&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`


计算并整理回归分析结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(Y)}=&amp;&amp;-0.47&amp;&amp;+0.80log(X2)&amp;&amp;+0.20log(X3)&amp;&amp;-0.00X4\\ &amp;\text{(t)}&amp;&amp;(-10.9334)&amp;&amp;(45.9984)&amp;&amp;(11.4406)&amp;&amp;(-3.5293)\\&amp;\text{(se)}&amp;&amp;(0.0428)&amp;&amp;(0.0175)&amp;&amp;(0.0176)&amp;&amp;(0.0008)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9996;&amp;&amp; \bar{R^2}=0.9995\\&amp; &amp;&amp; F^{\ast}=37832.61;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
### 说明性例子（EViews软件报告）


&lt;img src="pic/chpt7-example-EViews-10-2-US.png" width="793" style="display: block; margin: auto;" /&gt;


---
### 说明性例子（R软件报告）

利用`R`软件给出更为详细的分析报告如下：


```

Call:
lm(formula = mod_mat, data = data_income)

Residuals:
     Min       1Q   Median       3Q      Max 
-0.01844 -0.01000  0.00034  0.00704  0.03258 

Coefficients:
             Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) -0.467711   0.042778  -10.93  7.3e-15 ***
log(X2)      0.804873   0.017498   46.00  &lt; 2e-16 ***
log(X3)      0.201270   0.017593   11.44  1.4e-15 ***
X4          -0.002689   0.000762   -3.53   0.0009 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.0119 on 50 degrees of freedom
Multiple R-squared:     1,	Adjusted R-squared:     1 
F-statistic: 3.78e+04 on 3 and 50 DF,  p-value: &lt;2e-16
```


---
### 说明性例子（ANOVA）



<div id="htmlwidget-9e746e799994433c3447" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-9e746e799994433c3447">{"x":{"filter":"none","data":[["1","2","3"],["回归平方和","残差平方和","总平方和"],["ESS","RSS","TSS"],["\\( \\sum{\\hat{y}_i^2} \\)","\\( \\sum{e_i^2} \\)","\\( \\sum{y_i^2}\\)"],[3,50,53],["\\(MSS_{ESS}\\)","\\(MSS_{RSS}\\)","\\(MSS_{TSS}\\)"],["\\(  \\boldsymbol{\\hat{\\beta}'X'y}-n\\bar{Y}^2= \\)16.1637","\\( \\boldsymbol{yy'-\\hat{\\beta}'X'y} \\)=0.0071","\\( \\boldsymbol{y'y}-n\\bar{Y}^2 \\)=16.1709"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>变异来源<\/th>\n      <th>平方和符号SS<\/th>\n      <th>平方和计算公式<\/th>\n      <th>自由度df<\/th>\n      <th>均方和符号MSS<\/th>\n      <th>均方和计算公式<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false}},"evals":[],"jsHooks":[]}</script>

---
### 说明性例子（F检验）

根据方差分析表ANOVA和样本F统计量计算公式，可以得到：

`$$\begin{align}
F^{\ast}=\frac{ESS_U/df_{ESS_U}}{RSS_U/df_{RSS_U}}=\frac{\left(\boldsymbol{\hat{\beta}'X'y}-n\bar{Y}^2\right)/{(k-1)}}{\left(\boldsymbol{yy'-\hat{\beta}'X'y}\right)/{(n-k)}}
=\frac{16.1637/3}{0.0071/50} =37832.6142
\end{align}$$`

得到显著性检验的判断结论。因为
`\(F^{\ast}=\)` 37832.6142 .red[**大于**] `\(F_{1-\alpha}(k-1,n-k)=F_{0.95}\)`(3,50)=2.7900，所以模型整体显著性的F检验结果**显著**。

---
### 说明性例子（其他模型）

此外，我们可以分别构建如下回归模型，并得到回归分析结果：

.pull-left[

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(X3)}=&amp;&amp;+1.65&amp;&amp;+0.99log(X2)\\ &amp;\text{(t)}&amp;&amp;(8.8575)&amp;&amp;(42.0585)\\&amp;\text{(se)}&amp;&amp;(0.1868)&amp;&amp;(0.0235)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9714;&amp;&amp; \bar{R^2}=0.9709\\&amp; &amp;&amp; F^{\ast}=1768.92;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(X2)}=&amp;&amp;-1.40&amp;&amp;+0.98log(X3)\\ &amp;\text{(t)}&amp;&amp;(-6.2996)&amp;&amp;(42.0585)\\&amp;\text{(se)}&amp;&amp;(0.2223)&amp;&amp;(0.0234)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9714;&amp;&amp; \bar{R^2}=0.9709\\&amp; &amp;&amp; F^{\ast}=1768.92;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

]

.pull-right[

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(Y)}=&amp;&amp;-0.07&amp;&amp;+1.00log(X2)\\ &amp;\text{(t)}&amp;&amp;(-1.6471)&amp;&amp;(178.3010)\\&amp;\text{(se)}&amp;&amp;(0.0444)&amp;&amp;(0.0056)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9984;&amp;&amp; \bar{R^2}=0.9983\\&amp; &amp;&amp; F^{\ast}=31791.26;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{log(Y)}=&amp;&amp;-1.52&amp;&amp;+0.98log(X3)\\ &amp;\text{(t)}&amp;&amp;(-8.3771)&amp;&amp;(51.5957)\\&amp;\text{(se)}&amp;&amp;(0.1814)&amp;&amp;(0.0191)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9808;&amp;&amp; \bar{R^2}=0.9805\\&amp; &amp;&amp; F^{\ast}=2662.12;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

]

---
class: center, middle,inverse
# 如何诊断模型多重共线性问题？


---
## 多重共线性诊断：郎利案例



&lt;div class="figure" style="text-align: center"&gt;
<div id="htmlwidget-2776c2e77ada3afcf920" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-2776c2e77ada3afcf920">{"x":{"filter":"none","data":[["1","2","3","4","5","6","7","8","9","10","11","12","13","14","15","16"],[1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,null],[1947,1948,1949,1950,1951,1952,1953,1954,1955,1956,1957,1958,1959,1960,1961,1962],[60323,61122,60171,61187,63221,63639,64989,63761,66019,67857,68169,66513,68655,69564,69331,70551],[830,885,882,895,962,981,990,1000,1012,1046,1084,1108,1126,1142,1157,1169],[234289,259426,258054,284599,328975,346999,365385,363112,397469,419180,442769,444546,482704,502601,518173,554894],[2356,2325,3682,3351,2099,1932,1870,3578,2904,2822,2936,4681,3813,3931,4806,4007],[1590,1456,1616,1650,3099,3594,3547,3350,3048,2857,2798,2637,2552,2514,2572,2827],[107608,108632,109773,110929,112075,113270,115094,116219,117388,118734,120445,121950,123366,125368,127852,130081],[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>obs<\/th>\n      <th>Year<\/th>\n      <th>Y<\/th>\n      <th>X2<\/th>\n      <th>X3<\/th>\n      <th>X4<\/th>\n      <th>X5<\/th>\n      <th>X6<\/th>\n      <th>X7<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":5,"dom":"tip","columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100]}},"evals":[],"jsHooks":[]}</script>
&lt;p class="caption"&gt;美国就业情况的郎利数据(n=16)&lt;/p&gt;
&lt;/div&gt;

- `\(Y_i\)`表示就业人数；

- `\(X_{2i}\)`表示消费价格指数（CPI）；
`\(X_{3i}\)`表示国民生产总值（GNP，以百万美元计); 
`\(X_{4i}\)`表示失业人数(以千人计) ;
`\(X_{5i}\)`表示军队中的人数; 
`\(X_{6i}\)`表示14 岁以上的非机构人口数; 
`\(X_{7i}\)`表示时间（
`\(t=1,2,\cdots,n\)`）

---
## 相关性分析诊断法
 
利用样本数据绘制图形和图表，分析自变量之间是否存在明显相关关系。如果有，则表明模型很可能会产生**多重共线性**问题。

判断依据：

- **相关系数矩阵**发现高度线性相关（相关系数大于0.8）

- **散点图矩阵**发现高度线性相关的数据分布模式

---
## 相关性分析诊断法（案例）

郎利案例中，相关系数矩阵表计算如下：

&lt;div class="figure" style="text-align: center"&gt;
<div id="htmlwidget-64f2742228d97d5b4c97" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-64f2742228d97d5b4c97">{"x":{"filter":"none","data":[["X2","X3","X4","X5","X6","X7"],[1,0.991589178024782,0.620633392559096,0.464744187600675,0.979163432977498,0.991149190067205],[0.991589178024782,1,0.604260939889558,0.446436791892627,0.991090069458478,0.995273483764785],[0.620633392559096,0.604260939889558,1,-0.177420629501878,0.686551516365312,0.668256604562175],[0.464744187600675,0.446436791892627,-0.177420629501878,1,0.364416267189032,0.417245149834945],[0.979163432977498,0.991090069458478,0.686551516365312,0.364416267189032,1,0.993952846232926],[0.991149190067205,0.995273483764785,0.668256604562175,0.417245149834945,0.993952846232926,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>X2<\/th>\n      <th>X3<\/th>\n      <th>X4<\/th>\n      <th>X5<\/th>\n      <th>X6<\/th>\n      <th>X7<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"pageLength":7,"dom":"t","columnDefs":[{"className":"dt-right","targets":[1,2,3,4,5,6]},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[7,10,25,50,100],"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 3, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 4, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 5, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 6, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
&lt;p class="caption"&gt;相关系数矩阵表&lt;/p&gt;
&lt;/div&gt;

&gt; 
`\(Y_i\)`表示就业人数；
`\(X_{2i}\)`表示消费价格指数（CPI）；
`\(X_{3i}\)`表示国民生产总值（GNP，以百万美元计); 
`\(X_{4i}\)`表示失业人数(以千人计) ;
`\(X_{5i}\)`表示军队中的人数; 
`\(X_{6i}\)`表示14 岁以上的非机构人口数; 
`\(X_{7i}\)`表示时间（
`\(t=1,2,\cdots,n\)`）

---
## 相关性分析诊断法（案例）

郎利案例中，散点矩阵图如下：

&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;


---
## 主回归方程诊断法

如果主回归方程分析报告结果异常，则可能认为存在多重共线性问题。

**诊断依据**：

- 主回归分析报告的
`\(R^2\)`值高（大于0.8）

- 分析报告
`\(F^{\ast}\)`检验显著

- 不显著的
`\(t^\ast\)`检验较多（多于回归系数个数的一半及以上）



---
## 主回归方程诊断法（案例简要报告）

郎利案例中，我们构建如下主回归模型：

`$$\begin{equation} \begin{alignedat}{999} &amp;Y=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X3&amp;&amp; + \beta_{4} X4\\ &amp;\text{(cont.)}&amp;&amp; + \beta_{5} X5&amp;&amp; + \beta_{6} X6&amp;&amp; + \beta_{7} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

主回归模型的回归分析结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{Y}=&amp;&amp;+77270.12&amp;&amp;+1.51X2&amp;&amp;-0.04X3&amp;&amp;-2.02X4\\ &amp;\text{(t)}&amp;&amp;(3.4332)&amp;&amp;(0.1774)&amp;&amp;(-1.0695)&amp;&amp;(-4.1364)\\&amp;\text{(se)}&amp;&amp;(22506.7070)&amp;&amp;(8.4915)&amp;&amp;(0.0335)&amp;&amp;(0.4884)\\&amp;\text{(cont.)}&amp;&amp;-1.03X5&amp;&amp;-0.05X6&amp;&amp;+1829.15X7\\&amp;\text{(t)}&amp;&amp;(-4.8220)&amp;&amp;(-0.2261)&amp;&amp;(4.0159)\\&amp;\text{(se)}&amp;&amp;(0.2143)&amp;&amp;(0.2261)&amp;&amp;(455.4785)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9955;&amp;&amp; \bar{R^2}=0.9925\\&amp; &amp;&amp; F^{\ast}=330.29;&amp;&amp; p=0.0000\\ \end{alignedat} \end{equation}$$`

---
## 主回归方程诊断法（案例R报告）

&lt;!---`R`软件下的详细回归分析结果见左边：---&gt;


```

Call:
lm(formula = mods$main, data = data_longley)

Residuals:
   Min     1Q Median     3Q    Max 
-410.1 -157.7  -28.2  101.6  455.4 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 77270.1155 22506.7070    3.43  0.00747 ** 
X2              1.5062     8.4915    0.18  0.86314    
X3             -0.0358     0.0335   -1.07  0.31268    
X4             -2.0202     0.4884   -4.14  0.00254 ** 
X5             -1.0332     0.2143   -4.82  0.00094 ***
X6             -0.0511     0.2261   -0.23  0.82621    
X7           1829.1515   455.4785    4.02  0.00304 ** 
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 305 on 9 degrees of freedom
Multiple R-squared:  0.995,	Adjusted R-squared:  0.992 
F-statistic:  330 on 6 and 9 DF,  p-value: 4.98e-10
```


---
## 辅助回归方程诊断法（判定系数）

**辅助回归方程**：为侦察多元回归模型是否存在多重共线性，构建关于自变量之间的一类线性回归方程。

正式地，对于样本回归模型，我们可以构建得到如下辅助回归方程：

`$$\begin{align}
X_{2i}&amp;=\hat{\alpha}_1+\cdots+\hat{\alpha}_jX_{ji}+\cdots+\hat{\alpha}_kX_{ki}+\epsilon_{2i} \\
      &amp;\vdots\\   
X_{ji}&amp;=\hat{\alpha}_1+\hat{\alpha}_2X_{2i}+\cdots+\hat{\alpha}_kX_{ki}+\epsilon_{ji} \\
      &amp;\vdots\\                                                                                      X_{ki}&amp;=\hat{\alpha}_1+\hat{\alpha}_2X_{2i}+\cdots+\hat{\alpha}_jX_{ji}+\cdots+\epsilon_{ki} \\
\end{align}$$`


对于样本回归模型，在普通最小二乘法下，我们可以证明：

`$$\begin{align}
S^2_{\hat{\beta}_j}=\frac{\hat{\sigma}^2}{(n-1)S^2_{X_j}} \cdot \frac{1}{1-R^2_j} 
\end{align}$$`

其中
`\(R^2_j\)`为辅助回归方程的判定系数。
`\(S^2_{X_j}\)`为变量
`\(X_j\)`的样本方差。

---
## 辅助回归方程诊断法（判定系数）

**克莱因经验法则**(Klein‘s rule of  thumb)： 当来自一个辅助回归的
`\(R^2_j\)`𝟐大于得自主回归中的
`\(R^2\)`值时，多重共线性才算是一个麻烦的问题。


---
## 辅助回归方程诊断法（辅助方程X2）

郎利案例中，辅助回归方程（
`\(X_2\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X2=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X3&amp;&amp; + \beta_{3} X4&amp;&amp; + \beta_{4} X5&amp;&amp; + \beta_{5} X6&amp;&amp; + \beta_{6} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X2}=&amp;&amp;+2044.58&amp;&amp;+0.00X3&amp;&amp;+0.03X4&amp;&amp;+0.01X5&amp;&amp;-0.02X6&amp;&amp;-9.99X7\\ &amp;\text{(t)}&amp;&amp;(3.8333)&amp;&amp;(2.7006)&amp;&amp;(2.1098)&amp;&amp;(1.1770)&amp;&amp;(-2.7720)&amp;&amp;(-0.5996)\\&amp;\text{(se)}&amp;&amp;(533.3698)&amp;&amp;(0.0009)&amp;&amp;(0.0151)&amp;&amp;(0.0075)&amp;&amp;(0.0063)&amp;&amp;(16.6654)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9926;&amp;&amp; \bar{R^2}=0.9889\\&amp; &amp;&amp; F^{\ast}=269.06;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（辅助方程X3）

郎利案例中，辅助回归方程（
`\(X_3\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X3=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X4&amp;&amp; + \beta_{4} X5&amp;&amp; + \beta_{5} X6&amp;&amp; + \beta_{6} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X3}=&amp;&amp;-480986.04&amp;&amp;+164.66X2&amp;&amp;-13.79X4&amp;&amp;-3.00X5&amp;&amp;+5.62X6&amp;&amp;+10902.88X7\\ &amp;\text{(t)}&amp;&amp;(-3.2408)&amp;&amp;(2.7006)&amp;&amp;(-9.1921)&amp;&amp;(-1.6774)&amp;&amp;(4.7649)&amp;&amp;(4.2411)\\&amp;\text{(se)}&amp;&amp;(148413.7872)&amp;&amp;(60.9699)&amp;&amp;(1.5002)&amp;&amp;(1.7873)&amp;&amp;(1.1804)&amp;&amp;(2570.7562)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9994;&amp;&amp; \bar{R^2}=0.9992\\&amp; &amp;&amp; F^{\ast}=3575.03;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（辅助方程X4）

郎利案例中，辅助回归方程（
`\(X_4\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X4=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X3&amp;&amp; + \beta_{4} X5&amp;&amp; + \beta_{5} X6&amp;&amp; + \beta_{6} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X4}=&amp;&amp;-28518.24&amp;&amp;+9.65X2&amp;&amp;-0.06X3&amp;&amp;-0.27X5&amp;&amp;+0.35X6&amp;&amp;+768.55X7\\ &amp;\text{(t)}&amp;&amp;(-2.4914)&amp;&amp;(2.1098)&amp;&amp;(-9.1921)&amp;&amp;(-2.4895)&amp;&amp;(3.6779)&amp;&amp;(4.6007)\\&amp;\text{(se)}&amp;&amp;(11446.8866)&amp;&amp;(4.5736)&amp;&amp;(0.0071)&amp;&amp;(0.1090)&amp;&amp;(0.0954)&amp;&amp;(167.0507)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9703;&amp;&amp; \bar{R^2}=0.9554\\&amp; &amp;&amp; F^{\ast}=65.24;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（辅助方程X5）

郎利案例中，辅助回归方程（
`\(X_5\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X5=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X3&amp;&amp; + \beta_{4} X4&amp;&amp; + \beta_{5} X6&amp;&amp; + \beta_{6} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X5}=&amp;&amp;-11881.24&amp;&amp;+13.82X2&amp;&amp;-0.07X3&amp;&amp;-1.41X4&amp;&amp;+0.20X6&amp;&amp;+1167.78X7\\ &amp;\text{(t)}&amp;&amp;(-0.3600)&amp;&amp;(1.1770)&amp;&amp;(-1.6774)&amp;&amp;(-2.4895)&amp;&amp;(0.6084)&amp;&amp;(2.0791)\\&amp;\text{(se)}&amp;&amp;(33002.4231)&amp;&amp;(11.7447)&amp;&amp;(0.0437)&amp;&amp;(0.5663)&amp;&amp;(0.3276)&amp;&amp;(561.6770)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.7214;&amp;&amp; \bar{R^2}=0.5820\\&amp; &amp;&amp; F^{\ast}=5.18;&amp;&amp; p=0.0133 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（辅助方程X6）

郎利案例中，辅助回归方程（
`\(X_6\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X6=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X3&amp;&amp; + \beta_{4} X4&amp;&amp; + \beta_{5} X5&amp;&amp; + \beta_{6} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X6}=&amp;&amp;+95694.37&amp;&amp;-24.76X2&amp;&amp;+0.12X3&amp;&amp;+1.64X4&amp;&amp;+0.18X5&amp;&amp;-782.04X7\\ &amp;\text{(t)}&amp;&amp;(11.0221)&amp;&amp;(-2.7720)&amp;&amp;(4.7649)&amp;&amp;(3.6779)&amp;&amp;(0.6084)&amp;&amp;(-1.3319)\\&amp;\text{(se)}&amp;&amp;(8682.0335)&amp;&amp;(8.9319)&amp;&amp;(0.0259)&amp;&amp;(0.4454)&amp;&amp;(0.2943)&amp;&amp;(587.1614)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9975;&amp;&amp; \bar{R^2}=0.9962\\&amp; &amp;&amp; F^{\ast}=796.30;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（辅助方程X7）

郎利案例中，辅助回归方程（
`\(X_7\)`）的简要结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;X7=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X2&amp;&amp; + \beta_{3} X3&amp;&amp; + \beta_{4} X4&amp;&amp; + \beta_{5} X5&amp;&amp; + \beta_{6} X6&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{X7}=&amp;&amp;+8.31&amp;&amp;-0.00X2&amp;&amp;+0.00X3&amp;&amp;+0.00X4&amp;&amp;+0.00X5&amp;&amp;-0.00X6\\ &amp;\text{(t)}&amp;&amp;(0.5392)&amp;&amp;(-0.5996)&amp;&amp;(4.2411)&amp;&amp;(4.6007)&amp;&amp;(2.0791)&amp;&amp;(-1.3319)\\&amp;\text{(se)}&amp;&amp;(15.4036)&amp;&amp;(0.0058)&amp;&amp;(0.0000)&amp;&amp;(0.0002)&amp;&amp;(0.0001)&amp;&amp;(0.0001)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9987;&amp;&amp; \bar{R^2}=0.9980\\&amp; &amp;&amp; F^{\ast}=1515.96;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 辅助回归方程诊断法（判定系数比较）



主回归模型的判定系数为0.9955，辅助回归的判定系数见下表：




&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-07caa828518750f64f52" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-07caa828518750f64f52">{"x":{"filter":"none","data":[["1","2","3","4","5","6"],["X2","X3","X4","X5","X6","X7"],[0.992621692543197,0.999440876454294,0.97025481857758,0.721365435910039,0.997494682603577,0.998682443262164],["...","严重问题","...","...","严重问题","严重问题"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th> <\/th>\n      <th>models<\/th>\n      <th>R2<\/th>\n      <th>判定系数诊断结论<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"},{"orderable":false,"targets":0}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;辅助回归方程的判定系数&lt;/p&gt;
&lt;/div&gt;


---
## 辅助回归方程诊断法（VIF）

辅助回归方程方差膨胀因子的理论计算公式为
    
`$$\begin{equation}
VIF_j=\frac{1}{1-R^2_j},(j=1,2,\cdots,k-1) 
\end{equation}$$`

诊断依据：

- 辅助回归方程的方差膨胀因子中如果
`\(VIF_j \in [10,100]\)`表明中度多重共线性；

- 如果
`\(VIF_j \geq 100\)`表明严重多重共线性

---
## 辅助回归方程诊断法（VIF）

郎利案例中，辅助回归方程的VIF诊断结果为：






&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-ee8260f76f6980e9d52d" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ee8260f76f6980e9d52d">{"x":{"filter":"none","data":[["X2","X3","X4","X5","X6","X7"],[0.992621692543197,0.999440876454294,0.97025481857758,0.721365435910039,0.997494682603577,0.998682443262164],[135.532438279973,1788.51348271776,33.618890596048,3.58893019344555,399.151022312547,758.980597406721],["非常严重","非常严重","比较严重","...","非常严重","非常严重"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>models<\/th>\n      <th>R2<\/th>\n      <th>VIF<\/th>\n      <th>VIF诊断结论<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;方差膨胀因子分析&lt;/p&gt;
&lt;/div&gt;


根据计算结果汇总，可以认为主模型存在较严重的多重共线性问题。其中VIF值大于100的系数就包括X2、X3、X6、X7。

---
## 辅助回归方程诊断法（VIF）

`EViews`软件给出的VIF结果：

&lt;img src="pic/chpt7-EViews-VIF.png" width="685" style="display: block; margin: auto;" /&gt;


---
## 辅助回归方程诊断法（TOL）

辅助回归方程容忍度的理论计算公式为：
    
`$$\begin{equation}
TOL_j=1-R^2_j=\frac{1}{VIF_j},(j=1, 2, \cdots,k-1) 
\end{equation}$$`


诊断依据：

- 如果辅助回归方程的容忍度
`\(TOL_j\in[0.01,0.1]\)`表明中度多重共线性；

- 如果辅助回归方程的容忍度
`\(TOL_j\leq{0.01}\)`表明存在严重的多重共线性

---
## 辅助回归方程诊断法（TOL）

郎利案例中，辅助回归方程的TOL诊断结果为：






&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-d728d29907d07f75def5" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-d728d29907d07f75def5">{"x":{"filter":"none","data":[["X2","X3","X4","X5","X6","X7"],[0.992621692543197,0.999440876454294,0.97025481857758,0.721365435910039,0.997494682603577,0.998682443262164],[135.532438279973,1788.51348271776,33.618890596048,3.58893019344555,399.151022312547,758.980597406721],[0.00737830745680437,0.00055912354570592,0.0297451814224218,0.278634564089961,0.00250531739642388,0.00131755673783598],["非常严重","非常严重","...","比较严重","非常严重","非常严重"]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>models<\/th>\n      <th>R2<\/th>\n      <th>VIF<\/th>\n      <th>TOL<\/th>\n      <th>TOL结论<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 3, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;容忍度分析结果&lt;/p&gt;
&lt;/div&gt;


---
## 回归系数方差分解诊断法(CVD)

回归系数方差分解法（Coefficient Variance Decomposition）：通过计算特征值(eigenvalues)，进而得到病态数(
`\(K_j\)`)和方差分解比率
`\(VDP_j\)`，最后做出多重共线性的诊断结论：

对于k变量回归模型：

`$$\begin{align}
\mathbf{y} &amp;= \mathbf{X}\mathbf{\hat{\beta}}+\mathbf{e}  &amp;&amp; \text{(SRM)} 
\end{align}$$`

可以得到OLS下参数估计的方差协方差矩阵：

`$$\begin{align}
var-cov(\boldsymbol{\hat{\beta}})
= \sigma^2 \boldsymbol{(X'X)^{-1}} 
= \sigma^2 \boldsymbol{VD^{-1}V'} 
\end{align}$$`

其中，
`\(\mathbf{D}\)`是含有矩阵
`\(\mathbf{(X'X)^{-1}}\)`的**特征值**（eigenvalues）
`\(E_m\)`（
`\(m \in 1,2,\cdots,k\)`）的一个对角矩阵，而
`\(\mathbf{V}\)`是由相应**特征向量**构成的一个矩阵。

---
## 回归系数方差分解诊断法(CVD)

**病态数**（condition number）采用微分的方法，考察引入一个变量对（多重共线性）模型结果恶化情形出现的相对改变数，一般记为K，并正式定义为：

`$$\begin{align}
K_j&amp;= \frac{min(E_m)}{E_j}
\end{align}$$`
  
**方差分解比率**（variance-decomposition proportion），一般记为
`\(VDP_{ji}\)`，并正式定义为：

`$$\begin{align}
\phi_{ij}=\frac{v^2_{ij}}{E_j}; \quad
VDP_{ji}= \frac{\phi_{ij}}{\phi_i}  
\end{align}$$`

其中
`\(v_{ij}\)`为矩阵
`\(\mathbf{V}\)`的第
`\((i,j)\)`个元素。

---
## 回归系数方差分解诊断法(CVD)

病态数和方差分解比率：用来诊断多元线性回归模型的多重共线性问题严重程度的指标。诊断依据为：

- 若发现至少一个病态数
`\(K_j \leq 0.001\)`，则表明存在严重多重共线性；

- 观察病态数最小时所对应的方差分解比率，如果有多个斜率系数的 
`\(VDP_j \geq 0.5\)`，则表明存在严重的多重共线性

- 系数方差分解诊断方法由Belsley, Kuh and Welsch (BKW) 2004提出，具体细节可以参考[Eviews帮助文档](http://www.eviews.com/help/helpintro.html#page/content/testing-Coefficient_Diagnostics.html)，网址为：http://www.eviews.com/help/helpintro.html#page/content/testing-Coefficient_Diagnostics.html]。

- Eviews软件中病态数的计算是基于矩阵
`\(\mathbf{(X'X)}^{-1}\)`，而不是基于矩阵
`\(\mathbf{X}\)`

---
## 回归系数方差分解诊断法(CVD)

郎利案例中，主回归模型系数方差分解的多重共线性EViews诊断结果为：

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="pic/chpt7-vcd-full-result.png" alt="主回归模型系数方差分解的Eviews报告" width="589" /&gt;
&lt;p class="caption"&gt;主回归模型系数方差分解的Eviews报告&lt;/p&gt;
&lt;/div&gt;

---
class: center, middle,inverse
# 如何修正模型多重共线性问题？


---
## 删除变量法

一旦发现模型存在比较严重的多重共线性问题，则需要对模型进行修正处理，具体方法可参考：

简单剔除变量法：

- 依据经济学和实践经验观察，进行变量甄选或变量变换。利用先验信息（成为研究领域的专家！）酌情删除特定变量，减弱模型的多重共线性问题。那怎样才能获得先验信息呢？它往往源自经验研究工作或者有关基础理论。 

&gt; 怎样获得先验信息呢?它可以经验研究工作或者有关基础理论。例如，在柯布-道格拉斯生产函数中，如果人们预期规模报酬不变成立，则有
`\(\beta_2+\beta_3=1\)`。如果劳动和资本之间存在共线性，这一变换就减轻或消除了共线性问题。

- 变量变换法，进行变量处理。具体又包括差分变换法、比率变换法

---
## 删除变量法（案例）

郎利案例中，模型各变量的含义：

- `\(Y_i\)`表示就业人数；

- `\(X_{2i}\)`表示消费价格指数（CPI）；
`\(X_{3i}\)`表示国民生产总值（GNP，以百万美元计)；
`\(X_{4i}\)`表示失业人数(以千人计) ；
`\(X_{5i}\)`表示军队中的人数；
`\(X_{6i}\)`表示14岁以上的非机构人口数；
`\(X_{7i}\)`表示时间（
`\(t=1,2,\cdots,n\)`）。

简单删除的依据：

- 不用名义GNP，改用真实GNP。将名义GNP（X3） 除以价格指数CPI（X2），得到**实际GNP**（X3/X2）。

- 留下X5（军队中的人数），去掉X6（14 岁以上非机构人口数）。因为X6（14 岁以上非机构人口数）随时间（X7）不断增长，它与时间变量X7高度相。

- 去掉变量X4（失业人数）。可能失业率是劳动力市场状况的一个更好的度量指标，但我们没有这方面的数据，而失业人数X4也没有充分的理由包括进来。

---
## 删除变量法（案例R报告）

运用删除变量法，调整后的回归模型为：

`$$\begin{equation} \begin{alignedat}{999} &amp;Y=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} I(X3/X2)&amp;&amp; + \beta_{3} X5&amp;&amp; + \beta_{4} X6&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

回归结果为：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{Y}=&amp;&amp;+65720.37&amp;&amp;+97.36I(X3/X2)&amp;&amp;-0.69X5&amp;&amp;-0.30X6\\ &amp;\text{(t)}&amp;&amp;(6.1856)&amp;&amp;(5.4347)&amp;&amp;(-2.1350)&amp;&amp;(-2.1130)\\&amp;\text{(se)}&amp;&amp;(10624.8077)&amp;&amp;(17.9155)&amp;&amp;(0.3222)&amp;&amp;(0.1418)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9814;&amp;&amp; \bar{R^2}=0.9768\\&amp; &amp;&amp; F^{\ast}=211.10;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 变量变换法（一阶差分法）

面对严重的共线性，最简单的方法就是去掉某些变量，但剔除变量会导致设定误差。实际中需要权衡利弊。

**一阶差分法**(first difference form)巧妙删除变量：模型中两个解释变量
`\(X_{k,i}\)`和
`\(X_{w,i}\)`可能导致高度多重共线性，但是分别对二者进行一阶差分，再进行回归建模，新模型可能的多重共线性问题很可能大大缓解！具体变换如下:

`$$\begin{align}
Y_t &amp; =\beta_1+\beta_2X_{2,t}+\beta_3X_{3,t}+u_t &amp;&amp; \text{原模型} \\
Y_{t-1} &amp; =\beta_1+\beta_2X_{2,t-1}+\beta_3X_{3,t-1}+u_{t-1} &amp;&amp; \text{滞后1阶变量模型} \\
Y_t-Y_{t-1} &amp; =\beta_2(X_{2,t}-X_{2,t-1})+\beta_3(X_{3,t}-X_{3,t-1})+(u_t-u_{t-1}) &amp;&amp; \text{一阶差分模型}  \\
Y^{\ast}_t &amp;=\beta_2X^{\ast}_{2,t}+\beta_3X^{\ast}_{3,t}+v_t &amp;&amp; \text{精简化模型} 
\end{align}$$`

&gt;需要注意的是，“按下葫芦浮起瓢”，治疗比疾病更糟糕？

&gt;差分变换
`\(Y_{t-1}\)`减少了自由度；同时
`\(v_t=(u_t-u_{t-1})\)`可能带来异方差问题。

---
## 变量变换法（比率变换法）


**比率变换法**(ratio transformation)巧妙删除变量：模型中两个解释变量
`\(X_{k,i}\)`和
`\(X_{w,i}\)`可能导致高度多重共线性，如果可以用其中的一个变量同时对模型其他变量进行比率变换，而且如果变换后的所有变量还能具有经济学含义，那么理论上将至少消掉一个回归元，从而大大缓解甚至消除多重共线性问题！

以**消费支出决定**为例：

`\(Y_t\)` 为以真实价格表示的消费支出，
`\(X_{2,t}\)`表示GDP，
`\(X_{3,t}\)`表示总人口。
    
`$$\begin{align}
Y_t &amp; =\beta_1+\beta_2X_{2,t}+\beta_3X_{3,t}+u_t &amp;&amp; \text{原模型}  \\
\frac{Y_t}{X_{3,t}} &amp; =\frac{\beta_1}{X_{3,t}}+\beta_2\frac{X_{2,t}}{X_{3,t}}+\frac{u_t}{X_{3,t}} &amp;&amp; \text{比率变换模型} \\
Y^{\ast}_t &amp;=\beta^{\ast}_1+\beta^{\ast}_2X^{\ast}_{2,t}+v_t &amp;&amp; \text{精简化模型} 
\end{align}$$`

&gt;同样需要注意的是，“按下葫芦浮起瓢”，治疗比疾病更糟糕？

&gt; 
`\(v_t=\frac{u_t}{X_{3,t}}\)`可能带来异方差问题。

---
## 逐步回归法

**逐步最小二乘回归法**（Stepwise Least Squares Regression）通过多个统计标准，可以自动判断模型该引入还是删除某些自变量X。这些统计标准主要包括分析引入新变量对回归平方和ESS的贡献大小，及F检验等。

- **前向逐步回归法**（Stepwise-Forwards），是从一个简化模型（很少X变量）开始，再逐步引入新的X变量，直至达到某个统计标准（主要是p值标准）

- **后向逐步回归法**（Stepwise-Backwards），是从一个完全模型（全部X变量）开始，对模型逐步删除某些X变量，直至剩余变量都达到某个统计标准（主要是p值标准）

逐步最小二乘回归法一般采用如下标准来自动筛选变量：

- p值判别法：
`\(p\in[0.1,0.05)\)`(比较显著)；
`\(p\in[0.05,0.01)\)`(比较显著)；
`\(p\leq 0.01\)`(极其显著)

- `\(t^{\ast}\)`值判别法：2t法则

- **AIC**信息准则：越小越好

`R`统计软件下采用**后向逐步回归法**（Stepwise-Backwards）自动删除变量，最终回归结果报告见下页--&gt;

---
class:duke-softblue


```

Call:
lm(formula = Y ~ X3 + X4 + X5 + X7, data = data_longley)

Residuals:
   Min     1Q Median     3Q    Max 
-421.7 -124.6  -24.2   83.7  452.7 

Coefficients:
              Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept) 74169.5322  4251.5849   17.45  2.3e-09 ***
X3             -0.0402     0.0165   -2.44  0.03283 *  
X4             -2.0884     0.2900   -7.20  1.7e-05 ***
X5             -1.0146     0.1837   -5.52  0.00018 ***
X7           1887.4095   382.7665    4.93  0.00045 ***
---
Signif. codes:  
0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 279 on 11 degrees of freedom
Multiple R-squared:  0.995,	Adjusted R-squared:  0.994 
F-statistic:  590 on 4 and 11 DF,  p-value: 9.5e-13
```

.footnote[ [*] 后向逐步回归的具体细节，请参看**附录7-A**给出的.red[详细报告]]


---
## 逐步回归法（案例报告）




经过**后向逐步回归法**的标准筛选变量，得到的新模型为：

`$$\begin{equation} \begin{alignedat}{999} &amp;Y=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X3&amp;&amp; + \beta_{3} X4&amp;&amp; + \beta_{4} X5&amp;&amp; + \beta_{5} X7&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

逐步回归法矫正后回归模型的简要回归报告如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{Y}=&amp;&amp;+74169.53&amp;&amp;-0.04X3&amp;&amp;-2.09X4&amp;&amp;-1.01X5&amp;&amp;+1887.41X7\\ &amp;\text{(t)}&amp;&amp;(17.4451)&amp;&amp;(-2.4398)&amp;&amp;(-7.2021)&amp;&amp;(-5.5223)&amp;&amp;(4.9310)\\&amp;\text{(se)}&amp;&amp;(4251.5849)&amp;&amp;(0.0165)&amp;&amp;(0.2900)&amp;&amp;(0.1837)&amp;&amp;(382.7665)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9954;&amp;&amp; \bar{R^2}=0.9937\\&amp; &amp;&amp; F^{\ast}=589.76;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

---
## 补充新数据法（有时候有用！）

由于多重共线性是一个样本特性，故有可能在关于同样变量的另一样本中共线性没有第一个样本那么严重

在三变量回归中，有：

`$$\begin {align} 
\operatorname{var}\left(\hat{\beta}_{2}\right)=\frac{\sigma^{2}}{\sum x_{2 i}^{2}\left(1-r_{23}^{2}\right)}
\end {align}$$`

- 随着样本增加， 
`\(\sum x_{2 i}^{2}\)`一般地都会增加（为什么？）

- 因此，对任何给定的
`\(r^2_{23}\)`， 
`\(\hat{\beta}_2\)`的方差将会减小，从而减低
`\(\hat{\beta}_2\)`的标准误，使我们能够更准确地估计
`\(\beta_2\)`。

---
## 多项式回归法（原理）

多项式回归模型中离差形式或正交多项式(orthogonal polynomials)可以降低共线性的影响。

- 多项式回归模型的一个特点是解释变量以不同的幂出现，从而容易导致多重共线性

- 处理办法：离差形式或正交多项式(orthogonal polynomials)方法

.footnote[ 相关资料可以参考宾夕法尼亚大学的[Reducing Structural Multicollinearity](https://newonlinecourses.science.psu.edu/stat501/node/349/) ]

---
## 多项式回归法（案例数据）




&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-5503ce109a05ca06a450" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-5503ce109a05ca06a450">{"x":{"filter":"none","data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30],[881,1290,2147,1909,1282,1530,2067,1982,1019,1651,752,1687,1782,1529,969,1660,2121,1382,1714,1959,1158,965,1456,1273,1418,1743,1997,2177,1965,1264],[34.6,45,62.3,58.9,42.5,44.3,67.9,58.5,35.6,49.6,33,52,61.4,50.2,34.1,52.5,69.9,38.8,50.6,69.4,37.4,35.1,43,44.1,49.8,54.4,68.5,69.5,63,43.2],[1197.16,2025,3881.29,3469.21,1806.25,1962.49,4610.41,3422.25,1267.36,2460.16,1089,2704,3769.96,2520.04,1162.81,2756.25,4886.01,1505.44,2560.36,4816.36,1398.76,1232.01,1849,1944.81,2480.04,2959.36,4692.25,4830.25,3969,1866.24],[-676.633333333333,-267.633333333333,589.366666666667,351.366666666667,-275.633333333333,-27.6333333333334,509.366666666667,424.366666666667,-538.633333333333,93.3666666666666,-805.633333333333,129.366666666667,224.366666666667,-28.6333333333334,-588.633333333333,102.366666666667,563.366666666667,-175.633333333333,156.366666666667,401.366666666667,-399.633333333333,-592.633333333333,-101.633333333333,-284.633333333333,-139.633333333333,185.366666666667,439.366666666667,619.366666666667,407.366666666667,-293.633333333333],[-16.0366666666667,-5.63666666666667,11.6633333333333,8.26333333333333,-8.13666666666667,-6.33666666666667,17.2633333333333,7.86333333333333,-15.0366666666667,-1.03666666666667,-17.6366666666667,1.36333333333333,10.7633333333333,-0.436666666666667,-16.5366666666667,1.86333333333333,19.2633333333333,-11.8366666666667,-0.0366666666666688,18.7633333333333,-13.2366666666667,-15.5366666666667,-7.63666666666667,-6.53666666666667,-0.836666666666673,3.76333333333333,17.8633333333333,18.8633333333333,12.3633333333333,-7.43666666666667],[257.174677777778,31.7720111111112,136.033344444444,68.2826777777777,66.2053444444445,40.1533444444445,298.022677777778,61.8320111111111,226.101344444445,1.07467777777778,311.052011111111,1.85867777777777,115.849344444444,0.190677777777778,273.461344444445,3.4720111111111,371.076011111111,140.106677777778,0.0013444444444446,352.062677777778,175.209344444445,241.388011111111,58.3186777777778,42.7280111111111,0.700011111111122,14.1626777777777,319.098677777778,355.825344444444,152.852011111111,55.3040111111111]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>obs<\/th>\n      <th>Y<\/th>\n      <th>X<\/th>\n      <th>\\(X^2\\)<\/th>\n      <th>y<\/th>\n      <th>\\( x \\)<\/th>\n      <th>\\(x^2 \\)<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"tip","pageLength":6,"columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[6,10,25,50,100],"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 3, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 4, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 5, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 6, 2, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;成本与产量数据n=30)&lt;/p&gt;
&lt;/div&gt;

其中：
`\(Y\)`表示成本（`Cost`）；
`\(X\)`表示产品产量（`Output`）；
`\(X\)`表示产品产量的平方；
`\(y\)`表示成本的离差；
`\(x\)`表示产量的离差；
`\(x^2\)`表示产量离差的平方。

---
## 多项式回归法（案例绘图1）

--
.pull-left[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-55-1.png" alt="产量和成本的散点图"  /&gt;
&lt;p class="caption"&gt;产量和成本的散点图&lt;/p&gt;
&lt;/div&gt;

]

--

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-56-1.png" alt="产量和产量平方的散点图"  /&gt;
&lt;p class="caption"&gt;产量和产量平方的散点图&lt;/p&gt;
&lt;/div&gt;
]

--

实际上产量
`\(X\)`和产量平方
`\(X^2\)`之间的相关系数为：
`\(r_{(X,X^2)}=\)` 0.995。人们可能执意构建如下的**二次多项式抛物线模型1**：

`$$\begin{equation} \begin{alignedat}{999} &amp;Y=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} X&amp;&amp; + \beta_{3} I(X^2)&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

---
## 多项式回归法（案例回归1）

上述**二次多项式抛物线模型1**回归分析结果如下：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{Y}=&amp;&amp;-1464.40&amp;&amp;+88.31X&amp;&amp;-0.54I(X^2)\\ &amp;\text{(t)}&amp;&amp;(-3.5596)&amp;&amp;(5.3606)&amp;&amp;(-3.3900)\\&amp;\text{(se)}&amp;&amp;(411.4012)&amp;&amp;(16.4735)&amp;&amp;(0.1582)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9377;&amp;&amp; \bar{R^2}=0.9331\\&amp; &amp;&amp; F^{\ast}=203.16;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

--

对以上模型进行方差膨胀因子（VIF）分析，结果为：




&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-f093521944f8fd17d92a" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-f093521944f8fd17d92a">{"x":{"filter":"none","data":[["\\( X \\)","\\( X^2 \\)"],[99.9426130734453,99.9426130734453]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>terms<\/th>\n      <th>VIF<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;自变量方差膨胀因子分析结果&lt;/p&gt;
&lt;/div&gt;

---
## 多项式回归法（案例绘图2）

--
.pull-left[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-61-1.png" alt="产量离差和成本离差的散点图"  /&gt;
&lt;p class="caption"&gt;产量离差和成本离差的散点图&lt;/p&gt;
&lt;/div&gt;

]

--

.pull-right[

&lt;div class="figure" style="text-align: center"&gt;
&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-62-1.png" alt="产量离差和产量离差平方的散点图"  /&gt;
&lt;p class="caption"&gt;产量离差和产量离差平方的散点图&lt;/p&gt;
&lt;/div&gt;
]

--

此时产量离差
`\(x\)`和产量离差平方
`\(x^2\)`之间的相关系数为：
`\(r_{(x,x^2)}=\)` 0.22。我们再构建如下的**二次多项式抛物线模型2**：


`$$\begin{equation} \begin{alignedat}{999} &amp;y=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} x&amp;&amp; + \beta_{3} I(x^2)&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

---
## 多项式回归法（案例回归2）

**二次多项式抛物线模型2**的回归分析结果为：

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{y}=&amp;&amp;+74.56&amp;&amp;+34.00x&amp;&amp;-0.54I(x^2)\\ &amp;\text{(t)}&amp;&amp;(2.5406)&amp;&amp;(20.1297)&amp;&amp;(-3.3900)\\&amp;\text{(se)}&amp;&amp;(29.3486)&amp;&amp;(1.6890)&amp;&amp;(0.1582)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9377;&amp;&amp; \bar{R^2}=0.9331\\&amp; &amp;&amp; F^{\ast}=203.16;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

--

对该模型进行方差膨胀因子（VIF）分析，结果为：




&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-7e44a29ea184ec9e7408" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-7e44a29ea184ec9e7408">{"x":{"filter":"none","data":[["\\( x \\)","\\( x^2 \\)"],[1.05062775264011,1.05062775264011]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>terms<\/th>\n      <th>VIF<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;自变量方差膨胀因子分析结果&lt;/p&gt;
&lt;/div&gt;

---
## 岭回归法（基本原理）

**岭回归法**(ridge regression)，也被称为脊回归，常被用来"解决"多重共线性问题。可惜这些技术都要利用矩阵代数才便于讨论&lt;sup&gt;*&lt;/sup&gt;。

`$$\begin {align}
\boldsymbol{y} &amp; = \boldsymbol{X\beta +u} \\
\boldsymbol{\hat{\beta}}^{ridge} &amp; =\underset{\boldsymbol{\beta} \in \mathbb{R}}{\operatorname{argmin}}\| \boldsymbol{y-X B} \|_{2}^{2}+\lambda\| \boldsymbol{B} \|_{2}^{2} \\
\boldsymbol{\hat{\beta}}_{\text {ridge}} &amp; =\boldsymbol{\left(X^{T} X+\lambda I\right)^{-1} X^{T} y}
\end {align}$$`

其中：
`\(\| \boldsymbol{B} \|_{2}=\sqrt{\beta_{1}^{2}+\beta_{2}^{2}+\cdots+\beta_{k}^{2}}\)`表示**向量范数**（vector norm）运算



.footnote[[*] 岭回归的具体细节，请参看附录[材料1 Ridge Regression for Better Usage](https://towardsdatascience.com/ridge-regression-for-better-usage-2f19b3a202db)、[材料2](https://drsimonj.svbtle.com/ridge-regression-with-glmnet) 和[材料3 Ridge Regression](http://r-statistics.co/Ridge-Regression-With-R.html) ]

---
## 岭回归法（R软件分析）

利用`R`软件进行岭回归分析，结果报告如下：



&lt;img src="pic/chpt7-ridge-reg.png" width="767" style="display: block; margin: auto;" /&gt;

---
## 主成分分析法（原理）

**主成分分析法**（Principal components regression ，PCR），其核心思想是**降维**（Dimensionality reduction），也即通过一定的统计方法把**原来**个数较多的自变量，转换为个数相对较少的**新的**自变量，然后再利用较少的自变量进行回归，从而达到消除多重共线性的目的。
 
- 先根据主成分分析确定主成分个数（看累积解释百分比）

- 再用**主成分变量**进行回归分析

.footnote[ 技术细节可以参看[Performing Principal Components Regression (PCR) in R](http://www.quantide.com/performing-principal-components-regression-pcr-in-r/) ，以及[Principal Component Regression in
R](https://27411.compute.dtu.dk/filemanager/uploads/27411/eNotepdfs/eNote4-PCRinR.pdf)]

---
### 主成分分析法（案例）

郎利案例中，`R`软件下主成分分析结果为：


```
Data: 	X dimension: 16 6 
	Y dimension: 16 1
Fit method: svdpc
Number of components considered: 6

VALIDATION: RMSEP
Cross-validated using 10 random segments.
       (Intercept)  1 comps  2 comps  3 comps  4 comps
CV            3627     1158     1100    504.1    605.9
adjCV         3627     1143     1086    498.6    597.0
       5 comps  6 comps
CV       423.9    425.3
adjCV    415.8    413.8

TRAINING: % variance explained
   1 comps  2 comps  3 comps  4 comps  5 comps
X    76.72    96.31     99.7    99.95    99.99
Y    91.43    92.89     98.6    98.61    99.40
   6 comps
X   100.00
Y    99.55
```

---
### 主成分分析法（案例：碎石图）

主成分分析的**判定系数**与**主成分数量**的变化情况如下：

&lt;img src="07-multi-linearity-slide_files/figure-html/unnamed-chunk-70-1.png" style="display: block; margin: auto;" /&gt;

从上图可以看出，**主成分**个数为3个时，**判定系数**基本就达到很高的水平了。

---
### 主成分分析法（案例：载荷矩阵）



此时，我们可以看看主成分的**载荷矩阵**：

&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-96b386e4335d6f6abd34" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-96b386e4335d6f6abd34">{"x":{"filter":"none","data":[["X2","X3","X4","X5","X6","X7"],[0.461834898166772,0.461504346679002,0.321316698306542,0.201509741873208,0.46227935304156,0.46494028423307],[-0.0578427676677561,-0.0532122862362264,0.59551376271271,-0.798192547956448,0.0455444698016807,-0.000618788393180556],[0.14911989205308,0.277682326807799,-0.728305685780268,-0.561607523719203,0.195984591081625,0.128115731248627],[0.792873558903561,-0.121621225164949,0.00764579549379912,-0.077254979078646,-0.589744964749654,-0.0522865542202941],[-0.3379378262071,0.149573191817692,-0.00923196110077494,-0.0242524723087071,-0.548578173093233,0.749542835596957],[0.135187070638873,-0.818480820696095,-0.107452680630903,-0.01797096028166,0.311570867763095,0.4504088836356]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>vars<\/th>\n      <th>PC1<\/th>\n      <th>PC2<\/th>\n      <th>PC3<\/th>\n      <th>PC4<\/th>\n      <th>PC5<\/th>\n      <th>PC6<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 3, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 4, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 5, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 6, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;主成分分析的载荷矩阵&lt;/p&gt;
&lt;/div&gt;

请注意前三个主成分`PC1`、`PC2`、`PC3`，以及它们在自变量上的载荷分布。

---
### 主成分分析法（案例：因子得分及新数据）

我们把因变量进行标准化变换
`\(Y^{\ast}_i=\frac{Y_i-\bar{Y}}{S_{Y_i}}\)`，同时利用以上主成分分析的因子得分，可以得到**主成分降维**后的新数据：

&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-e6e679636db370b43c79" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-e6e679636db370b43c79">{"x":{"filter":"none","data":[[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16],[-1.42199458930516,-1.19448684464059,-1.46527516150667,-1.17597870521231,-0.596816311410414,-0.477794737856238,-0.0933949189611716,-0.443056383852387,0.199887905825434,0.723241140735902,0.812080209991651,0.34054976548037,0.950464144793874,1.20929335618322,1.142948794848,1.4903323348865],[-3.47885116461591,-3.01051049855093,-2.34330004185856,-2.09390206675872,-1.43823978975673,-1.01025832576328,-0.702427324412702,0.0324903553282046,0.0995120782946485,0.449431612773825,0.955062653093871,1.8170995373291,1.93998934698446,2.36112184416995,3.07801834512015,3.34476343862262],[0.751466260852026,0.849040549823804,1.53999657499663,1.27632044197399,-1.23579413551051,-1.92210442043256,-1.91056317495043,-0.593047850840494,-0.693492715226238,-0.547844423501594,-0.429448029428114,0.863171794787778,0.38657073582799,0.499103927567887,0.989955825010999,0.176668639048833],[0.307945816507745,0.642231173899522,-0.493433528839948,-0.111293183614468,-0.0290952725820153,-0.161216650943881,0.0671359308977066,-1.03899732376216,-0.0975673209092177,0.292948256342065,0.445238249201229,-0.677417816899047,0.265962368310368,0.365672101839671,-0.201967849931779,0.423855050484207]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>obs<\/th>\n      <th>Y.std<\/th>\n      <th>PC1<\/th>\n      <th>PC2<\/th>\n      <th>PC3<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"tip","pageLength":5,"columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"lengthMenu":[5,10,25,50,100],"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 3, 4, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 4, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;因子得分及新数据&lt;/p&gt;
&lt;/div&gt;

---
### 主成分分析法（案例：新模型回归）

利用以上**降维**后的新数据，可以构造如下新的模型：

`$$\begin{equation} \begin{alignedat}{999} &amp;Y.std=&amp;&amp; + \beta_{1} &amp;&amp; + \beta_{2} PC1&amp;&amp; + \beta_{3} PC2&amp;&amp; + \beta_{4} PC3&amp;&amp;+e_i\\ \end{alignedat} \end{equation}$$`

`$$\begin{equation} \begin{alignedat}{999} &amp;\widehat{Y.std}=&amp;&amp;-0.00&amp;&amp;+0.45PC1&amp;&amp;-0.11PC2&amp;&amp;+0.53PC3\\ &amp;\text{(t)}&amp;&amp;(-0.0000)&amp;&amp;(27.9607)&amp;&amp;(-3.5371)&amp;&amp;(6.9867)\\&amp;\text{(se)}&amp;&amp;(0.0331)&amp;&amp;(0.0159)&amp;&amp;(0.0315)&amp;&amp;(0.0758)\\&amp;\text{(fitness)}&amp;&amp; R^2=0.9860;&amp;&amp; \bar{R^2}=0.9825\\&amp; &amp;&amp; F^{\ast}=281.04;&amp;&amp; p=0.0000 \end{alignedat} \end{equation}$$`

--

对该模型进行方差膨胀因子（VIF）分析，VIF都比较小，多重共线性问题得到解决。



&lt;div class="figure" style="text-align: center"&gt;
&lt;div id="htmlwidget-d843b07efa1a0b426f19" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-d843b07efa1a0b426f19">{"x":{"filter":"none","data":[["PC1","PC2","PC3"],[1,1,1]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>terms<\/th>\n      <th>VIF<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-center","targets":"_all"}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 4, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script><&lt;p class="caption"&gt;新模型的方差膨胀因子分析结果&lt;/p&gt;
&lt;/div&gt;


---
## 多重共线性一定是坏事吗?

如果回归分析的唯一目的是预测或预报，则多重共线性就不是一个严重的问题。因为
`\(R^2\)`值越高，预测越准。

**无为而治**：多重共线性是普遍存在的，它并不是OLS 或其他一般性统计方法引起的问题


---
background-image: url("pic/thank-you-gif-funny-little-yellow.gif")
class: inverse,center
# 本章结束
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
